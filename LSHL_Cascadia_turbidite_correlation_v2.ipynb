{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a219ff43",
   "metadata": {},
   "source": [
    "# Correlation of Cascadia magnetic susceptibility logs using dynamic time warping\n",
    "\n",
    "Supplementary material to the manuscript \"Turbidite correlation for paleoseismology\" by Nieminski et al.\n",
    "\n",
    "Requirements\n",
    "* numpy\n",
    "* matplotlib\n",
    "* pandas\n",
    "* librosa\n",
    "* scipy\n",
    "* tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ad6ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from librosa.sequence import dtw\n",
    "from scipy import stats\n",
    "\n",
    "# # set up graphics:\n",
    "# %matplotlib qt\n",
    "# plt.rcParams['svg.fonttype'] = 'none'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a0c9e8",
   "metadata": {},
   "source": [
    "## Functions used in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0609b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_log(log, **kwargs):\n",
    "    \"\"\"\n",
    "    Normalize log values to a 0-1 range using either specified min/max values or percentile-based bounds.\n",
    "    \n",
    "    Parameters:\n",
    "        log (array): Input log data to normalize\n",
    "        **kwargs: Optional keyword arguments\n",
    "            minlog (float): Minimum value to use for normalization. If not provided, uses 1st percentile\n",
    "            maxlog (float): Maximum value to use for normalization. If not provided, uses 99th percentile\n",
    "            \n",
    "    Returns:\n",
    "        array: Normalized log data clipped to 0-1 range\n",
    "        \n",
    "    Notes:\n",
    "        - Uses 1st and 99th percentiles by default to avoid outlier effects\n",
    "        - Clips values outside 0-1 range\n",
    "        - Handles null values (-999.25) by excluding them from percentile calculations\n",
    "    \"\"\"\n",
    "    if len(kwargs) == 0: # if 'minlog' and 'maxlog' are not defined\n",
    "        minlog = np.nanpercentile(log[log != -999.25], 1)  # Use 1st percentile excluding null values\n",
    "        maxlog = np.nanpercentile(log[log != -999.25], 99) # Use 99th percentile excluding null values\n",
    "    else:\n",
    "        minlog = kwargs['minlog']\n",
    "        maxlog = kwargs['maxlog']\n",
    "    if minlog != maxlog:\n",
    "        log_n = (log - minlog)/(maxlog - minlog) # calculate normalized log curve\n",
    "        log_n[log_n > 1] = 1.0 # clip maximum values to 1\n",
    "        log_n[log_n < 0] = 0.0 # clip minimum values to 0\n",
    "    else:\n",
    "        log_n = log\n",
    "    return log_n\n",
    "\n",
    "def correlate_logs(log1, log2, exponent):\n",
    "    \"\"\"\n",
    "    Correlate two logs using dynamic time warping (DTW) algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        log1 (array): First log data\n",
    "        log2 (array): Second log data \n",
    "        exponent (float): Exponent used in similarity matrix calculation to control sensitivity\n",
    "        \n",
    "    Returns:\n",
    "        p (array): Correlation indices for first log\n",
    "        q (array): Correlation indices for second log\n",
    "        D (array): DTW cost matrix\n",
    "        \n",
    "    Notes:\n",
    "        - Creates similarity matrix using absolute differences raised to specified exponent\n",
    "        - Uses librosa DTW implementation to find optimal warping path\n",
    "        - Returns indices that map corresponding points between logs\n",
    "    \"\"\"\n",
    "    r = len(log1)\n",
    "    c = len(log2)\n",
    "    sm = np.zeros((r,c)) # similarity matrix\n",
    "    for i in range(0,r):\n",
    "        sm[i,:] = (np.abs(log2 - log1[i]))**exponent\n",
    "    D, wp = dtw(C = sm) # dynamic time warping\n",
    "    p = wp[:,0] # correlation indices for first curve\n",
    "    q = wp[:,1] # correlation indices for second curve\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    return p, q, D\n",
    "\n",
    "def get_yl_br_color(log_value):\n",
    "    \"\"\"\n",
    "    Generate a color in the yellow-brown spectrum based on log value.\n",
    "    \n",
    "    Parameters:\n",
    "        log_value (float): Value between 0-1 to determine color\n",
    "        \n",
    "    Returns:\n",
    "        array: RGB color values in range 0-1\n",
    "        \n",
    "    Notes:\n",
    "        - Maps log values to yellow-brown color spectrum\n",
    "        - Yellow represents low values, brown represents high values\n",
    "        - RGB values are clipped to valid 0-1 range\n",
    "    \"\"\"\n",
    "    color = np.array([1-0.4*log_value, 1-0.7*log_value, 0.6-0.6*log_value])\n",
    "    color[color > 1] = 1\n",
    "    color[color < 0] = 0\n",
    "    return color\n",
    "\n",
    "def plot_correlation(log1, log2, d1, d2, p, q, step):\n",
    "    \"\"\"\n",
    "    Plot correlation between two logs with colored intervals showing matches.\n",
    "    \n",
    "    Parameters:\n",
    "        log1 (array): First log data\n",
    "        log2 (array): Second log data\n",
    "        d1 (array): Depth values for first log\n",
    "        d2 (array): Depth values for second log\n",
    "        p (array): Correlation indices for first log\n",
    "        q (array): Correlation indices for second log\n",
    "        step (int): Step size for plotting correlation intervals\n",
    "        \n",
    "    Notes:\n",
    "        - Creates figure with two logs side by side\n",
    "        - Shows correlation between logs using colored intervals\n",
    "        - Colors represent log values (yellow-brown spectrum)\n",
    "        - Depth axes shown on both sides\n",
    "        - Handles reversed intervals where indices decrease\n",
    "        - Sets up consistent depth labeling at 100cm intervals\n",
    "    \"\"\"\n",
    "    # for correlation at the sample scale\n",
    "    fig = plt.figure(figsize=(6,20))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(log1, d1 - np.min(d1), 'b', linewidth=1) # we want units of d1 and d2\n",
    "    ax.plot(log2 + 2, d2 - np.min(d2), 'b', linewidth=1)\n",
    "    ax.plot([1, 1],[0, np.max(d1-np.min(d1))], 'k', linewidth=0.5)\n",
    "    ax.plot([2, 2],[0, np.max(d2-np.min(d2))], 'k', linewidth=0.5)\n",
    "    ax.set_xlim(0, 3)\n",
    "    ax.set_ylim(0, max(np.max(d1-np.min(d1)), np.max(d2-np.min(d2))))\n",
    "    log1 = 1 - log1\n",
    "    log2 = 1 - log2\n",
    "    min_d = min(np.min(d1),np.min(d2))\n",
    "    max_d = max(np.max(d1),np.max(d2))\n",
    "    for i in range(0, len(p)-step, step):\n",
    "        # intervals for log on the left:\n",
    "        depth1_base = d1[p[i]]-np.min(d1)\n",
    "        depth1_top = d1[p[i+step]]-np.min(d1)\n",
    "        if p[i+step] < p[i]:\n",
    "            mean_log1 = np.mean(log1[p[i+step]: p[i]])\n",
    "            x = [0, 1, 1, 0]\n",
    "            y = [depth1_base, depth1_base, depth1_top, depth1_top]\n",
    "            ax.fill(x, y, facecolor=get_yl_br_color(mean_log1), edgecolor=None)\n",
    "        else:\n",
    "            mean_log1 = log1[p[i]]\n",
    "        # intervals for log on the right:\n",
    "        depth2_base = d2[q[i]]-np.min(d2)\n",
    "        depth2_top = d2[q[i+step]]-np.min(d2)\n",
    "        if q[i+step] < q[i]:  \n",
    "            mean_log2 = np.mean(log2[q[i+step]: q[i]])\n",
    "            x = [2, 3, 3, 2]\n",
    "            y = [depth2_base, depth2_base, depth2_top, depth2_top]\n",
    "            ax.fill(x, y, facecolor=get_yl_br_color(mean_log2), edgecolor=None)\n",
    "        else:\n",
    "            mean_log2 = log2[q[i]]\n",
    "        # intervals between the two logs:\n",
    "        if (p[i+step] < p[i]) or (q[i+step] < q[i]):\n",
    "            mean_logs = (mean_log1 + mean_log2)*0.5\n",
    "            x = [1, 2, 2, 1]\n",
    "            y = [depth1_base, depth2_base, depth2_top, depth1_top]\n",
    "            plt.fill(x, y, facecolor=get_yl_br_color(mean_logs), edgecolor=None)\n",
    "    depth1_base = d1[p[i+step]]-np.min(d1) # last layer, log on left\n",
    "    depth1_top = d1[p[-1]]-np.min(d1)\n",
    "    if p[-1] < p[i+step]:\n",
    "        mean_log1 = np.mean(log1[p[-1] : p[i+step]])\n",
    "        x = [0, 1, 1, 0]\n",
    "        y = [depth1_base, depth1_base, depth1_top, depth1_top]\n",
    "        plt.fill(x, y, facecolor=get_yl_br_color(mean_log1), edgecolor=None)\n",
    "    else:\n",
    "        mean_log1 = log1[p[i+step]]\n",
    "    depth2_base = d2[q[i+step]]-np.min(d2) # last layer, log on right\n",
    "    depth2_top = d2[q[-1]]-np.min(d2)\n",
    "    if q[-1] < q[i+step]:  \n",
    "        mean_log2 = np.mean(log2[q[-1] : q[i+step]])\n",
    "        x = [2, 3, 3, 2]\n",
    "        y = [depth2_base, depth2_base, depth2_top, depth2_top]\n",
    "        plt.fill(x, y, facecolor=get_yl_br_color(mean_log2), edgecolor=None)\n",
    "    else:\n",
    "        mean_log2 = log2[q[i+step]]\n",
    "    # intervals between the two logs (last layer):\n",
    "    if (p[-1] < p[i+step]) or (q[-1] < q[i+step]):\n",
    "        mean_logs = (mean_log1 + mean_log2)*0.5\n",
    "        x = [1, 2, 2, 1]\n",
    "        y = [depth1_base, depth2_base, depth2_top, depth1_top]\n",
    "        plt.fill(x, y, facecolor=get_yl_br_color(mean_logs), edgecolor=None)\n",
    "    ax.set_xticks([])\n",
    "    ax.invert_yaxis()\n",
    "    labels = []\n",
    "    start_depth = np.ceil(np.min(d1)/100) * 100\n",
    "    end_depth = np.floor(np.max(d1)/100) * 100\n",
    "    for label in np.arange(start_depth, end_depth+1, 100):\n",
    "        labels.append(str(label))\n",
    "    ax.set_yticks(np.arange(start_depth, end_depth+1, 100) - np.min(d1))\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_ylabel('depth (cm)', fontsize=12)\n",
    "    ax2 = ax.twinx()\n",
    "    labels = []\n",
    "    start_depth = np.ceil(np.min(d2)/100) * 100\n",
    "    end_depth = np.floor(np.max(d2)/100) * 100\n",
    "    for label in np.arange(start_depth, end_depth+1, 100):\n",
    "        labels.append(str(label))\n",
    "    ax2.set_yticks(np.arange(start_depth, end_depth+1, 100) - np.min(d2))\n",
    "    ax2.set_yticklabels(labels)\n",
    "    ax2.set_ylim(0, max(np.max(d1-np.min(d1)), np.max(d2-np.min(d2))))\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    \n",
    "def correlate_and_plot_logs(log1, log2, d1, d2, exponent):\n",
    "    \"\"\"\n",
    "    Correlate two logs using DTW, plot results, and calculate correlation statistics.\n",
    "    \n",
    "    Parameters:\n",
    "        log1 (array): First log data\n",
    "        log2 (array): Second log data\n",
    "        d1 (array): Depth values for first log\n",
    "        d2 (array): Depth values for second log\n",
    "        exponent (float): Exponent for DTW similarity matrix calculation\n",
    "        \n",
    "    Returns:\n",
    "        slope (float): Linear regression slope\n",
    "        intercept (float): Linear regression intercept\n",
    "        r_value (float): Correlation coefficient\n",
    "        p_value (float): Statistical significance\n",
    "        slope_std_error (float): Standard error of slope\n",
    "        \n",
    "    Notes:\n",
    "        - Combines correlation, plotting and statistical analysis\n",
    "        - Handles repeated indices by setting them to NaN\n",
    "        - Uses scipy.stats for linear regression and correlation statistics\n",
    "    \"\"\"\n",
    "    p, q, D = correlate_logs(log1, log2, exponent = exponent) \n",
    "    plot_correlation(log1, log2, d1, d2, p, q, step=1)\n",
    "    vsh_means1 = log1[p]\n",
    "    vsh_means1[np.where(np.diff(p)==0)[0] + 1] = np.nan\n",
    "    vsh_means2 = log2[q]\n",
    "    vsh_means2[np.where(np.diff(q)==0)[0] + 1] = np.nan\n",
    "    slope, intercept, r_value, p_value, slope_std_error = stats.linregress(vsh_means1[(np.isnan(vsh_means1)==0) & (np.isnan(vsh_means2)==0)], \n",
    "                                                                          vsh_means2[(np.isnan(vsh_means1)==0) & (np.isnan(vsh_means2)==0)])\n",
    "    return slope, intercept, r_value, p_value, slope_std_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df79a8e0",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e67a99ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Nora_analyzed_data/Lora_MS-log_data/'\n",
    "\n",
    "\n",
    "# Define core names\n",
    "core_name_22 = 'M9907-22PC'\n",
    "core_name_23 = 'M9907-23PC'\n",
    "core_name_25 = 'M9907-25PC'\n",
    "core_name_12 = 'M9907-12PC'\n",
    "core_name_11 = 'M9907-11PC'\n",
    "core_name_14 = 'M9907-14TC'\n",
    "core_name_56 = 'RR0207-56PC'\n",
    "core_name_02 = 'RR0207-02PC'\n",
    "core_name_30 = 'M9907-30PC'\n",
    "core_name_31 = 'M9907-31PC'\n",
    "core_name_09 = 'M9907-09TC'\n",
    "\n",
    "# Read data using core names\n",
    "df = pd.read_csv(dirname + core_name_22 + '.csv')\n",
    "ms_22 = np.array(df['MS'])\n",
    "md_22 = np.array(df['DEPTH'])\n",
    "\n",
    "df = pd.read_csv(dirname + core_name_23 + '.csv')\n",
    "ms_23 = np.array(df['MS'])\n",
    "md_23 = np.array(df['DEPTH'])\n",
    "\n",
    "df = pd.read_csv(dirname + core_name_25 + '.csv')\n",
    "ms_25 = np.array(df['MS'])\n",
    "md_25 = np.array(df['DEPTH'])\n",
    "\n",
    "df = pd.read_csv(dirname + core_name_12 + '.csv')\n",
    "ms_12 = np.array(df['MS'])\n",
    "md_12 = np.array(df['DEPTH'])\n",
    "\n",
    "df = pd.read_csv(dirname + core_name_11 + '.csv')\n",
    "ms_11 = np.array(df['MS'])\n",
    "md_11 = np.array(df['DEPTH'])\n",
    "\n",
    "df = pd.read_csv(dirname + core_name_14 + '.csv')\n",
    "ms_14 = np.array(df['MS'])\n",
    "md_14 = np.array(df['DEPTH'])\n",
    "\n",
    "df = pd.read_csv(dirname + core_name_56 + '.csv')\n",
    "ms_56 = np.array(df['MS'])\n",
    "md_56 = np.array(df['DEPTH'])\n",
    "\n",
    "df = pd.read_csv(dirname + core_name_02 + '.csv')\n",
    "ms_02 = np.array(df['MS'])\n",
    "md_02 = np.array(df['DEPTH'])\n",
    "\n",
    "df = pd.read_csv(dirname + core_name_30 + '.csv')\n",
    "ms_30 = np.array(df['MS'])\n",
    "md_30 = np.array(df['DEPTH'])\n",
    "\n",
    "df = pd.read_csv(dirname + core_name_31 + '.csv')\n",
    "ms_31 = np.array(df['MS'])\n",
    "md_31 = np.array(df['DEPTH'])\n",
    "\n",
    "df = pd.read_csv(dirname + core_name_09 + '.csv')\n",
    "ms_09 = np.array(df['MS'])\n",
    "md_09 = np.array(df['DEPTH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e00539",
   "metadata": {},
   "source": [
    "# 22-23 correlation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc751ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results depend a lot on how exactly the normalization is done!\n",
    "slope, intercept, r_value, p_value, slope_std_error = correlate_and_plot_logs(normalize_log(ms_22), \n",
    "                                                    normalize_log(ms_23)[95:], md_22, md_23[95:], exponent = 0.3)\n",
    "# The correlate_and_plot_logs function is used to analyze and visualize correlations between magnetic susceptibility logs from different wells\n",
    "# Parameters:\n",
    "# - log1, log2: The magnetic susceptibility data from two wells to compare\n",
    "# - depth1, depth2: The corresponding depth values for each log\n",
    "# - exponent: Controls the normalization strength (default=0.3)\n",
    "#   - Lower exponent (e.g. 0.3) reduces the impact of extreme values\n",
    "#   - Higher exponent emphasizes larger variations\n",
    "#\n",
    "# In this case:\n",
    "# - normalize_log() is applied to both ms_22 and ms_23 data to standardize the values\n",
    "# - ms_23[95:] slices the data to remove noisy surface measurements\n",
    "# - exponent=0.3 helps highlight meaningful correlations while reducing noise\n",
    "#\n",
    "# The function returns:\n",
    "# - slope: Linear regression slope between the logs\n",
    "# - intercept: Y-intercept of regression line  \n",
    "# - r_value: Pearson correlation coefficient (ranges from -1 to 1)\n",
    "# - p_value: Statistical significance of correlation\n",
    "# - slope_std_error: Standard error of regression slope\n",
    "#\n",
    "# The high r_value (0.73) and low p_value (5.58e-72) here indicate a strong, \n",
    "# statistically significant correlation between wells 22 and 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5403d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe56c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to record r-values and p-values\n",
    "r_values_dict = {}\n",
    "p_values_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f876740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_values_dict['22-23'] = r_value\n",
    "p_values_dict['22-23'] = p_value\n",
    "print('R values:', r_values_dict)\n",
    "print('P values:', p_values_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0980c",
   "metadata": {},
   "source": [
    "## 23-25 correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963867e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, slope_std_error = correlate_and_plot_logs(normalize_log(ms_23), \n",
    "                                                    normalize_log(ms_25[:670]), md_23, md_25[:670], exponent=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16673abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd8bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7aa935",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_values_dict['23-25'] = r_value\n",
    "p_values_dict['23-25'] = p_value\n",
    "print('R values:', r_values_dict)\n",
    "print('P values:', p_values_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f36c9",
   "metadata": {},
   "source": [
    "## 25-22 correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, slope_std_error = correlate_and_plot_logs(normalize_log(ms_25[:670])[85:], \n",
    "                                                    normalize_log(ms_22), md_25[85:670], md_22, exponent=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56255870",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1493f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec973a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_values_dict['25-22'] = r_value\n",
    "p_values_dict['25-22'] = p_value\n",
    "print('R values:', r_values_dict)\n",
    "print('P values:', p_values_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d15608",
   "metadata": {},
   "source": [
    "## 23-12 correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c142c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, slope_std_error = correlate_and_plot_logs(normalize_log(ms_23), \n",
    "                                                    normalize_log(ms_12[:236]), md_23, md_12[:236], exponent = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f257568",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc9caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b8f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_values_dict['23-12'] = r_value\n",
    "p_values_dict['23-12'] = p_value\n",
    "print('R values:', r_values_dict)\n",
    "print('P values:', p_values_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade999e8",
   "metadata": {},
   "source": [
    "## 12-11 correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee4b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, slope_std_error = correlate_and_plot_logs(normalize_log(ms_12), \n",
    "                                                    normalize_log(ms_11), md_12, md_11, exponent = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec5ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_values_dict['12-11'] = r_value\n",
    "p_values_dict['12-11'] = p_value\n",
    "print('R values:', r_values_dict)\n",
    "print('P values:', p_values_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045cdb6c",
   "metadata": {},
   "source": [
    "#### This is the Goldfinger et al. correlation (between T6 and T20):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, slope_std_error = correlate_and_plot_logs(normalize_log(ms_12)[77:321], \n",
    "                                            normalize_log(ms_11)[:211], md_12[77:321], md_11[:211], exponent = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcdd3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94836a38",
   "metadata": {},
   "source": [
    "#### Correlation of a restricted interval (with better looking correlation than the previous two):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9308ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, slope_std_error = correlate_and_plot_logs(normalize_log(ms_12)[157:359], \n",
    "                                            normalize_log(ms_11)[156:334], md_12[157:359], md_11[156:334], exponent = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be0c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc201fc",
   "metadata": {},
   "source": [
    "## 23-14 correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f0bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, slope_std_error = correlate_and_plot_logs(normalize_log(ms_23)[:592], \n",
    "                                                    normalize_log(ms_14)[:590], md_23[:592], md_14[:590], exponent = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_values_dict['23-14'] = r_value\n",
    "p_values_dict['23-14'] = p_value\n",
    "print('R values:', r_values_dict)\n",
    "print('P values:', p_values_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d7944f",
   "metadata": {},
   "source": [
    "## 56-31 correlation\n",
    "\n",
    "Hydrate Ridge Basin West - Rogue Canyon\n",
    "\n",
    "If you take away T19 at the bottom of 56, the correlation looks worse (and the correlation coefficient is smaller)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, slope_std_error = correlate_and_plot_logs(normalize_log(ms_56)[55:], \n",
    "                                                normalize_log(ms_31[:587]), md_56[55:], md_31[:587], exponent = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, slope_std_error = correlate_and_plot_logs(normalize_log(ms_56), \n",
    "                                                normalize_log(ms_31[:587]), md_56, md_31[:587], exponent = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70921d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69896aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc877397",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_values_dict['56-31'] = r_value\n",
    "p_values_dict['56-31'] = p_value\n",
    "print('R values:', r_values_dict)\n",
    "print('P values:', p_values_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e056917",
   "metadata": {},
   "source": [
    "## 30-31 correlation\n",
    "Rogue Canyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7838f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 30 log needs a lot of fixing; this way it matches Figure 33 in Goldfinger et al. 2012 \n",
    "md_30 = np.hstack((md_30[:615], md_30[715:]))\n",
    "ms_30 = np.hstack((ms_30[:615], ms_30[715:]))\n",
    "inds = np.where((md_30 > 400) & (md_30 < 500))[0]\n",
    "ms_30[inds] = ms_30[inds[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45314525",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, slope_std_error = correlate_and_plot_logs(normalize_log(ms_30[:747]), \n",
    "                                            normalize_log(ms_31[27:]), md_30[:747], md_31[27:], exponent = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16cbbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_values_dict['30-31'] = r_value\n",
    "p_values_dict['30-31'] = p_value\n",
    "print('R values:', r_values_dict)\n",
    "print('P values:', p_values_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d10f13",
   "metadata": {},
   "source": [
    "## Randomization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f9349b",
   "metadata": {},
   "source": [
    "### Create 'database' of individual turbidites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42cd8811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onclick_boundary(event, xs, fig):\n",
    "    \"\"\"function for collecting depth values for a stratigraphic top from a chronostratigraphic diagram\n",
    "\n",
    "    :param event: mouse click event\n",
    "    :param xs: x coordinates of points clicked in cross section\n",
    "    :param ys: y coordinates (depth values) of points clicked in cross section\n",
    "    :param fig: figure handle of cross section\"\"\"\n",
    "    x1 = event.xdata\n",
    "    xs.append(x1)\n",
    "    ax = fig.axes[0]\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    ax.plot([x1, x1], [-30, 600], 'r')\n",
    "    fig.canvas.draw_idle()\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "def create_figure_for_clicking(md, log, miny, maxy):\n",
    "    fig = plt.figure(figsize=(18,3))\n",
    "    plt.plot(md, log)\n",
    "    plt.ylim(miny, maxy)\n",
    "    plt.xlim(md[0], md[-1])\n",
    "    plt.xlabel('depth(cm)')\n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcfd960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for manually interpreting turbidite tops (see collected data below)\n",
    "fig = create_figure_for_clicking(md_31, ms_31, -30, 600)\n",
    "xs = []\n",
    "cid = fig.canvas.mpl_connect('button_press_event', lambda event: onclick_boundary(event, xs, fig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20dadb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tops_22 = [0.0, 13.040143369175638, 43.848745519713276, 91.6551971326165, 129.19211469534054, \n",
    "           175.5820788530466, 231.53333333333336, 273.673835125448, 307.31541218637994, \n",
    "           367.5161290322581, 403.28243727598567, 439.4028673835126, 486.50107526881726]\n",
    "\n",
    "tops_23 = [7.784946236559151, 30.40143369175628, 73.37275985663082, 112.27311827956989, \n",
    "           155.69677419354838, 209.97634408602153, 261.089605734767, 318.9878136200717, \n",
    "           376.88602150537633, 426.64229390681, 463.28100358422944, 492.68243727598565, \n",
    "           518.0129032258064, 533.8444444444444, 559.6272401433691, 587.6716845878136, \n",
    "           618.8824372759857, 632]\n",
    "\n",
    "tops_25 = [0, 47.103225806451604, 111.98924731182797, 171.752688172043, 240.05376344086022, \n",
    "           308.92401433691754, 380.64014336917563, 427.8817204301076, 476.8308243727599, \n",
    "           506.9971326164875, 529.7641577060931, 559.9304659498207, 615.7096774193549, \n",
    "           662.9512544802867, 684.5799283154122, 706.7777777777778, 743.2050179211469, \n",
    "           780.7706093189963]\n",
    "\n",
    "tops_56 = [0, 80.721146953405, 104.02795698924729, 176.2222222222222, 218.2881720430107, \n",
    "           302.4200716845878, 333.6853046594981, 435.43942652329747, 525.2559139784946, \n",
    "           567.3218637992832, 609.9562724014337, 643.495340501792, 655.4329749103943, \n",
    "           664.5283154121863, 685.5612903225806, 724.2164874551971, 781.6308243727598]\n",
    "\n",
    "tops_30 = [0, 15.574193548387086, 44.419354838709666, 96.4537634408602, 129.82365591397848, \n",
    "           163.19354838709677, 206.74408602150538, 243.50752688172042, 288.18924731182796, \n",
    "           341.92043010752684, 376.9870967741935, 431.8494623655914, 462.95698924731175, \n",
    "           478.79354838709673, 537.6150537634409, 574.3784946236559, 594.174193548387, \n",
    "           616.2322580645161, 639.421505376344, 667.1354838709677]\n",
    "\n",
    "tops_31 = [0, 17.430107526881727, 39.88458781362007, 65.62508960573479, 113.82007168458782, \n",
    "           143.941935483871, 174.0637992831541, 210.75770609318997, 250.1899641577061, \n",
    "           286.33620071684595, 354.247311827957, 394.77491039426525, 441.8745519713261, \n",
    "           481.3068100358423, 522.3820788530466, 555.24229390681, 573.863082437276, \n",
    "           595.2222222222222, 641.226523297491, 664.2286738351255, 680.6587813620072, 700.3749103942653]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7468bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs = [0] + xs\n",
    "# tops_31 = xs\n",
    "print(tops_31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13625806",
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_logs = []\n",
    "depth_logs = []\n",
    "log_number = []\n",
    "\n",
    "def add_turbidites_to_database(turb_logs, depth_logs, log_number, log, md, tops):\n",
    "    count = 0\n",
    "    for i in range(len(tops) - 1):\n",
    "        turb_logs.append(log[(md < tops[i+1]) & (md >= tops[i])])\n",
    "        depth_log = md[(md < tops[i+1]) & (md >= tops[i])]\n",
    "        depth_log = depth_log - min(depth_log)\n",
    "        depth_logs.append(depth_log)\n",
    "        log_number.append(count)\n",
    "        count += 1\n",
    "\n",
    "add_turbidites_to_database(turb_logs, depth_logs, log_number, ms_22, md_22, tops_22)\n",
    "add_turbidites_to_database(turb_logs, depth_logs, log_number, ms_23, md_23, tops_23)\n",
    "add_turbidites_to_database(turb_logs, depth_logs, log_number, ms_25, md_25, tops_25)\n",
    "add_turbidites_to_database(turb_logs, depth_logs, log_number, ms_56, md_56, tops_56)\n",
    "add_turbidites_to_database(turb_logs, depth_logs, log_number, ms_30, md_30, tops_30)\n",
    "add_turbidites_to_database(turb_logs, depth_logs, log_number, ms_31, md_31, tops_31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8cf831",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(depth_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f16ba190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logs_w_turb_boundaries(md, log, tops):\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    plt.plot(md, normalize_log(log), '.-')\n",
    "    plt.ylim(0, 1)\n",
    "    count = 0\n",
    "    for i in range(len(tops)):\n",
    "        plt.plot([tops[i], tops[i]], [0, 1], 'r' )\n",
    "        plt.text(tops[i], 0.2, str(count))\n",
    "        count += 1\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7665f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how the tops look like when plotted on the log\n",
    "fig = plot_logs_w_turb_boundaries(md_23, ms_23, tops_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3698f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def process_single_shuffled_iteration(i, md, log, thickness, exponent, same_log_turb_logs, same_log_depth_logs):\n",
    "    log1 = normalize_log(log)\n",
    "    d1 = md\n",
    "    fake_log = np.array([])\n",
    "    md_log = np.array([])\n",
    "    max_depth = 0\n",
    "    while max_depth <= thickness:\n",
    "        ind = random.choices(np.arange(len(same_log_turb_logs)), k=1)[0]\n",
    "        fake_log = np.hstack((fake_log, same_log_turb_logs[ind].flatten()))\n",
    "        if len(md_log) == 0:\n",
    "            md_log = np.hstack((md_log, 1 + same_log_depth_logs[ind]))\n",
    "        else:\n",
    "            md_log = np.hstack((md_log, 1 + md_log[-1] + same_log_depth_logs[ind]))\n",
    "        max_depth = md_log[-1]\n",
    "    log2 = normalize_log(fake_log)\n",
    "    d2 = md_log\n",
    "    p, q, D = correlate_logs(log1, log2, exponent)\n",
    "    vsh_means1 = log1[p]\n",
    "    vsh_means1[np.where(np.diff(p)==0)[0] + 1] = np.nan\n",
    "    vsh_means2 = log2[q]\n",
    "    vsh_means2[np.where(np.diff(q)==0)[0] + 1] = np.nan\n",
    "    slope, intercept, r_value, p_value, slope_std_rror = stats.linregress(vsh_means1[(np.isnan(vsh_means1)==0) & (np.isnan(vsh_means2)==0)], \n",
    "                                                                      vsh_means2[(np.isnan(vsh_means1)==0) & (np.isnan(vsh_means2)==0)])\n",
    "    return r_value\n",
    "\n",
    "def correlate_log_w_shuffled_log(md, log, tops, thickness, nit, exponent):\n",
    "    \"\"\"\n",
    "    Correlate original log with shuffled synthetic logs created from the same log's turbidites.\n",
    "    This version uses only the turbidites from the same log to create shuffled synthetic logs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    md : array-like\n",
    "        Measured depth array for the original log\n",
    "    log : array-like\n",
    "        Original log data\n",
    "    tops : array-like\n",
    "        Turbidite boundaries for the original log\n",
    "    thickness : float\n",
    "        Target thickness for synthetic logs\n",
    "    nit : int\n",
    "        Number of iterations (synthetic logs to generate)\n",
    "    exponent : float\n",
    "        Exponent parameter for DTW correlation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of correlation coefficients (r_values) from each iteration\n",
    "    \"\"\"\n",
    "    from joblib import Parallel, delayed\n",
    "    \n",
    "    # Create turbidite database from the same log only\n",
    "    same_log_turb_logs = []\n",
    "    same_log_depth_logs = []\n",
    "    \n",
    "    for i in range(len(tops) - 1):\n",
    "        turb_segment = log[(md < tops[i+1]) & (md >= tops[i])]\n",
    "        depth_segment = md[(md < tops[i+1]) & (md >= tops[i])]\n",
    "        depth_segment = depth_segment - min(depth_segment)\n",
    "        same_log_turb_logs.append(turb_segment)\n",
    "        same_log_depth_logs.append(depth_segment)\n",
    "    \n",
    "    # Use joblib for parallel processing\n",
    "    r_values = Parallel(n_jobs=-1)(\n",
    "        delayed(process_single_shuffled_iteration)(\n",
    "            i, md, log, thickness, exponent, same_log_turb_logs, same_log_depth_logs\n",
    "        ) for i in trange(nit)\n",
    "    )\n",
    "    \n",
    "    return r_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad8c2ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def process_single_iteration(i, md, log, thickness, exponent, turb_logs, depth_logs):\n",
    "    log1 = normalize_log(log)\n",
    "    d1 = md\n",
    "    fake_log = np.array([])\n",
    "    md_log = np.array([])\n",
    "    max_depth = 0\n",
    "    while max_depth <= thickness:\n",
    "        ind = random.choices(np.arange(len(turb_logs)), k=1)[0]\n",
    "        fake_log = np.hstack((fake_log, turb_logs[ind].flatten()))\n",
    "        if len(md_log) == 0:\n",
    "            md_log = np.hstack((md_log, 1 + depth_logs[ind]))\n",
    "        else:\n",
    "            md_log = np.hstack((md_log, 1 + md_log[-1] + depth_logs[ind]))\n",
    "        max_depth = md_log[-1]\n",
    "    log2 = normalize_log(fake_log)\n",
    "    d2 = md_log\n",
    "    p, q, D = correlate_logs(log1, log2, exponent)\n",
    "    vsh_means1 = log1[p]\n",
    "    vsh_means1[np.where(np.diff(p)==0)[0] + 1] = np.nan\n",
    "    vsh_means2 = log2[q]\n",
    "    vsh_means2[np.where(np.diff(q)==0)[0] + 1] = np.nan\n",
    "    slope, intercept, r_value, p_value, slope_std_rror = stats.linregress(vsh_means1[(np.isnan(vsh_means1)==0) & (np.isnan(vsh_means2)==0)], \n",
    "                                                                      vsh_means2[(np.isnan(vsh_means1)==0) & (np.isnan(vsh_means2)==0)])\n",
    "    return r_value\n",
    "\n",
    "def correlate_log_w_synthetic_logs(md, log, thickness, nit, exponent, turb_logs, depth_logs):\n",
    "    from joblib import Parallel, delayed\n",
    "    \n",
    "    # Use joblib for parallel processing\n",
    "    r_values = Parallel(n_jobs=-1)(\n",
    "        delayed(process_single_iteration)(\n",
    "            i, md, log, thickness, exponent, turb_logs, depth_logs\n",
    "        ) for i in trange(nit)\n",
    "    )\n",
    "    \n",
    "    return r_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "917ca72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_synth_log_w_synth_logs(thickness, nit, exponent, turb_logs, depth_logs):\n",
    "    \"\"\"\n",
    "    correlate synthetic log pairs\n",
    "    \"\"\"\n",
    "    from joblib import Parallel, delayed\n",
    "    \n",
    "    # Use joblib for parallel processing\n",
    "    r_values = Parallel(n_jobs=-1)(\n",
    "        delayed(process_synthetic_iteration)(\n",
    "            i, thickness, exponent, turb_logs, depth_logs\n",
    "        ) for i in trange(nit)\n",
    "    )\n",
    "    \n",
    "    return r_values\n",
    "\n",
    "def process_synthetic_iteration(i, thickness, exponent, turb_logs, depth_logs):\n",
    "    # create first log:\n",
    "    log1, d1, inds = create_synthetic_log(thickness, turb_logs, depth_logs)\n",
    "    \n",
    "    # create second log with different turbidites:\n",
    "    log2, d2, _ = create_synthetic_log(thickness, turb_logs, depth_logs, exclude_inds=inds)\n",
    "    \n",
    "    # correlate the logs\n",
    "    p, q, D = correlate_logs(log1, log2, exponent)\n",
    "    \n",
    "    # calculate statistics\n",
    "    vsh_means1 = log1[p]\n",
    "    vsh_means1[np.where(np.diff(p)==0)[0] + 1] = np.nan\n",
    "    vsh_means2 = log2[q]\n",
    "    vsh_means2[np.where(np.diff(q)==0)[0] + 1] = np.nan\n",
    "    \n",
    "    slope, intercept, r_value, p_value, slope_std_rror = stats.linregress(\n",
    "        vsh_means1[(np.isnan(vsh_means1)==0) & (np.isnan(vsh_means2)==0)], \n",
    "        vsh_means2[(np.isnan(vsh_means1)==0) & (np.isnan(vsh_means2)==0)]\n",
    "    )\n",
    "        \n",
    "    return r_value\n",
    "\n",
    "def create_synthetic_log(thickness, turb_logs, depth_logs, exclude_inds=None):\n",
    "    fake_log = np.array([])\n",
    "    md_log = np.array([])\n",
    "    max_depth = 0\n",
    "    inds = []\n",
    "    \n",
    "    while max_depth <= thickness:\n",
    "        ind = random.choices(np.arange(len(turb_logs)), k=1)[0]\n",
    "        \n",
    "        # Skip if this index should be excluded\n",
    "        if exclude_inds is not None and ind in exclude_inds:\n",
    "            continue\n",
    "            \n",
    "        inds.append(ind)\n",
    "        fake_log = np.hstack((fake_log, turb_logs[ind].flatten()))\n",
    "        \n",
    "        if len(md_log) == 0:\n",
    "            md_log = np.hstack((md_log, 1 + depth_logs[ind]))\n",
    "        else:\n",
    "            md_log = np.hstack((md_log, 1 + md_log[-1] + depth_logs[ind]))\n",
    "            \n",
    "        max_depth = md_log[-1]\n",
    "        \n",
    "    log = normalize_log(fake_log)\n",
    "    d = md_log.copy()\n",
    "    \n",
    "    return log, d, inds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9fe41ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "\n",
    "def plot_turbidite_correlation_distribution(r_values_data, data_type=\"synthetic_to_synthetic\", \n",
    "                                          actual_correlations=None,\n",
    "                                          save_figure=True, figure_name=None,\n",
    "                                          core_name=None, save_csv=False, csv_name=None,\n",
    "                                          pdf_method='KDE', kde_bandwidth=0.05):\n",
    "    \"\"\"\n",
    "    Plot distribution of correlation coefficients from turbidite correlation analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    r_values_data : list or array\n",
    "        List/array of correlation coefficients (r-values) from randomization analysis\n",
    "    data_type : str\n",
    "        Type of data being plotted (\"synthetic_to_synthetic\", \"log_vs_synthetic\", \"log_to_shuffled\", etc.)\n",
    "    actual_correlations : dict, optional\n",
    "        Dictionary of actual core correlation r-values (e.g., r_values_dict from notebook)\n",
    "    save_figure : bool\n",
    "        Whether to save the figure as PNG\n",
    "    figure_name : str, optional\n",
    "        Custom filename for saved figure\n",
    "    core_name : str, optional\n",
    "        Name of the core when comparing actual logs to synthetic logs\n",
    "    save_csv : bool\n",
    "        Whether to save the r_values_data as CSV file\n",
    "    csv_name : str, optional\n",
    "        Custom filename for saved CSV file\n",
    "    pdf_method : str\n",
    "        Method for probability density function overlay: 'KDE' (default), 'skew-normal', or 'normal'\n",
    "    kde_bandwidth : float\n",
    "        Bandwidth for KDE when pdf_method='KDE' (default: 0.05)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fig, ax : matplotlib figure and axes objects\n",
    "    fit_params : dict\n",
    "        Dictionary containing fitted parameters (location, scale, shape for skew-normal; mean, std for normal; bandwidth for KDE)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to numpy array for easier handling\n",
    "    r_values = np.array(r_values_data)\n",
    "    \n",
    "    # Remove any NaN values\n",
    "    r_values = r_values[~np.isnan(r_values)]\n",
    "    \n",
    "    # Save CSV if requested\n",
    "    if save_csv:\n",
    "        if csv_name is None:\n",
    "            csv_name = f'r_values_{data_type}.csv'\n",
    "        \n",
    "        # Create DataFrame and save\n",
    "        df = pd.DataFrame({'r_values': r_values})\n",
    "        df.to_csv(csv_name, index=False)\n",
    "        print(f\"R-values data saved as: {csv_name}\")\n",
    "    \n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Calculate total count for percentage\n",
    "    total_count = len(r_values)\n",
    "    \n",
    "    # Plot histogram of correlation coefficients as percentage\n",
    "    hist, bins, _ = ax.hist(r_values, bins=50, alpha=0.7, color='skyblue', \n",
    "                            edgecolor='black', weights=np.ones(total_count)*100/total_count)\n",
    "    \n",
    "    # Initialize fit_params dictionary\n",
    "    fit_params = {}\n",
    "    \n",
    "    # Add probability density function curve based on method\n",
    "    if len(r_values) > 1:  # Only plot PDF if we have multiple values\n",
    "        x = np.linspace(r_values.min(), r_values.max(), 1000)\n",
    "        \n",
    "        # Calculate area under the histogram in percentage terms\n",
    "        bin_width = bins[1] - bins[0]\n",
    "        hist_area = np.sum(hist) * bin_width\n",
    "        \n",
    "        if pdf_method.upper() == 'KDE':\n",
    "            # Use Kernel Density Estimation (current method)\n",
    "            kde = stats.gaussian_kde(r_values, bw_method=kde_bandwidth)\n",
    "            y = kde(x) * hist_area\n",
    "            fit_params['method'] = 'KDE'\n",
    "            fit_params['bandwidth'] = kde_bandwidth\n",
    "            \n",
    "            ax.plot(x, y, 'r-', linewidth=2, alpha=0.8, \n",
    "                    label=f'KDE\\n(bandwidth = {kde_bandwidth})\\n(n = {total_count:,})')\n",
    "        \n",
    "        elif pdf_method.upper() == 'SKEW-NORMAL':\n",
    "            # Fit skew-normal distribution\n",
    "            try:\n",
    "                # Fit skew-normal distribution using maximum likelihood estimation\n",
    "                # Let scipy determine the best fit parameters automatically\n",
    "                shape, location, scale = stats.skewnorm.fit(r_values)\n",
    "                \n",
    "                # Generate PDF\n",
    "                y = stats.skewnorm.pdf(x, shape, location, scale) * hist_area\n",
    "                \n",
    "                fit_params['method'] = 'skew-normal'\n",
    "                fit_params['shape'] = shape\n",
    "                fit_params['location'] = location\n",
    "                fit_params['scale'] = scale\n",
    "                fit_params['skewness'] = shape / np.sqrt(1 + shape**2) * np.sqrt(2/np.pi)\n",
    "                \n",
    "                ax.plot(x, y, 'r-', linewidth=2, alpha=0.8, \n",
    "                        label=f'Skew-Normal Fit\\n(α = {shape:.3f})\\n(μ = {location:.3f})\\n(σ = {scale:.3f})\\n(n = {total_count:,})')\n",
    "                \n",
    "            except (RuntimeError, ValueError, TypeError) as e:\n",
    "                print(f\"Warning: Skew-normal fitting failed: {e}\")\n",
    "                print(\"Falling back to normal distribution fit\")\n",
    "                pdf_method = 'normal'\n",
    "        \n",
    "        if pdf_method.upper() == 'NORMAL':\n",
    "            # Fit normal distribution\n",
    "            mean_val, std_val = stats.norm.fit(r_values)\n",
    "            \n",
    "            # Generate PDF\n",
    "            y = stats.norm.pdf(x, mean_val, std_val) * hist_area\n",
    "            \n",
    "            fit_params['method'] = 'normal'\n",
    "            fit_params['mean'] = mean_val\n",
    "            fit_params['std'] = std_val\n",
    "            \n",
    "            ax.plot(x, y, 'r-', linewidth=2, alpha=0.8, \n",
    "                    label=f'Normal Fit\\n(μ = {mean_val:.3f})\\n(σ = {std_val:.3f})\\n(n = {total_count:,})')\n",
    "    \n",
    "    # Calculate statistics for legend\n",
    "    median_corr = np.median(r_values)\n",
    "    mean_corr = np.mean(r_values)\n",
    "    std_corr = np.std(r_values)\n",
    "    \n",
    "    # Add vertical line for median\n",
    "    ax.axvline(median_corr, color='b', linestyle='-', linewidth=2, \n",
    "               label=f'Median: {median_corr:.3f}')\n",
    "    \n",
    "    # Add vertical line for mean\n",
    "    ax.axvline(mean_corr, color='green', linestyle='-', linewidth=2,\n",
    "               label=f'Mean: {mean_corr:.3f}\\nσ: {std_corr:.3f}')\n",
    "    \n",
    "    # Add vertical line for 97.5th percentile (significance threshold)\n",
    "    percentile_975 = np.percentile(r_values, 97.5)\n",
    "    ax.axvline(percentile_975, color='k', linestyle='dashed', linewidth=2,\n",
    "               label=f'P97.5: {percentile_975:.3f}')\n",
    "    \n",
    "    # Track legend entries for significance\n",
    "    significant_pairs = []\n",
    "    non_significant_pairs = []\n",
    "    \n",
    "    # If actual correlations are provided, plot them\n",
    "    if actual_correlations is not None:\n",
    "        for pair_name, r_value in actual_correlations.items():\n",
    "            if isinstance(r_value, (int, float)) and not np.isnan(r_value):\n",
    "                # Calculate percentile for this correlation\n",
    "                percentile = (r_values < r_value).mean() * 100\n",
    "                \n",
    "                # Determine significance\n",
    "                is_significant = r_value > percentile_975\n",
    "                \n",
    "                if is_significant:\n",
    "                    linestyle = 'solid'\n",
    "                    alpha = 0.85\n",
    "                    significant_pairs.append(pair_name)\n",
    "                else:\n",
    "                    linestyle = 'dotted'\n",
    "                    alpha = 0.6\n",
    "                    non_significant_pairs.append(pair_name)\n",
    "                \n",
    "                # Plot the line\n",
    "                line = ax.axvline(r_value, color='orange', linestyle=linestyle, \n",
    "                                 linewidth=2, alpha=alpha)\n",
    "                \n",
    "                # Add text annotation for pair name along the line\n",
    "                y_pos = ax.get_ylim()[1] * 0.9  # Position near top of plot\n",
    "                ax.text(r_value, y_pos, pair_name, rotation=90, \n",
    "                       verticalalignment='top', horizontalalignment='center',\n",
    "                       fontsize=8, alpha=alpha, color='darkred', fontweight='normal',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.9))\n",
    "    \n",
    "    # Add legend entries for significance levels\n",
    "    if significant_pairs:\n",
    "        # Create a dummy line for legend\n",
    "        ax.plot([], [], color='orange', linestyle='solid', linewidth=1.5, alpha=0.85,\n",
    "               label=f'p < 0.05$^{{**}}$')\n",
    "    \n",
    "    if non_significant_pairs:\n",
    "        # Create a dummy line for legend\n",
    "        ax.plot([], [], color='orange', linestyle='dotted', linewidth=1.5, alpha=0.6,\n",
    "               label=f'p ≥ 0.05')\n",
    "    \n",
    "    # Set x-axis limits\n",
    "    x_min = 0\n",
    "    x_max = 1.0\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('Correlation Coefficient (r)', fontsize=12)\n",
    "    ax.set_ylabel('Percentage (%)', fontsize=12)\n",
    "    \n",
    "    if data_type == \"synthetic_to_synthetic\":\n",
    "        title = 'Distribution of Correlation Coefficients\\nSynthetic Log Pairs (Randomization Test)'\n",
    "    elif data_type == \"log_vs_synthetic\":\n",
    "        if core_name:\n",
    "            title = f'Distribution of Correlation Coefficients\\nActual Log ({core_name}) vs Synthetic Logs'\n",
    "        else:\n",
    "            title = 'Distribution of Correlation Coefficients\\nActual Logs vs Synthetic Logs'\n",
    "    elif data_type == \"log_to_shuffled\":\n",
    "        if core_name:\n",
    "            title = f'Distribution of Correlation Coefficients\\nActual Log ({core_name}) vs Shuffled Log'\n",
    "        else:\n",
    "            title = 'Distribution of Correlation Coefficients\\nActual Log vs Shuffled Log'\n",
    "    else:\n",
    "        title = f'Distribution of Correlation Coefficients\\n{data_type}'\n",
    "    \n",
    "    plt.title(title, fontsize=14, pad=20)\n",
    "    \n",
    "    # Create legend with better positioning\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save the figure if requested\n",
    "    if save_figure:\n",
    "        if figure_name is None:\n",
    "            figure_name = f'turbidite_correlation_distribution_{data_type}_{pdf_method.lower()}.png'\n",
    "        \n",
    "        plt.savefig(figure_name, dpi=300, bbox_inches='tight', \n",
    "                   facecolor='white', edgecolor='none')\n",
    "        print(f\"Figure saved as: {figure_name}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nSummary Statistics for {data_type} Correlation Coefficients:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Number of correlations: {total_count:,}\")\n",
    "    print(f\"Median: {median_corr:.3f}\")\n",
    "    print(f\"Mean: {mean_corr:.3f}\")\n",
    "    print(f\"Standard Deviation: {std_corr:.3f}\")\n",
    "    print(f\"Min: {r_values.min():.3f}\")\n",
    "    print(f\"Max: {r_values.max():.3f}\")\n",
    "    print(f\"95th percentile: {percentile_975:.3f}\")\n",
    "    print(f\"99th percentile: {np.percentile(r_values, 99):.3f}\")\n",
    "    \n",
    "    # Print distribution fitting results\n",
    "    if fit_params:\n",
    "        print(f\"\\nDistribution Fitting Results ({fit_params['method']}):\")\n",
    "        print(f\"{'='*60}\")\n",
    "        if fit_params['method'] == 'KDE':\n",
    "            print(f\"Bandwidth: {fit_params['bandwidth']} (Controls smoothness of the density curve)\")\n",
    "        elif fit_params['method'] == 'skew-normal':\n",
    "            print(f\"Shape parameter (α): {fit_params['shape']:.5f} (Asymmetry; α=0 gives normal distribution; positive α = right skew, negative α = left skew; magnitude indicates strength of skew)\")\n",
    "            print(f\"Location parameter (μ): {fit_params['location']:.5f} (Center/peak position of the distribution; larger magnitude shifts distribution)\")\n",
    "            print(f\"Scale parameter (σ): {fit_params['scale']:.5f} (Width/spread of the distribution; magnitude indicates the degree of spread)\")\n",
    "            print(f\"Skewness: {fit_params['skewness']:.5f} (Measure of asymmetry; positive = longer right tail; magnitude indicates severity of skew)\")\n",
    "        elif fit_params['method'] == 'normal':\n",
    "            print(f\"Mean (μ): {fit_params['mean']:.5f} (Center of the distribution; magnitude indicates distance from zero)\")\n",
    "            print(f\"Standard deviation (σ): {fit_params['std']:.5f} (Width/spread of the distribution; larger magnitude increases variability)\")\n",
    "    \n",
    "    # If actual correlations are provided, show their percentiles and significance\n",
    "    if actual_correlations is not None:\n",
    "        print(f\"\\nActual Core Pair Correlations vs {data_type} Distribution:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for pair_name, r_value in actual_correlations.items():\n",
    "            if isinstance(r_value, (int, float)) and not np.isnan(r_value):\n",
    "                percentile = (r_values < r_value).mean() * 100\n",
    "                p_value = (r_values >= r_value).mean()\n",
    "                significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
    "                print(f\"{pair_name:<12}: r = {r_value:.5f}, {percentile:5.1f}th percentile, p = {p_value:.3f} {significance}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig, ax, fit_params\n",
    "\n",
    "\n",
    "def plot_comparison_with_actual_correlations(r_values_synthetic, actual_correlations_dict):\n",
    "    \"\"\"\n",
    "    Create a comparison plot showing both synthetic and actual correlations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    r_values_synthetic : array-like\n",
    "        Array of synthetic correlation coefficients\n",
    "    actual_correlations_dict : dict\n",
    "        Dictionary of actual core pair correlations from the notebook\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example usage with the data from the notebook\n",
    "    fig, ax = plot_turbidite_correlation_distribution(\n",
    "        r_values_data=r_values_synthetic,\n",
    "        data_type=\"synthetic_pairs\",\n",
    "        actual_correlations=actual_correlations_dict,\n",
    "        save_figure=True,\n",
    "        figure_name=\"cascadia_turbidite_correlations_vs_synthetic.png\"\n",
    "    )\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dca96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_numbers = [\"22\", \"23\", \"25\", \"11\", \"12\", \"14\", \"56\", \"02\", \"30\", \"31\", \"09\"]\n",
    "core_numbers = [\"22\", \"23\", \"25\"]\n",
    "\n",
    "for core_number in core_numbers:\n",
    "    md_var = f\"md_{core_number}\"\n",
    "    ms_var = f\"ms_{core_number}\" \n",
    "    tops_var = f\"tops_{core_number}\"\n",
    "    r_values_logshuffled = correlate_log_w_shuffled_log(locals()[md_var], locals()[ms_var], locals()[tops_var], 700, 10000, 0.3)\n",
    "\n",
    "    core_name = f\"core_name_{core_number}\"\n",
    "\n",
    "    # Plot log-shuffled correlations\n",
    "    fig3, ax3, fit_params3 = plot_turbidite_correlation_distribution(\n",
    "        r_values_data=r_values_logshuffled,\n",
    "        data_type=\"log_to_shuffled\",     #log_to_shuffled, log_to_synthetic, synthetic_to_synthetic\n",
    "        pdf_method='skew-normal',        #'KDE', 'skew-normal', 'normal'\n",
    "        kde_bandwidth=0.1,               #only work for KDE pdf_method\n",
    "        save_figure=True,\n",
    "        figure_name=f\"outputs/r-values_CSZ_Null_Hypothesis_logshuffled_{locals()[core_name]}.png\",\n",
    "        core_name=locals()[core_name],\n",
    "        save_csv=True,\n",
    "        csv_name=f\"outputs/r-values_CSZ_Null_Hypothesis_logshuffled_{locals()[core_name]}.csv\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d87ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_numbers = [\"22\", \"23\", \"25\", \"11\", \"12\", \"14\", \"56\", \"02\", \"30\", \"31\", \"09\"]\n",
    "core_numbers = [\"22\", \"23\", \"25\"]\n",
    "\n",
    "for core_number in core_numbers:\n",
    "    md_var = f\"md_{core_number}\"\n",
    "    ms_var = f\"ms_{core_number}\" \n",
    "    r_values_logsyn = correlate_log_w_synthetic_logs(locals()[md_var], locals()[ms_var], np.max(locals()[md_var]), 10000, 0.3, turb_logs, depth_logs)\n",
    "\n",
    "    core_name = f\"core_name_{core_number}\"\n",
    "\n",
    "    # Plot log-synthetic correlations\n",
    "    fig2, ax2, fit_params2 = plot_turbidite_correlation_distribution(\n",
    "        r_values_data=r_values_logsyn,\n",
    "        data_type=\"log_vs_synthetic\",   #log_to_shuffled, log_to_synthetic, synthetic_to_synthetic\n",
    "        pdf_method='skew-normal',               #'KDE', 'skew-normal', 'normal'\n",
    "        kde_bandwidth=0.1,              #only work for KDE pdf_method\n",
    "        save_figure=True,\n",
    "        figure_name=f\"outputs/r-values_CSZ_Null_Hypothesis_logsyn_{locals()[core_name]}.png\",\n",
    "        core_name=locals()[core_name],\n",
    "        save_csv=True,\n",
    "        csv_name=f\"outputs/r-values_CSZ_Null_Hypothesis_logsyn_{locals()[core_name]}.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e94e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute r values for 1000000 synthetic log pairs (this is better than correlating actual logs with synthetic logs)\n",
    "r_values_synsyn = correlate_synth_log_w_synth_logs(700, 10000, 0.3, turb_logs, depth_logs)\n",
    "\n",
    "# plot the distribution of r values\n",
    "fig1, ax1, fit_params1 = plot_turbidite_correlation_distribution(\n",
    "    r_values_data=r_values_synsyn,\n",
    "    data_type=\"synthetic_to_synthetic\",  #log_to_shuffled, log_to_synthetic, synthetic_to_synthetic\n",
    "    pdf_method='skew-normal',                    #'KDE', 'skew-normal', 'normal'\n",
    "    kde_bandwidth=0.1,                   #only work for KDE pdf_method\n",
    "    save_figure=True,\n",
    "    figure_name=\"outputs/r-values_CSZ_Null_Hypothesis_synsyn.png\",\n",
    "    save_csv=True,\n",
    "    csv_name=\"outputs/r-values_CSZ_Null_Hypothesis_synsyn.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84896584",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(r_values_dict, index = ['r values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(turb_logs) # 90 turbidites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "37c5b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datafrme of r values of core correlations\n",
    "df = pd.DataFrame(r_values_dict, index = ['r values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb34d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb61fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('OSU_dataset/Cascade_well_list.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e703be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['x (m)'][df2['wellname'] == 'M9907-23PC'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3185bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist(wname1, wname2, df):\n",
    "    # compute distnace between two cores\n",
    "    x1 = df['x (m)'][df['wellname'] == wname1].values[0]\n",
    "    y1 = df['y (m)'][df['wellname'] == wname1].values[0]\n",
    "    x2 = df['x (m)'][df['wellname'] == wname2].values[0]\n",
    "    y2 = df['y (m)'][df['wellname'] == wname2].values[0]\n",
    "    dist = ((x2 - x1)**2 + (y2 - y1)**2)**0.5\n",
    "    return dist\n",
    "\n",
    "compute_dist('M9907-22PC', 'M9907-23PC', df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9cfc2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of core names for correlated core pairs\n",
    "wnames1 = ['M9907-22PC', 'M9907-23PC', 'M9907-25PC', 'M9907-23PC', 'M9907-12PC', 'M9907-23PC', 'RR0207-56PC', 'M9907-30PC']\n",
    "wnames2 = ['M9907-23PC', 'M9907-25PC', 'M9907-22PC', 'M9907-12PC', 'M9907-11PC', 'M9907-14TC', 'M9907-31PC', 'M9907-31PC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb07c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distances between core pairs\n",
    "dists = {}\n",
    "for wname1, wname2, colname in zip(wnames1, wnames2, df.columns):\n",
    "    dists[colname] = compute_dist(wname1, wname2, df2)\n",
    "dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07acaffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe of distances\n",
    "df3 = pd.DataFrame(dists, index = ['distance (m)'])\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f030a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add distances to dataframe of r values\n",
    "df = pd.concat([df, df3], axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5d1aa239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comoute p values\n",
    "r_values = np.array(r_values_synsyn)\n",
    "p_values = {}\n",
    "for colname, r_value in zip(df.columns, df.iloc[0]):\n",
    "    p_values[colname] = len(r_values[r_values > r_value])/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a452783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(p_values, index = ['p values'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f84943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add p values to dataframe\n",
    "df = pd.concat([df, df2], axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e293e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final version of dataframe\n",
    "df = df.transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(r_values, 50)\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "for r_value in df['r values']:\n",
    "    plt.plot([r_value, r_value], [0, plt.gca().get_ylim()[1]], color=cmap(1), linewidth = 2)\n",
    "plt.xlim(0, 0.9)\n",
    "plt.ylim(0, plt.gca().get_ylim()[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aeb242e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('p_values_of_correlations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a02d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(df['distance (m)'], df['p values'])\n",
    "plt.plot([-10000, 320000], [0.05, 0.05], '--', color=cmap(1))\n",
    "plt.xlim(-10000, 320000)\n",
    "plt.xlabel('distance (m)')\n",
    "plt.ylabel('p value');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
