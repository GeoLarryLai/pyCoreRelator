{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25kTe2nS84Tf"
   },
   "source": [
    "# Correlating well log pairs: Complex Dynamic Time Warping with boundary constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4awuK6ffO2t"
   },
   "source": [
    "## Introduction to dynamic time warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47Kk07X_84Th"
   },
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import glob\n",
    "from IPython.display import Image as IPImage, display\n",
    "from scipy import stats\n",
    "from matplotlib.lines import Line2D\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyCoreRelator import (\n",
    "    run_comprehensive_dtw_analysis,\n",
    "    find_complete_core_paths,\n",
    "    calculate_interpolated_ages,\n",
    "    load_log_data,\n",
    "    plot_core_data,\n",
    "    plot_correlation_distribution,\n",
    "    load_core_age_constraints\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Test with Cascadia hi-res MS logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define core pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define core names as variables for easy reference\n",
    "# CORE_A = \"M9907-22PC\"\n",
    "CORE_A = \"M9907-23PC\"\n",
    "# CORE_A = \"M9907-11PC\"\n",
    "# CORE_A = \"RR0207-56PC\" \n",
    "\n",
    "# CORE_B = \"M9907-25PC\"\n",
    "CORE_B = \"M9907-11PC\"\n",
    "# CORE_B = \"RR0207-56PC\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data structures and core images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log columns to extract\n",
    "# LOG_COLUMNS = ['hiresMS', 'CT', 'R', 'G', 'B']  # Choose which logs to include\n",
    "# LOG_COLUMNS = ['hiresMS', 'CT', 'Lumin']  # Choose which logs to include\n",
    "LOG_COLUMNS = ['hiresMS']  # Choose which logs to include\n",
    "DEPTH_COLUMN = 'SB_DEPTH_cm'\n",
    "\n",
    "# Define directory paths\n",
    "mother_dir = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/'\n",
    "\n",
    "# Define paths for Core A\n",
    "core_a_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_MST_MLfilled.csv'\n",
    "}\n",
    "\n",
    "core_a_rgb_img_path = f\"{mother_dir}_compiled_logs/{CORE_A}/{CORE_A}_RGB.tiff\"\n",
    "core_a_ct_img_path = f\"{mother_dir}_compiled_logs/{CORE_A}/{CORE_A}_CT.tiff\"\n",
    "\n",
    "# Define paths for Core B\n",
    "core_b_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_MST_MLfilled.csv'\n",
    "}\n",
    "core_b_rgb_img_path = f\"{mother_dir}_compiled_logs/{CORE_B}/{CORE_B}_RGB.tiff\"\n",
    "core_b_ct_img_path = f\"{mother_dir}_compiled_logs/{CORE_B}/{CORE_B}_CT.tiff\"\n",
    "\n",
    "# Define column mapping for alternative column names\n",
    "column_alternatives = {\n",
    "    'hiresMS': ['MS'],\n",
    "    'CT': ['CT_value'],\n",
    "    'R': ['R', 'red', 'Red'],\n",
    "    'G': ['G', 'green', 'Green'],\n",
    "    'B': ['B', 'blue', 'Blue'],\n",
    "    'Lumin': ['luminance', 'Luminance'],\n",
    "    'Den_gm/cc': ['Density', 'density']\n",
    "}\n",
    "\n",
    "# Load data for Core A\n",
    "log_a, md_a, available_columns_a, rgb_img_a, ct_img_a = load_log_data(\n",
    "    core_a_log_paths,\n",
    "    {'rgb': core_a_rgb_img_path, 'ct': core_a_ct_img_path},\n",
    "    LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")\n",
    "\n",
    "print(\"\\n=== DEBUG: Core A Loading ===\")\n",
    "print(f\"LOG_COLUMNS requested: {LOG_COLUMNS}\")\n",
    "print(f\"Available columns loaded: {available_columns_a}\")\n",
    "print(f\"Shape of log_a: {log_a.shape}\")\n",
    "print(f\"Type of log_a: {type(log_a)}\")\n",
    "if hasattr(log_a, 'ndim'):\n",
    "    print(f\"log_a dimensions: {log_a.ndim}\")\n",
    "    if log_a.ndim > 1:\n",
    "        print(f\"log_a has {log_a.shape[1]} columns\\n\")\n",
    "    else:\n",
    "        print(\"log_a is 1D (single column)\\n\")\n",
    "\n",
    "# Load data for Core B\n",
    "log_b, md_b, available_columns_b, rgb_img_b, ct_img_b = load_log_data(\n",
    "    core_b_log_paths,\n",
    "    {'rgb': core_b_rgb_img_path, 'ct': core_b_ct_img_path},\n",
    "    LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")\n",
    "\n",
    "print(\"\\n=== DEBUG: Core B Loading ===\")\n",
    "print(f\"LOG_COLUMNS requested: {LOG_COLUMNS}\")\n",
    "print(f\"Available columns loaded: {available_columns_b}\")\n",
    "print(f\"Shape of log_b: {log_b.shape}\")\n",
    "print(f\"Type of log_b: {type(log_b)}\")\n",
    "if hasattr(log_b, 'ndim'):\n",
    "    print(f\"log_b dimensions: {log_b.ndim}\")\n",
    "    if log_b.ndim > 1:\n",
    "        print(f\"log_b has {log_b.shape[1]} columns\\n\")\n",
    "    else:\n",
    "        print(\"log_b is 1D (single column)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Define paths to the CSV files\n",
    "pickeddepth_a_csv = f'pickeddepth/{CORE_A}_pickeddepth.csv'\n",
    "pickeddepth_b_csv = f'pickeddepth/{CORE_B}_pickeddepth.csv'\n",
    "\n",
    "# Load picked depths and categories from CSV files\n",
    "if os.path.exists(pickeddepth_b_csv):\n",
    "    picked_data_b = pd.read_csv(pickeddepth_b_csv)\n",
    "    # Combine depths and categories into tuples\n",
    "    picked_b = list(zip(picked_data_b['picked_depths_cm'].values.tolist(), \n",
    "                        picked_data_b['category'].values.tolist()))\n",
    "    print(f\"Loaded {len(picked_b)} picked depths for {CORE_B}\")\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_b_csv} not found. Using empty list for picked_b.\")\n",
    "    picked_b = []\n",
    "\n",
    "if os.path.exists(pickeddepth_a_csv):\n",
    "    picked_data_a = pd.read_csv(pickeddepth_a_csv)\n",
    "    # Combine depths and categories into tuples\n",
    "    picked_a = list(zip(picked_data_a['picked_depths_cm'].values.tolist(), \n",
    "                        picked_data_a['category'].values.tolist()))\n",
    "    print(f\"Loaded {len(picked_a)} picked depths for {CORE_A}\")\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_a_csv} not found. Using empty list for picked_a.\")\n",
    "    picked_a = []\n",
    "\n",
    "# Create uncertainty arrays (assuming uncertainty size is 2 cm)\n",
    "picked_uncertainty_b = [1] * len(picked_b)\n",
    "picked_uncertainty_a = [1] * len(picked_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract depths and categories from the loaded tuples\n",
    "picked_depths_a = [depth for depth, category in picked_a] if picked_a else []\n",
    "picked_categories_a = [category for depth, category in picked_a] if picked_a else []\n",
    "\n",
    "picked_depths_b = [depth for depth, category in picked_b] if picked_b else []\n",
    "picked_categories_b = [category for depth, category in picked_b] if picked_b else []\n",
    "\n",
    "# Now plot the cores with enhanced plot_core_data function\n",
    "is_multilog = log_a.ndim > 1 and log_a.shape[1] > 1\n",
    "fig_a, ax_a = plot_core_data(\n",
    "    # Input data\n",
    "    md_a,                                           # depth array\n",
    "    log_a,                                          # log data array\n",
    "    f\"{CORE_A}\",                                    # core name\n",
    "    # Image data\n",
    "    rgb_img=rgb_img_a,                              # RGB image array\n",
    "    ct_img=ct_img_a,                                # CT scan image array\n",
    "    # Data configuration\n",
    "    available_columns=available_columns_a,          # available data columns\n",
    "    is_multilog=is_multilog,                        # multi-column log flag\n",
    "    # Picked depth data\n",
    "    picked_depths=picked_depths_a,                  # picked depth values\n",
    "    picked_categories=picked_categories_a,          # picked categories\n",
    "    picked_uncertainties=picked_uncertainty_a,      # uncertainty values\n",
    "    # Visualization settings\n",
    "    figsize=(20, 4),                                # figure size (width, height)\n",
    "    show_category=[1],                              # categories to display\n",
    "    show_bed_number=True                            # show bed numbers flag\n",
    ")\n",
    "\n",
    "# Do the same for Core B\n",
    "is_multilog = log_b.ndim > 1 and log_b.shape[1] > 1\n",
    "fig_b, ax_b = plot_core_data(\n",
    "    # Input data\n",
    "    md_b,                                           # depth array\n",
    "    log_b,                                          # log data array\n",
    "    f\"{CORE_B}\",                                    # core name\n",
    "    # Image data\n",
    "    rgb_img=rgb_img_b,                              # RGB image array\n",
    "    ct_img=ct_img_b,                                # CT scan image array\n",
    "    # Data configuration\n",
    "    available_columns=available_columns_b,          # available data columns\n",
    "    is_multilog=is_multilog,                        # multi-column log flag\n",
    "    # Picked depth data\n",
    "    picked_depths=picked_depths_b,                  # picked depth values\n",
    "    picked_categories=picked_categories_b,          # picked categories\n",
    "    picked_uncertainties=picked_uncertainty_b,      # uncertainty values\n",
    "    # Visualization settings\n",
    "    figsize=(20, 4),                                # figure size (width, height)\n",
    "    show_category=[1],                              # categories to display\n",
    "    show_bed_number=True                            # show bed numbers flag\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Usage Examples and Executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a subset of picked depths of category 1 for both cores\n",
    "all_depths_a_cat1 = np.array([depth for depth, category in picked_a if category == 1]).astype('float32')\n",
    "all_depths_b_cat1 = np.array([depth for depth, category in picked_b if category == 1]).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load age constraints for both cores\n",
    "consider_adjacent_core = True\n",
    "\n",
    "data_columns = {\n",
    "    'age': 'calib502_agebp',\n",
    "    'pos_error': 'calib502_2sigma_pos', \n",
    "    'neg_error': 'calib502_2sigma_neg',\n",
    "    'min_depth': 'mindepth_cm',\n",
    "    'max_depth': 'maxdepth_cm',\n",
    "    'in_sequence': 'in_sequence',\n",
    "    'core': 'core',\n",
    "    'interpreted_bed': 'interpreted_bed'\n",
    "}\n",
    "\n",
    "# Configuration\n",
    "age_base_path = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/Age constraints/Goldfinger2012'\n",
    "\n",
    "# Load age constraints for both cores\n",
    "age_data_a = load_core_age_constraints(CORE_A, age_base_path, consider_adjacent_core, data_columns)\n",
    "age_data_b = load_core_age_constraints(CORE_B, age_base_path, consider_adjacent_core, data_columns)\n",
    "\n",
    "uncertainty_method='MonteCarlo'   # 'MonteCarlo', 'Linear', or 'Gaussian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate interpolated ages for Core A using the function\n",
    "pickeddepth_ages_a = calculate_interpolated_ages(\n",
    "    # Input data\n",
    "    picked_depths=all_depths_a_cat1,                                     # depths to interpolate ages for\n",
    "    age_constraints_depths=age_data_a['depths'],                         # age constraint depths\n",
    "    age_constraints_ages=age_data_a['ages'],                             # age constraint ages\n",
    "    age_constraints_pos_errors=age_data_a['pos_errors'],                 # positive errors\n",
    "    age_constraints_neg_errors=age_data_a['neg_errors'],                 # negative errors\n",
    "    age_constraints_in_sequence_flags=age_data_a['in_sequence_flags'],   # in-sequence flags\n",
    "    age_constraint_source_core=age_data_a['core'],                       # source core for each constraint\n",
    "    # Core boundaries\n",
    "    top_bottom=True,                                                     # include top and bottom depths/ages\n",
    "    top_depth=0.0,                                                       # top of core depth\n",
    "    bottom_depth=md_a[-1],                                               # max depth of core a\n",
    "    top_age=0,                                                           # default age at top of core\n",
    "    top_age_pos_error=75,                                                # default positive uncertainty of top age\n",
    "    top_age_neg_error=75,                                                # default negative uncertainty of top age\n",
    "    # Uncertainty calculation\n",
    "    uncertainty_method=uncertainty_method,                               # uncertainty calculation method: 'MonteCarlo', 'Linear', or 'Gaussian'\n",
    "    n_monte_carlo=10000,                                                 # number of Monte Carlo iterations\n",
    "    # Visualization and output\n",
    "    show_plot=True,                                                      # display plot\n",
    "    core_name=CORE_A,                                                    # core name for plot title\n",
    "    export_csv=False                                                      # export results to CSV\n",
    ")\n",
    "\n",
    "# Print the interpolated ages\n",
    "print(f\"\\nEstimated Ages for picked depths in {CORE_A}:\")\n",
    "for i, depth in enumerate(pickeddepth_ages_a['depths']):\n",
    "    print(f\"Depth: {depth:.2f} cm, Age: {pickeddepth_ages_a['ages'][i]:.1f} years BP (+{pickeddepth_ages_a['pos_uncertainties'][i]:.1f} ; -{pickeddepth_ages_a['neg_uncertainties'][i]:.1f})\")\n",
    "\n",
    "# Calculate interpolated ages for Core B using the function\n",
    "pickeddepth_ages_b = calculate_interpolated_ages(\n",
    "    # Input data\n",
    "    picked_depths=all_depths_b_cat1,                                     # depths to interpolate ages for\n",
    "    age_constraints_depths=age_data_b['depths'],                         # age constraint depths\n",
    "    age_constraints_ages=age_data_b['ages'],                             # age constraint ages\n",
    "    age_constraints_pos_errors=age_data_b['pos_errors'],                 # positive errors\n",
    "    age_constraints_neg_errors=age_data_b['neg_errors'],                 # negative errors\n",
    "    age_constraints_in_sequence_flags=age_data_b['in_sequence_flags'],   # in-sequence flags\n",
    "    age_constraint_source_core=age_data_b['core'],                       # source core for each constraint\n",
    "    # Core boundaries\n",
    "    top_bottom=True,                                                     # include top and bottom depths/ages\n",
    "    top_depth=0.0,                                                       # top of core depth\n",
    "    bottom_depth=md_b[-1],                                               # max depth of core b\n",
    "    top_age=0,                                                           # default age at top of core\n",
    "    top_age_pos_error=75,                                                # default positive uncertainty of top age\n",
    "    top_age_neg_error=75,                                                # default negative uncertainty of top age\n",
    "    # Uncertainty calculation\n",
    "    uncertainty_method=uncertainty_method,                               # uncertainty calculation method: 'MonteCarlo', 'Linear', or 'Gaussian'\n",
    "    n_monte_carlo=10000,                                                 # number of Monte Carlo sampling iterations\n",
    "    # Visualization and output\n",
    "    show_plot=True,                                                      # display plot\n",
    "    core_name=CORE_B,                                                    # core name for plot title\n",
    "    export_csv=False                                                      # export results to CSV\n",
    ")\n",
    "\n",
    "print(f\"\\nEstimated Ages for picked depths in {CORE_B}:\")\n",
    "for i, depth in enumerate(pickeddepth_ages_b['depths']):\n",
    "    print(f\"Depth: {depth:.2f} cm, Age: {pickeddepth_ages_b['ages'][i]:.1f} years BP (+{pickeddepth_ages_b['pos_uncertainties'][i]:.1f} ; -{pickeddepth_ages_b['neg_uncertainties'][i]:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load estimated boundary age data for both cores from CSV files\n",
    "# cores = [CORE_A, CORE_B]\n",
    "# pickeddepth_ages = {}\n",
    "\n",
    "# for core in cores:\n",
    "#     core_age_csv = f\"outputs/{core}_pickeddepth_age_{uncertainty_method}.csv\"\n",
    "#     if os.path.exists(core_age_csv):\n",
    "#         df_ages = pd.read_csv(core_age_csv)\n",
    "#         pickeddepth_ages[core] = {\n",
    "#             'depths': df_ages['picked_depths_cm'].values.astype('float32').tolist(),\n",
    "#             'ages': df_ages['est_age'].values.astype('float32').tolist(),\n",
    "#             'pos_uncertainties': df_ages['est_age_poserr'].values.astype('float32').tolist(),\n",
    "#             'neg_uncertainties': df_ages['est_age_negerr'].values.astype('float32').tolist()\n",
    "#         }\n",
    "#         print(f\"Loaded estimated boundary age data for {core} from CSV file\")\n",
    "#     else:\n",
    "#         print(f\"Warning: Could not find boundary age data CSV for {core}\")\n",
    "\n",
    "# # Assign to individual variables for backward compatibility\n",
    "# if CORE_A in pickeddepth_ages:\n",
    "#     pickeddepth_ages_a = pickeddepth_ages[CORE_A]\n",
    "# if CORE_B in pickeddepth_ages:\n",
    "#     pickeddepth_ages_b = pickeddepth_ages[CORE_B]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Compute r-value distribution for all stituation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Multi-Parameter Distribution Analysis\n",
    "# Run all parameter combinations and plot distribution curves together\n",
    "\n",
    "# Define all parameter combinations to test\n",
    "parameter_combinations = [\n",
    "    # {'age_consideration': True, 'restricted_age_correlation': True, 'shortest_path_search': False},\n",
    "    {'age_consideration': True, 'restricted_age_correlation': True, 'shortest_path_search': True},\n",
    "    # {'age_consideration': True, 'restricted_age_correlation': False, 'shortest_path_search': False},\n",
    "    # {'age_consideration': True, 'restricted_age_correlation': False, 'shortest_path_search': True},\n",
    "    # {'age_consideration': False, 'restricted_age_correlation': None, 'shortest_path_search': False},\n",
    "    {'age_consideration': False, 'restricted_age_correlation': None, 'shortest_path_search': True}\n",
    "]\n",
    "\n",
    "# Define all quality indices to process\n",
    "quality_indices = ['corr_coef', 'norm_dtw', 'dtw_ratio', 'perc_diag']\n",
    "\n",
    "# Prepare master CSV files for incremental saving\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "print(f\"Running {len(parameter_combinations)} parameter combinations for {len(quality_indices)} quality indices...\")\n",
    "\n",
    "# Loop through all parameter combinations\n",
    "for idx, params in enumerate(tqdm(parameter_combinations, desc=\"Parameter combinations\")):\n",
    "    \n",
    "    # Extract parameters\n",
    "    age_consideration = params['age_consideration']\n",
    "    restricted_age_correlation = params['restricted_age_correlation']\n",
    "    shortest_path_search = params['shortest_path_search']\n",
    "    \n",
    "    # Generate parameter labels for file naming\n",
    "    if age_consideration:\n",
    "        if restricted_age_correlation:\n",
    "            age_label = 'restricted_age'\n",
    "        else:\n",
    "            age_label = 'loose_age'\n",
    "    else:\n",
    "        age_label = 'no_age'\n",
    "    \n",
    "    search_label = 'optimal' if shortest_path_search else 'random'\n",
    "    \n",
    "    # Create unique identifier for this combination\n",
    "    combo_id = f\"{age_label}_{search_label}\"\n",
    "    \n",
    "    # print(f\"\\n--- Running combination {idx+1}/{len(parameter_combinations)}: {combo_id} ---\")\n",
    "    # print(f\"age_consideration={age_consideration}, restricted_age_correlation={restricted_age_correlation}, shortest_path_search={shortest_path_search}\")\n",
    "    \n",
    "    try:        \n",
    "        # Run comprehensive DTW analysis\n",
    "        dtw_results, valid_dtw_pairs, segments_a, segments_b, depth_boundaries_a, depth_boundaries_b, dtw_distance_matrix_full = run_comprehensive_dtw_analysis(\n",
    "            log_a, log_b, md_a, md_b,\n",
    "            picked_depths_a=all_depths_a_cat1,\n",
    "            picked_depths_b=all_depths_b_cat1,\n",
    "            independent_dtw=False,\n",
    "            top_bottom=True,\n",
    "            top_depth=0.0,\n",
    "            exclude_deadend=True,\n",
    "            mute_mode=True,\n",
    "            age_consideration=age_consideration,\n",
    "            ages_a=pickeddepth_ages_a if age_consideration else None,\n",
    "            ages_b=pickeddepth_ages_b if age_consideration else None,\n",
    "            restricted_age_correlation=restricted_age_correlation if age_consideration else False,\n",
    "            all_constraint_ages_a=age_data_a['in_sequence_ages'] if age_consideration else None,\n",
    "            all_constraint_ages_b=age_data_b['in_sequence_ages'] if age_consideration else None,\n",
    "            all_constraint_depths_a=age_data_a['in_sequence_depths'] if age_consideration else None,\n",
    "            all_constraint_depths_b=age_data_b['in_sequence_depths'] if age_consideration else None,\n",
    "            all_constraint_pos_errors_a=age_data_a['in_sequence_pos_errors'] if age_consideration else None,\n",
    "            all_constraint_pos_errors_b=age_data_b['in_sequence_pos_errors'] if age_consideration else None,\n",
    "            all_constraint_neg_errors_a=age_data_a['in_sequence_neg_errors'] if age_consideration else None,\n",
    "            all_constraint_neg_errors_b=age_data_b['in_sequence_neg_errors'] if age_consideration else None\n",
    "        )\n",
    "        \n",
    "        # Find complete core paths (only for shortest path search - as requested)\n",
    "        if shortest_path_search:\n",
    "            _ = find_complete_core_paths(\n",
    "                valid_dtw_pairs,\n",
    "                segments_a,\n",
    "                segments_b,\n",
    "                log_a,\n",
    "                log_b,\n",
    "                depth_boundaries_a,\n",
    "                depth_boundaries_b,\n",
    "                dtw_results,\n",
    "                output_csv=f'temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv',\n",
    "                start_from_top_only=True,\n",
    "                batch_size=1000,\n",
    "                n_jobs=-1,\n",
    "                shortest_path_search=True,\n",
    "                shortest_path_level=2,\n",
    "                max_search_path=100000,\n",
    "                mute_mode=True\n",
    "            )\n",
    "        else:\n",
    "            # For random search, still run but with different parameters\n",
    "            _ = find_complete_core_paths(\n",
    "                valid_dtw_pairs,\n",
    "                segments_a,\n",
    "                segments_b,\n",
    "                log_a,\n",
    "                log_b,\n",
    "                depth_boundaries_a,\n",
    "                depth_boundaries_b,\n",
    "                dtw_results,\n",
    "                output_csv=f'temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv',\n",
    "                start_from_top_only=True,\n",
    "                batch_size=1000,\n",
    "                n_jobs=-1,\n",
    "                shortest_path_search=False,\n",
    "                shortest_path_level=2,\n",
    "                max_search_path=100000,\n",
    "                mute_mode=True\n",
    "            )\n",
    "        \n",
    "        # Loop through all quality indices for this parameter combination\n",
    "        for quality_index in quality_indices:\n",
    "            \n",
    "            # Extract fit_params using plot_correlation_distribution in mute mode\n",
    "            _, _, fit_params = plot_correlation_distribution(\n",
    "                csv_file=f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv',\n",
    "                quality_index=quality_index,\n",
    "                no_bins=30,\n",
    "                save_png=False,\n",
    "                pdf_method='normal',\n",
    "                kde_bandwidth=0.05,\n",
    "                mute_mode=True\n",
    "            )\n",
    "            \n",
    "            # Store fit_params with parameter combination info\n",
    "            if fit_params is not None:\n",
    "                fit_params_copy = fit_params.copy()\n",
    "                fit_params_copy['combination_id'] = combo_id\n",
    "                fit_params_copy['age_consideration'] = age_consideration\n",
    "                fit_params_copy['restricted_age_correlation'] = restricted_age_correlation\n",
    "                fit_params_copy['shortest_path_search'] = shortest_path_search\n",
    "                fit_params_copy['combination_index'] = idx\n",
    "                \n",
    "                # Define master CSV filename based on quality index\n",
    "                if quality_index == 'corr_coef':\n",
    "                    master_csv_filename = f'outputs/r-values_fit_params_{CORE_A}_{CORE_B}.csv'\n",
    "                else:\n",
    "                    master_csv_filename = f'outputs/{quality_index}_fit_params_{CORE_A}_{CORE_B}.csv'\n",
    "                \n",
    "                # Incrementally save to master CSV\n",
    "                df_single = pd.DataFrame([fit_params_copy])\n",
    "                if idx == 0:\n",
    "                    # Write header for first combination\n",
    "                    df_single.to_csv(master_csv_filename, mode='w', index=False, header=True)\n",
    "                else:\n",
    "                    # Append subsequent combinations without header\n",
    "                    df_single.to_csv(master_csv_filename, mode='a', index=False, header=False)\n",
    "\n",
    "                del df_single, fit_params_copy\n",
    "            else:\n",
    "                print(f\"✗ No fit_params generated for {combo_id} with {quality_index}\")\n",
    "            \n",
    "            # Clean up fit_params for this quality index\n",
    "            del fit_params\n",
    "        \n",
    "        # Clean up temporary files\n",
    "        if os.path.exists(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv'):\n",
    "            os.remove(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv')\n",
    "        \n",
    "        # Clean up variables and force garbage collection\n",
    "        del dtw_results, valid_dtw_pairs, segments_a, segments_b\n",
    "        del depth_boundaries_a, depth_boundaries_b, dtw_distance_matrix_full\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in combination {combo_id}: {str(e)}\")\n",
    "        # Clean up on error\n",
    "        if os.path.exists(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv'):\n",
    "            os.remove(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv')\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✓ Completed all parameter combinations for all quality indices\")\n",
    "for quality_index in quality_indices:\n",
    "    if quality_index == 'corr_coef':\n",
    "        filename = f'outputs/r-values_fit_params_{CORE_A}_{CORE_B}.csv'\n",
    "    else:\n",
    "        filename = f'outputs/{quality_index}_fit_params_{CORE_A}_{CORE_B}.csv'\n",
    "    print(f\"✓ {quality_index} fit_params saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions - moved outside the loop\n",
    "def reconstruct_raw_data_from_histogram(bins, hist_percentages, n_points):\n",
    "    \"\"\"Reconstruct raw data points from histogram bins and percentages\"\"\"\n",
    "    raw_data = []\n",
    "    \n",
    "    # Convert percentages to raw counts\n",
    "    raw_counts = (hist_percentages * n_points) / 100\n",
    "    \n",
    "    # Generate data points for each bin\n",
    "    for i, count in enumerate(raw_counts):\n",
    "        if count > 0:\n",
    "            n_samples = int(round(count))\n",
    "            if n_samples > 0:\n",
    "                # Sample uniformly within the bin\n",
    "                bin_samples = np.random.uniform(bins[i], bins[i+1], n_samples)\n",
    "                raw_data.extend(bin_samples)\n",
    "    \n",
    "    return np.array(raw_data)\n",
    "\n",
    "def cohens_d(x, y):\n",
    "    \"\"\"Calculate Cohen's d for effect size between two samples\"\"\"\n",
    "    n1, n2 = len(x), len(y)\n",
    "    s1, s2 = np.std(x, ddof=1), np.std(y, ddof=1)\n",
    "    # Pooled standard deviation\n",
    "    s_pooled = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n",
    "    # Cohen's d\n",
    "    d = (np.mean(x) - np.mean(y)) / s_pooled\n",
    "    return d\n",
    "\n",
    "# Loop through all quality indices\n",
    "quality_indices = ['corr_coef', 'norm_dtw', 'dtw_ratio', 'perc_diag']\n",
    "\n",
    "for quality_index in quality_indices:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROCESSING QUALITY INDEX: {quality_index}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Now create combined plot from master CSV\n",
    "    print(f\"\\n--- Creating combined distribution plot for {quality_index} ---\")\n",
    "\n",
    "    # Load all fit_params from master CSV\n",
    "    if quality_index == 'corr_coef':\n",
    "        master_csv_filename = f'outputs/r-values_fit_params_{CORE_A}_{CORE_B}.csv'\n",
    "    else:\n",
    "        master_csv_filename = f'outputs/{quality_index}_fit_params_{CORE_A}_{CORE_B}.csv'\n",
    "    \n",
    "    # Check if master CSV exists\n",
    "    if not os.path.exists(master_csv_filename):\n",
    "        print(f\"✗ Error: Master CSV file not found: {master_csv_filename}\")\n",
    "        print(f\"   Skipping {quality_index} and moving to next index...\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        df_all_params = pd.read_csv(master_csv_filename)\n",
    "        print(f\"✓ Loaded master CSV: {master_csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading master CSV {master_csv_filename}: {str(e)}\")\n",
    "        print(f\"   Skipping {quality_index} and moving to next index...\")\n",
    "        continue\n",
    "\n",
    "    # Define which categories to load - you can customize these filters\n",
    "    load_filters = {\n",
    "        'age_consideration': [True, False],  # Load both True and False, or specify [True] for only True\n",
    "        # 'restricted_age_correlation': [True, False, None],  # Load both True, False, and empty/NaN values\n",
    "        'restricted_age_correlation': [True, None],  # Load both True, False, and empty/NaN values\n",
    "        # 'shortest_path_search': [True, False]  # Load both, or specify [True] for only optimal search\n",
    "        'shortest_path_search': [True]  # Load both, or specify [True] for only optimal search\n",
    "    }\n",
    "\n",
    "    # Apply filters with NaN handling\n",
    "    mask = pd.Series([True] * len(df_all_params))\n",
    "    for column, values in load_filters.items():\n",
    "        if values is not None:  # Allow None to skip filtering on that parameter\n",
    "            # Handle NaN/empty values in the filter\n",
    "            if None in values:\n",
    "                # Include rows where the column is NaN or in the specified values\n",
    "                mask &= (df_all_params[column].isin([v for v in values if v is not None]) | \n",
    "                        df_all_params[column].isna())\n",
    "            else:\n",
    "                # Only include non-NaN values that match the filter\n",
    "                mask &= df_all_params[column].isin(values)\n",
    "\n",
    "    df_all_params = df_all_params[mask]\n",
    "    print(f\"Loaded {len(df_all_params)} rows after filtering\")\n",
    "\n",
    "    # Show what combinations were loaded (handle NaN display)\n",
    "    print(\"Loaded parameter combinations:\")\n",
    "    loaded_combos = df_all_params.groupby(['age_consideration', 'restricted_age_correlation', 'shortest_path_search'], dropna=False).size().reset_index(name='count')\n",
    "    for idx, row in loaded_combos.iterrows():\n",
    "        restricted_val = row['restricted_age_correlation'] if pd.notna(row['restricted_age_correlation']) else 'None/Empty'\n",
    "        print(f\"  age_consideration={row['age_consideration']}, restricted_age_correlation={restricted_val}, shortest_path_search={row['shortest_path_search']} ({row['count']} rows)\")\n",
    "\n",
    "    #####\n",
    "\n",
    "    # Load synthetic fit params from CSV for background\n",
    "    # Define input filename based on log columns and quality index\n",
    "    if LOG_COLUMNS == ['hiresMS']:\n",
    "        synthetic_csv_filename = f'outputs/synthetic_PDFs_MSonly_{quality_index}.csv'\n",
    "    elif LOG_COLUMNS == ['hiresMS','CT', 'Lumin']:\n",
    "        synthetic_csv_filename = f'outputs/synthetic_PDFs_MSCTLumin_{quality_index}.csv'\n",
    "    else:\n",
    "        synthetic_csv_filename = f'outputs/synthetic_PDFs_unspecified_{quality_index}.csv'\n",
    "    # Check if synthetic CSV exists\n",
    "    if not os.path.exists(synthetic_csv_filename):\n",
    "        print(f\"✗ Error: Synthetic CSV file not found: {synthetic_csv_filename}\")\n",
    "        print(f\"   Skipping {quality_index} and moving to next index...\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        df_synthetic_params = pd.read_csv(synthetic_csv_filename)\n",
    "        print(f\"✓ Loaded synthetic CSV: {synthetic_csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading synthetic CSV {synthetic_csv_filename}: {str(e)}\")\n",
    "        print(f\"   Skipping {quality_index} and moving to next index...\")\n",
    "        continue\n",
    "\n",
    "    # Reconstruct combined data from binned data (same as file_context_0)\n",
    "    all_raw_data = []\n",
    "\n",
    "    # Process each iteration to reconstruct raw data from binned data\n",
    "    for _, row in df_synthetic_params.iterrows():\n",
    "        # Extract binned data\n",
    "        bins = np.fromstring(row['bins'].strip('[]'), sep=' ') if 'bins' in row and pd.notna(row['bins']) else None\n",
    "        hist_percentages = np.fromstring(row['hist'].strip('[]'), sep=' ') if 'hist' in row and pd.notna(row['hist']) else None\n",
    "        n_points = row['n_points'] if 'n_points' in row and pd.notna(row['n_points']) else None\n",
    "        \n",
    "        if bins is not None and hist_percentages is not None and n_points is not None:\n",
    "            # Convert percentages back to raw counts\n",
    "            raw_counts = (hist_percentages * n_points) / 100\n",
    "            \n",
    "            # Reconstruct data points by sampling from each bin\n",
    "            bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "            bin_width = bins[1] - bins[0]\n",
    "            \n",
    "            for i, count in enumerate(raw_counts):\n",
    "                if count > 0:\n",
    "                    # Generate random points within each bin\n",
    "                    n_samples = int(round(count))\n",
    "                    if n_samples > 0:\n",
    "                        # Sample uniformly within the bin\n",
    "                        bin_samples = np.random.uniform(\n",
    "                            bins[i], bins[i+1], n_samples\n",
    "                        )\n",
    "                        all_raw_data.extend(bin_samples)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    combined_data = np.array(all_raw_data)\n",
    "\n",
    "    # Fit normal distribution to combined data\n",
    "    from scipy import stats\n",
    "    fitted_mean, fitted_std = stats.norm.fit(combined_data)\n",
    "\n",
    "    # Generate fitted curve\n",
    "    x_fitted = np.linspace(combined_data.min(), combined_data.max(), 1000)\n",
    "    y_fitted = stats.norm.pdf(x_fitted, fitted_mean, fitted_std)\n",
    "\n",
    "    # Create combined plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 4.5))\n",
    "\n",
    "    # Define colors and line styles\n",
    "    solid_color = 'brown'\n",
    "    dash_color = 'green'\n",
    "    # Different line patterns for differentiation within same color group\n",
    "    solid_line_patterns = ['-', '--', '-.']  # solid, dash-dot, dotted for brown group\n",
    "    dash_line_patterns = ['-', '--', '-.']  # different dash patterns for green group\n",
    "\n",
    "    # Plot combined histogram in gray bars (same style as file_context_0)\n",
    "    n_bins = 30\n",
    "    ax.hist(combined_data, bins=n_bins, alpha=0.3, color='gray', density=True, label='Synthetic Data')\n",
    "\n",
    "    # Plot fitted normal curve as gray dotted line (same style as original synthetic curves)\n",
    "    ax.plot(x_fitted, y_fitted, color='gray', linestyle=':', linewidth=2, alpha=0.8,\n",
    "            label='Null Hypothesis PDFs')\n",
    "\n",
    "    # Get unique combinations available in the CSV\n",
    "    unique_combinations = df_all_params.drop_duplicates(\n",
    "        subset=['age_consideration', 'restricted_age_correlation', 'shortest_path_search']\n",
    "    )[['age_consideration', 'restricted_age_correlation', 'shortest_path_search']].to_dict('records')\n",
    "\n",
    "    print(f\"Found {len(unique_combinations)} unique parameter combinations in CSV:\")\n",
    "    for combo in unique_combinations:\n",
    "        print(f\"  {combo}\")\n",
    "\n",
    "    # Separate combinations by search method and collect mean values\n",
    "    optimal_combinations = []  # For brown group (shortest_path_search=True)\n",
    "    random_combinations = []   # For green group (shortest_path_search=False)\n",
    "\n",
    "    solid_mean_values = []  # For optimal search group\n",
    "    dash_mean_values = []   # For random search group\n",
    "\n",
    "    for combo in unique_combinations:\n",
    "        combo_data = df_all_params[\n",
    "            (df_all_params['age_consideration'] == combo['age_consideration']) &\n",
    "            (df_all_params['restricted_age_correlation'] == combo['restricted_age_correlation']) &\n",
    "            (df_all_params['shortest_path_search'] == combo['shortest_path_search'])\n",
    "        ]\n",
    "        \n",
    "        combo_mean_values = []\n",
    "        for idx, row in combo_data.iterrows():\n",
    "            if 'mean' in row and pd.notna(row['mean']):\n",
    "                combo_mean_values.append(row['mean'])\n",
    "        \n",
    "        # Determine which search method group this combination belongs to\n",
    "        if combo['shortest_path_search']:\n",
    "            optimal_combinations.append(combo)\n",
    "            solid_mean_values.extend(combo_mean_values)\n",
    "        else:\n",
    "            random_combinations.append(combo)\n",
    "            dash_mean_values.extend(combo_mean_values)\n",
    "\n",
    "    # Plot vertical lines for each search method group with text labels\n",
    "    solid_mean_plotted = False\n",
    "    dash_mean_plotted = False\n",
    "\n",
    "    if solid_mean_values:\n",
    "        for mean_val in solid_mean_values:\n",
    "            label = 'Optimal-Search PDF Means' if not solid_mean_plotted else \"\"\n",
    "            ax.axvline(x=mean_val, color=solid_color, alpha=0.2, linewidth=4, label=label)\n",
    "            # Add text label for mean value\n",
    "            ax.text(mean_val, ax.get_ylim()[1] * 0.9, f'{mean_val:.3f}', \n",
    "                    rotation=90, ha='right', va='top', color=solid_color, fontweight='bold')\n",
    "            solid_mean_plotted = True\n",
    "\n",
    "    if dash_mean_values:\n",
    "        for mean_val in dash_mean_values:\n",
    "            label = 'Random-Search PDF Means' if not dash_mean_plotted else \"\"\n",
    "            ax.axvline(x=mean_val, color=dash_color, alpha=0.2, linewidth=4, label=label)\n",
    "            # Add text label for mean value\n",
    "            ax.text(mean_val, ax.get_ylim()[1] * 0.9, f'{mean_val:.3f}', \n",
    "                    rotation=90, ha='right', va='top', color=dash_color, fontweight='bold')\n",
    "            dash_mean_plotted = True\n",
    "\n",
    "    # Collect real data for statistical tests - separate by individual distributions\n",
    "    solid_real_data_by_combo = {}  # Dictionary to store data by combination\n",
    "    dash_real_data_by_combo = {}\n",
    "\n",
    "    # Plot each distribution curve and collect data for statistical tests\n",
    "    solid_count = 0\n",
    "    dash_count = 0\n",
    "\n",
    "    for idx, row in df_all_params.iterrows():\n",
    "        # Extract parameter values directly from CSV columns\n",
    "        age_consideration = row['age_consideration']\n",
    "        restricted_age_correlation = row['restricted_age_correlation']\n",
    "        shortest_path_search = row['shortest_path_search']\n",
    "        \n",
    "        # Parse x_range and y_values from stored strings\n",
    "        if 'x_range' in row and 'y_values' in row and pd.notna(row['x_range']) and pd.notna(row['y_values']):\n",
    "            try:\n",
    "                x_range = np.fromstring(row['x_range'].strip('[]'), sep=' ')\n",
    "                y_values = np.fromstring(row['y_values'].strip('[]'), sep=' ')\n",
    "                \n",
    "                # Create unique key for this parameter combination\n",
    "                combo_key = f\"age_{age_consideration}_restricted_{restricted_age_correlation}_shortest_{shortest_path_search}\"\n",
    "                \n",
    "                # Reconstruct raw data from histogram for statistical tests\n",
    "                if 'bins' in row and 'hist' in row and 'n_points' in row and \\\n",
    "                   pd.notna(row['bins']) and pd.notna(row['hist']) and pd.notna(row['n_points']):\n",
    "                    \n",
    "                    bins = np.fromstring(row['bins'].strip('[]'), sep=' ')\n",
    "                    hist_percentages = np.fromstring(row['hist'].strip('[]'), sep=' ')\n",
    "                    n_points = row['n_points']\n",
    "                    \n",
    "                    # Reconstruct raw data points\n",
    "                    raw_data_points = reconstruct_raw_data_from_histogram(bins, hist_percentages, n_points)\n",
    "                    \n",
    "                    # Group by search method based on shortest_path_search column\n",
    "                    if shortest_path_search:\n",
    "                        if combo_key not in solid_real_data_by_combo:\n",
    "                            solid_real_data_by_combo[combo_key] = []\n",
    "                        solid_real_data_by_combo[combo_key].extend(raw_data_points)\n",
    "                    else:\n",
    "                        if combo_key not in dash_real_data_by_combo:\n",
    "                            dash_real_data_by_combo[combo_key] = []\n",
    "                        dash_real_data_by_combo[combo_key].extend(raw_data_points)\n",
    "                \n",
    "                if len(x_range) > 0 and len(y_values) > 0:\n",
    "                    # Create descriptive label based on parameter combination\n",
    "                    # Determine age treatment description\n",
    "                    if age_consideration:\n",
    "                        if restricted_age_correlation:\n",
    "                            age_desc = \"Consider age strictly\"\n",
    "                        else:\n",
    "                            age_desc = \"Consider age loosely\"\n",
    "                    else:\n",
    "                        age_desc = \"Neglect age\"\n",
    "                    \n",
    "                    # Create label without mean value\n",
    "                    label = f'{age_desc}'\n",
    "                    \n",
    "                    # Determine color and line pattern based on shortest_path_search\n",
    "                    if shortest_path_search:\n",
    "                        color = solid_color\n",
    "                        line_pattern = solid_line_patterns[solid_count % len(solid_line_patterns)]\n",
    "                        solid_count += 1\n",
    "                    else:\n",
    "                        color = dash_color\n",
    "                        line_pattern = dash_line_patterns[dash_count % len(dash_line_patterns)]\n",
    "                        dash_count += 1\n",
    "                    \n",
    "                    # Plot distribution curve\n",
    "                    ax.plot(x_range, y_values, \n",
    "                           color=color, \n",
    "                           linestyle=line_pattern,\n",
    "                           linewidth=2, alpha=0.6, \n",
    "                           label=label)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not plot curve for row {idx}: {str(e)}\")\n",
    "\n",
    "    # Perform statistical tests for each individual distribution\n",
    "    solid_stats_by_combo = {}\n",
    "    dash_stats_by_combo = {}\n",
    "\n",
    "    # Calculate statistics for solid (optimal search) combinations\n",
    "    for combo_key, data in solid_real_data_by_combo.items():\n",
    "        if len(data) > 1:  # Need at least 2 data points for statistical tests\n",
    "            data_array = np.array(data)\n",
    "            t_stat, p_value = stats.ttest_ind(data_array, combined_data)\n",
    "            \n",
    "            # Replace z_test with one-sample t-test but keep variable name as z_stat\n",
    "            null_mean = np.mean(combined_data)\n",
    "            z_stat, _ = stats.ttest_1samp(data_array, null_mean)\n",
    "            \n",
    "            cohens_d_val = cohens_d(data_array, combined_data)\n",
    "            \n",
    "            solid_stats_by_combo[combo_key] = {\n",
    "                't_stat': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'z_stat': z_stat,  # This is actually a one-sample t-stat now\n",
    "                'cohens_d': cohens_d_val,\n",
    "                'n_samples': len(data_array),\n",
    "                'mean': np.mean(data_array),\n",
    "                'std': np.std(data_array)\n",
    "            }\n",
    "\n",
    "    # Calculate statistics for dash (random search) combinations\n",
    "    for combo_key, data in dash_real_data_by_combo.items():\n",
    "        if len(data) > 1:  # Need at least 2 data points for statistical tests\n",
    "            data_array = np.array(data)\n",
    "            t_stat, p_value = stats.ttest_ind(data_array, combined_data)\n",
    "            \n",
    "            # Replace z_test with one-sample t-test but keep variable name as z_stat\n",
    "            null_mean = np.mean(combined_data)\n",
    "            z_stat, _ = stats.ttest_1samp(data_array, null_mean)\n",
    "            \n",
    "            cohens_d_val = cohens_d(data_array, combined_data)\n",
    "            \n",
    "            dash_stats_by_combo[combo_key] = {\n",
    "                't_stat': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'z_stat': z_stat,  # This is actually a one-sample t-stat now\n",
    "                'cohens_d': cohens_d_val,\n",
    "                'n_samples': len(data_array),\n",
    "                'mean': np.mean(data_array),\n",
    "                'std': np.std(data_array)\n",
    "            }\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlabel(f\"{quality_index}\")\n",
    "    ax.set_ylabel('Probability Density (%)')\n",
    "    ax.set_title(f'{quality_index} Distribution Comparison\\n{CORE_A} vs {CORE_B}')\n",
    "\n",
    "    # Create grouped legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    # Separate handles and labels by groups\n",
    "    synthetic_handles = []\n",
    "    synthetic_labels = []\n",
    "    red_group_handles = []\n",
    "    red_group_labels = []\n",
    "    blue_group_handles = []\n",
    "    blue_group_labels = []\n",
    "\n",
    "    for handle, label in zip(handles, labels):\n",
    "        if 'Null' in label or 'Synthetic' in label:\n",
    "            synthetic_handles.append(handle)\n",
    "            synthetic_labels.append(label)\n",
    "        elif (hasattr(handle, 'get_color') and handle.get_color() == solid_color) or \\\n",
    "             (hasattr(handle, 'get_facecolor') and handle.get_facecolor() == solid_color) or \\\n",
    "             'Optimal' in label:\n",
    "            red_group_handles.append(handle)\n",
    "            red_group_labels.append(label)\n",
    "        elif (hasattr(handle, 'get_color') and handle.get_color() == dash_color) or \\\n",
    "             (hasattr(handle, 'get_facecolor') and handle.get_facecolor() == dash_color) or \\\n",
    "             'Random' in label:\n",
    "            blue_group_handles.append(handle)\n",
    "            blue_group_labels.append(label)\n",
    "\n",
    "    # Create grouped legend with titles and detailed statistics\n",
    "\n",
    "    legend_elements = []\n",
    "    legend_labels = []\n",
    "\n",
    "    # Add synthetic group\n",
    "    if synthetic_handles:\n",
    "        # Add group title (invisible line)\n",
    "        legend_elements.append(Line2D([0], [0], color='white', linewidth=0, alpha=0))\n",
    "        legend_labels.append('Null Hypotheses (Synthetic Data)')\n",
    "        # Add actual legend items\n",
    "        legend_elements.extend(synthetic_handles)\n",
    "        legend_labels.extend(synthetic_labels)\n",
    "        # Add spacer\n",
    "        legend_elements.append(Line2D([0], [0], color='white', linewidth=0, alpha=0))\n",
    "        legend_labels.append('')\n",
    "\n",
    "    # Add red group with individual statistics\n",
    "    if red_group_handles:\n",
    "        # Add group title (invisible line)\n",
    "        legend_elements.append(Line2D([0], [0], color='white', linewidth=0, alpha=0))\n",
    "        legend_labels.append('Optimal Search (Real Data)')\n",
    "        \n",
    "        # Add individual legend items with their statistics\n",
    "        for handle, label in zip(red_group_handles, red_group_labels):\n",
    "            legend_elements.append(handle)\n",
    "            \n",
    "            # Find corresponding combo_key for this label\n",
    "            combo_stats = None\n",
    "            for combo_key, stats_dict in solid_stats_by_combo.items():\n",
    "                # Parse combo_key to match with label\n",
    "                if \"age_True_restricted_True\" in combo_key and label == \"Consider age strictly\":\n",
    "                    combo_stats = stats_dict\n",
    "                    break\n",
    "                elif \"age_True_restricted_False\" in combo_key and label == \"Consider age loosely\":\n",
    "                    combo_stats = stats_dict\n",
    "                    break\n",
    "                elif \"age_False\" in combo_key and label == \"Neglect age\":\n",
    "                    combo_stats = stats_dict\n",
    "                    break\n",
    "            \n",
    "            if combo_stats:\n",
    "                enhanced_label = f\"{label} (t={combo_stats['t_stat']:.1f}, z={combo_stats['z_stat']:.1f}, d={combo_stats['cohens_d']:.1f})\"\n",
    "            else:\n",
    "                enhanced_label = label\n",
    "            \n",
    "            legend_labels.append(enhanced_label)\n",
    "        \n",
    "        # Add spacer\n",
    "        legend_elements.append(Line2D([0], [0], color='white', linewidth=0, alpha=0))\n",
    "        legend_labels.append('')\n",
    "\n",
    "    # Add blue group with individual statistics\n",
    "    if blue_group_handles:\n",
    "        # Add group title (invisible line)\n",
    "        legend_elements.append(Line2D([0], [0], color='white', linewidth=0, alpha=0))\n",
    "        legend_labels.append('Random Search (Real Data)')\n",
    "        \n",
    "        # Add individual legend items with their statistics\n",
    "        for handle, label in zip(blue_group_handles, blue_group_labels):\n",
    "            legend_elements.append(handle)\n",
    "            \n",
    "            # Find corresponding combo_key for this label\n",
    "            combo_stats = None\n",
    "            for combo_key, stats_dict in dash_stats_by_combo.items():\n",
    "                # Parse combo_key to match with label\n",
    "                if \"age_True_restricted_True\" in combo_key and label == \"Consider age strictly\":\n",
    "                    combo_stats = stats_dict\n",
    "                    break\n",
    "                elif \"age_True_restricted_False\" in combo_key and label == \"Consider age loosely\":\n",
    "                    combo_stats = stats_dict\n",
    "                    break\n",
    "                elif \"age_False\" in combo_key and label == \"Neglect age\":\n",
    "                    combo_stats = stats_dict\n",
    "                    break\n",
    "            \n",
    "            if combo_stats:\n",
    "                enhanced_label = f\"{label} (t={combo_stats['t_stat']:.1f}, z={combo_stats['z_stat']:.1f}, d={combo_stats['cohens_d']:.1f})\"\n",
    "            else:\n",
    "                enhanced_label = label\n",
    "            \n",
    "            legend_labels.append(enhanced_label)\n",
    "\n",
    "    # Apply legend with grouping\n",
    "    legend = ax.legend(legend_elements, legend_labels, bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "\n",
    "    # Style the group title labels\n",
    "    for i, label in enumerate(legend_labels):\n",
    "        if label.startswith('Null Hypotheses') or label.startswith('Optimal Search') or label.startswith('Random Search'):\n",
    "            legend.get_texts()[i].set_weight('bold')\n",
    "            legend.get_texts()[i].set_fontsize(10)\n",
    "            legend.get_texts()[i].set_ha('left')  # Align to left\n",
    "\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if quality_index == 'corr_coef':\n",
    "        ax.set_xlim(0, 1.0)\n",
    "    # For other indices, let matplotlib auto-scale\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if quality_index == 'corr_coef':\n",
    "        plt.savefig(f'outputs/r-values_comparison_{CORE_A}_{CORE_B}.png', dpi=150, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'outputs/{quality_index}_comparison_{CORE_A}_{CORE_B}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed statistical results\n",
    "    print(f\"\\n=== DETAILED STATISTICAL ANALYSIS FOR {quality_index} ===\")\n",
    "    print(f\"Null Hypothesis Distribution:\")\n",
    "    print(f\"  Mean: {fitted_mean:.1f}, SD: {fitted_std:.1f}\")\n",
    "    print(f\"  Sample size: {len(combined_data)}\")\n",
    "    print(f\"  Interpretation: Baseline distribution from synthetic data representing no true correlation\")\n",
    "    print()\n",
    "\n",
    "    print(f\"--- Optimal Search Results ---\")\n",
    "    for combo_key, stats in solid_stats_by_combo.items():\n",
    "        # Parse combo_key to get descriptive name\n",
    "        if \"age_True_restricted_True\" in combo_key:\n",
    "            desc = \"Consider age strictly\"\n",
    "        elif \"age_True_restricted_False\" in combo_key:\n",
    "            desc = \"Consider age loosely\"\n",
    "        elif \"age_False\" in combo_key:\n",
    "            desc = \"Neglect age\"\n",
    "        else:\n",
    "            desc = combo_key\n",
    "        \n",
    "        print(f\"{desc}:\")\n",
    "        print(f\"  Mean: {stats['mean']:.1f}, SD: {stats['std']:.1f}\")\n",
    "        print(f\"  t-statistic: {stats['t_stat']:.1f} (measures difference between means relative to variation)\")\n",
    "        print(f\"  z-statistic: {stats['z_stat']:.1f} (standard deviations from null hypothesis mean)\")\n",
    "        print(f\"  p-value: {stats['p_value']:.2g}\")\n",
    "        print(f\"  Cohen's d: {stats['cohens_d']:.1f}\")\n",
    "        print(f\"  Sample size: {stats['n_samples']}\")\n",
    "        \n",
    "        # Interpret effect size\n",
    "        if abs(stats['cohens_d']) < 0.2:\n",
    "            effect_size = \"negligible\"\n",
    "        elif abs(stats['cohens_d']) < 0.5:\n",
    "            effect_size = \"small\"\n",
    "        elif abs(stats['cohens_d']) < 0.8:\n",
    "            effect_size = \"medium\"\n",
    "        else:\n",
    "            effect_size = \"large\"\n",
    "        \n",
    "        # Statistical interpretation\n",
    "        if stats['t_stat'] > 0:\n",
    "            direction = \"higher than\"\n",
    "        else:\n",
    "            direction = \"lower than\"\n",
    "        \n",
    "        print(f\"  Effect size: {effect_size} difference between distributions\")\n",
    "        \n",
    "        # Only use \"significantly\" if p-value indicates statistical significance\n",
    "        if stats['p_value'] < 0.05:\n",
    "            print(f\"  Interpretation: Significantly {direction} null hypothesis with {effect_size} effect size\")\n",
    "        else:\n",
    "            print(f\"  Interpretation: no statistical significance (p-value = {stats['p_value']:.2e})\")\n",
    "        print()\n",
    "        \n",
    "\n",
    "    print(f\"--- Random Search Results ---\")\n",
    "    for combo_key, stats in dash_stats_by_combo.items():\n",
    "        # Parse combo_key to get descriptive name\n",
    "        if \"age_True_restricted_True\" in combo_key:\n",
    "            desc = \"Consider age strictly\"\n",
    "        elif \"age_True_restricted_False\" in combo_key:\n",
    "            desc = \"Consider age loosely\"\n",
    "        elif \"age_False\" in combo_key:\n",
    "            desc = \"Neglect age\"\n",
    "        else:\n",
    "            desc = combo_key\n",
    "        \n",
    "        print(f\"{desc}:\")\n",
    "        print(f\"  Mean: {stats['mean']:.1f}, SD: {stats['std']:.1f}\")\n",
    "        print(f\"  t-statistic: {stats['t_stat']:.1f} (measures difference between means relative to variation)\")\n",
    "        print(f\"  z-statistic: {stats['z_stat']:.1f} (standard deviations from null hypothesis mean)\")\n",
    "        print(f\"  p-value: {stats['p_value']:.2g}\")\n",
    "        print(f\"  Cohen's d: {stats['cohens_d']:.1f}\")\n",
    "        print(f\"  Sample size: {stats['n_samples']}\")\n",
    "        \n",
    "        # Interpret effect size\n",
    "        if abs(stats['cohens_d']) < 0.2:\n",
    "            effect_size = \"negligible\"\n",
    "        elif abs(stats['cohens_d']) < 0.5:\n",
    "            effect_size = \"small\"\n",
    "        elif abs(stats['cohens_d']) < 0.8:\n",
    "            effect_size = \"medium\"\n",
    "        else:\n",
    "            effect_size = \"large\"\n",
    "        \n",
    "        # Statistical interpretation\n",
    "        if stats['t_stat'] > 0:\n",
    "            direction = \"higher than\"\n",
    "        else:\n",
    "            direction = \"lower than\"\n",
    "        \n",
    "        print(f\"  Effect size: {effect_size} difference between distributions\")\n",
    "        \n",
    "        # Only use \"significantly\" if p-value indicates statistical significance\n",
    "        if stats['p_value'] < 0.05:\n",
    "            print(f\"  Interpretation: Significantly {direction} null hypothesis with {effect_size} effect size\")\n",
    "        else:\n",
    "            print(f\"  Interpretation: no statistical significance (p-value = {stats['p_value']:.2e})\")\n",
    "        print()\n",
    "\n",
    "    if quality_index == 'corr_coef':\n",
    "        print(f\"\\n✓ Combined plot saved as: outputs/r-values_comparison_{CORE_A}_{CORE_B}.png\")\n",
    "    else:\n",
    "        print(f\"\\n✓ Combined plot saved as: outputs/{quality_index}_comparison_{CORE_A}_{CORE_B}.png\")\n",
    "    print(f\"✓ Analysis complete for {quality_index}!\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ALL QUALITY INDICES PROCESSING COMPLETED\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "11-TimeSeriesCorrelation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
