{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25kTe2nS84Tf"
   },
   "source": [
    "# Correlating well log pairs: Complex Dynamic Time Warping with boundary constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4awuK6ffO2t"
   },
   "source": [
    "## Introduction to dynamic time warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47Kk07X_84Th"
   },
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import glob\n",
    "from IPython.display import Image as IPImage, display\n",
    "from scipy import stats\n",
    "from matplotlib.lines import Line2D\n",
    "from itertools import combinations\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyCoreRelator import (\n",
    "    run_comprehensive_dtw_analysis,\n",
    "    find_complete_core_paths,\n",
    "    calculate_interpolated_ages,\n",
    "    load_log_data,\n",
    "    plot_core_data,\n",
    "    plot_correlation_distribution,\n",
    "    load_core_age_constraints\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Test with Cascadia hi-res MS logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define core pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define core names as variables for easy reference\n",
    "# CORE_A = \"M9907-22PC\"\n",
    "# CORE_A = \"M9907-25PC\"\n",
    "# CORE_A = \"M9907-12PC\"\n",
    "CORE_A = \"RR0207-56PC\" \n",
    "\n",
    "# CORE_B = \"M9907-25PC\"\n",
    "CORE_B = \"M9907-12PC\"\n",
    "# CORE_B = \"RR0207-56PC\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data structures and core images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log columns to extract\n",
    "# LOG_COLUMNS = ['hiresMS', 'CT', 'R', 'G', 'B']  # Choose which logs to include\n",
    "# LOG_COLUMNS = ['hiresMS', 'CT', 'Lumin']  # Choose which logs to include\n",
    "LOG_COLUMNS = ['hiresMS']  # Choose which logs to include\n",
    "DEPTH_COLUMN = 'SB_DEPTH_cm'\n",
    "\n",
    "# Define directory paths\n",
    "mother_dir = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/'\n",
    "\n",
    "# Define paths for Core A\n",
    "core_a_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_MST_MLfilled.csv'\n",
    "}\n",
    "\n",
    "core_a_rgb_img_path = f\"{mother_dir}_compiled_logs/{CORE_A}/{CORE_A}_RGB.tiff\"\n",
    "core_a_ct_img_path = f\"{mother_dir}_compiled_logs/{CORE_A}/{CORE_A}_CT.tiff\"\n",
    "\n",
    "# Define paths for Core B\n",
    "core_b_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_MST_MLfilled.csv'\n",
    "}\n",
    "core_b_rgb_img_path = f\"{mother_dir}_compiled_logs/{CORE_B}/{CORE_B}_RGB.tiff\"\n",
    "core_b_ct_img_path = f\"{mother_dir}_compiled_logs/{CORE_B}/{CORE_B}_CT.tiff\"\n",
    "\n",
    "# Define column mapping for alternative column names\n",
    "column_alternatives = {\n",
    "    'hiresMS': ['MS'],\n",
    "    'CT': ['CT_value'],\n",
    "    'R': ['R', 'red', 'Red'],\n",
    "    'G': ['G', 'green', 'Green'],\n",
    "    'B': ['B', 'blue', 'Blue'],\n",
    "    'Lumin': ['luminance', 'Luminance'],\n",
    "    'Den_gm/cc': ['Density', 'density']\n",
    "}\n",
    "\n",
    "# Load data for Core A\n",
    "log_a, md_a, available_columns_a, rgb_img_a, ct_img_a = load_log_data(\n",
    "    core_a_log_paths,\n",
    "    {'rgb': core_a_rgb_img_path, 'ct': core_a_ct_img_path},\n",
    "    LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")\n",
    "\n",
    "print(\"\\n=== DEBUG: Core A Loading ===\")\n",
    "print(f\"LOG_COLUMNS requested: {LOG_COLUMNS}\")\n",
    "print(f\"Available columns loaded: {available_columns_a}\")\n",
    "print(f\"Shape of log_a: {log_a.shape}\")\n",
    "print(f\"Type of log_a: {type(log_a)}\")\n",
    "if hasattr(log_a, 'ndim'):\n",
    "    print(f\"log_a dimensions: {log_a.ndim}\")\n",
    "    if log_a.ndim > 1:\n",
    "        print(f\"log_a has {log_a.shape[1]} columns\\n\")\n",
    "    else:\n",
    "        print(\"log_a is 1D (single column)\\n\")\n",
    "\n",
    "# Load data for Core B\n",
    "log_b, md_b, available_columns_b, rgb_img_b, ct_img_b = load_log_data(\n",
    "    core_b_log_paths,\n",
    "    {'rgb': core_b_rgb_img_path, 'ct': core_b_ct_img_path},\n",
    "    LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")\n",
    "\n",
    "print(\"\\n=== DEBUG: Core B Loading ===\")\n",
    "print(f\"LOG_COLUMNS requested: {LOG_COLUMNS}\")\n",
    "print(f\"Available columns loaded: {available_columns_b}\")\n",
    "print(f\"Shape of log_b: {log_b.shape}\")\n",
    "print(f\"Type of log_b: {type(log_b)}\")\n",
    "if hasattr(log_b, 'ndim'):\n",
    "    print(f\"log_b dimensions: {log_b.ndim}\")\n",
    "    if log_b.ndim > 1:\n",
    "        print(f\"log_b has {log_b.shape[1]} columns\\n\")\n",
    "    else:\n",
    "        print(\"log_b is 1D (single column)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Define paths to the CSV files\n",
    "pickeddepth_a_csv = f'pickeddepth/{CORE_A}_pickeddepth.csv'\n",
    "pickeddepth_b_csv = f'pickeddepth/{CORE_B}_pickeddepth.csv'\n",
    "\n",
    "# Load picked depths and categories from CSV files\n",
    "if os.path.exists(pickeddepth_b_csv):\n",
    "    picked_data_b = pd.read_csv(pickeddepth_b_csv)\n",
    "    # Combine depths and categories into tuples\n",
    "    picked_b = list(zip(picked_data_b['picked_depths_cm'].values.tolist(), \n",
    "                        picked_data_b['category'].values.tolist()))\n",
    "    print(f\"Loaded {len(picked_b)} picked depths for {CORE_B}\")\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_b_csv} not found. Using empty list for picked_b.\")\n",
    "    picked_b = []\n",
    "\n",
    "if os.path.exists(pickeddepth_a_csv):\n",
    "    picked_data_a = pd.read_csv(pickeddepth_a_csv)\n",
    "    # Combine depths and categories into tuples\n",
    "    picked_a = list(zip(picked_data_a['picked_depths_cm'].values.tolist(), \n",
    "                        picked_data_a['category'].values.tolist()))\n",
    "    print(f\"Loaded {len(picked_a)} picked depths for {CORE_A}\")\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_a_csv} not found. Using empty list for picked_a.\")\n",
    "    picked_a = []\n",
    "\n",
    "# Create uncertainty arrays (assuming uncertainty size is 2 cm)\n",
    "picked_uncertainty_b = [1] * len(picked_b)\n",
    "picked_uncertainty_a = [1] * len(picked_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract depths and categories from the loaded tuples\n",
    "picked_depths_a = [depth for depth, category in picked_a] if picked_a else []\n",
    "picked_categories_a = [category for depth, category in picked_a] if picked_a else []\n",
    "\n",
    "picked_depths_b = [depth for depth, category in picked_b] if picked_b else []\n",
    "picked_categories_b = [category for depth, category in picked_b] if picked_b else []\n",
    "\n",
    "# Now plot the cores with enhanced plot_core_data function\n",
    "is_multilog = log_a.ndim > 1 and log_a.shape[1] > 1\n",
    "fig_a, ax_a = plot_core_data(\n",
    "    # Input data\n",
    "    md_a,                                           # depth array\n",
    "    log_a,                                          # log data array\n",
    "    f\"{CORE_A}\",                                    # core name\n",
    "    # Image data\n",
    "    rgb_img=rgb_img_a,                              # RGB image array\n",
    "    ct_img=ct_img_a,                                # CT scan image array\n",
    "    # Data configuration\n",
    "    available_columns=available_columns_a,          # available data columns\n",
    "    is_multilog=is_multilog,                        # multi-column log flag\n",
    "    # Picked depth data\n",
    "    picked_depths=picked_depths_a,                  # picked depth values\n",
    "    picked_categories=picked_categories_a,          # picked categories\n",
    "    picked_uncertainties=picked_uncertainty_a,      # uncertainty values\n",
    "    # Visualization settings\n",
    "    figsize=(20, 4),                                # figure size (width, height)\n",
    "    show_category=[1],                              # categories to display\n",
    "    show_bed_number=True                            # show bed numbers flag\n",
    ")\n",
    "\n",
    "# Do the same for Core B\n",
    "is_multilog = log_b.ndim > 1 and log_b.shape[1] > 1\n",
    "fig_b, ax_b = plot_core_data(\n",
    "    # Input data\n",
    "    md_b,                                           # depth array\n",
    "    log_b,                                          # log data array\n",
    "    f\"{CORE_B}\",                                    # core name\n",
    "    # Image data\n",
    "    rgb_img=rgb_img_b,                              # RGB image array\n",
    "    ct_img=ct_img_b,                                # CT scan image array\n",
    "    # Data configuration\n",
    "    available_columns=available_columns_b,          # available data columns\n",
    "    is_multilog=is_multilog,                        # multi-column log flag\n",
    "    # Picked depth data\n",
    "    picked_depths=picked_depths_b,                  # picked depth values\n",
    "    picked_categories=picked_categories_b,          # picked categories\n",
    "    picked_uncertainties=picked_uncertainty_b,      # uncertainty values\n",
    "    # Visualization settings\n",
    "    figsize=(20, 4),                                # figure size (width, height)\n",
    "    show_category=[1],                              # categories to display\n",
    "    show_bed_number=True                            # show bed numbers flag\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Usage Examples and Executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a subset of picked depths of category 1 for both cores\n",
    "all_depths_a_cat1 = np.array([depth for depth, category in picked_a if category == 1]).astype('float32')\n",
    "all_depths_b_cat1 = np.array([depth for depth, category in picked_b if category == 1]).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load age constraints for both cores\n",
    "consider_adjacent_core = True\n",
    "\n",
    "data_columns = {\n",
    "    'age': 'calib502_agebp',\n",
    "    'pos_error': 'calib502_2sigma_pos', \n",
    "    'neg_error': 'calib502_2sigma_neg',\n",
    "    'min_depth': 'mindepth_cm',\n",
    "    'max_depth': 'maxdepth_cm',\n",
    "    'in_sequence': 'in_sequence',\n",
    "    'core': 'core',\n",
    "    'interpreted_bed': 'interpreted_bed'\n",
    "}\n",
    "\n",
    "# Configuration\n",
    "age_base_path = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/Age constraints/Goldfinger2012'\n",
    "\n",
    "# Load age constraints for both cores\n",
    "age_data_a = load_core_age_constraints(CORE_A, age_base_path, consider_adjacent_core, data_columns)\n",
    "age_data_b = load_core_age_constraints(CORE_B, age_base_path, consider_adjacent_core, data_columns)\n",
    "\n",
    "uncertainty_method='MonteCarlo'   # 'MonteCarlo', 'Linear', or 'Gaussian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate interpolated ages for Core A using the function\n",
    "pickeddepth_ages_a = calculate_interpolated_ages(\n",
    "    # Input data\n",
    "    picked_depths=all_depths_a_cat1,                                     # depths to interpolate ages for\n",
    "    age_constraints_depths=age_data_a['depths'],                         # age constraint depths\n",
    "    age_constraints_ages=age_data_a['ages'],                             # age constraint ages\n",
    "    age_constraints_pos_errors=age_data_a['pos_errors'],                 # positive errors\n",
    "    age_constraints_neg_errors=age_data_a['neg_errors'],                 # negative errors\n",
    "    age_constraints_in_sequence_flags=age_data_a['in_sequence_flags'],   # in-sequence flags\n",
    "    age_constraint_source_core=age_data_a['core'],                       # source core for each constraint\n",
    "    # Core boundaries\n",
    "    top_bottom=True,                                                     # include top and bottom depths/ages\n",
    "    top_depth=0.0,                                                       # top of core depth\n",
    "    bottom_depth=md_a[-1],                                               # max depth of core a\n",
    "    top_age=0,                                                           # default age at top of core\n",
    "    top_age_pos_error=75,                                                # default positive uncertainty of top age\n",
    "    top_age_neg_error=75,                                                # default negative uncertainty of top age\n",
    "    # Uncertainty calculation\n",
    "    uncertainty_method=uncertainty_method,                               # uncertainty calculation method: 'MonteCarlo', 'Linear', or 'Gaussian'\n",
    "    n_monte_carlo=10000,                                                 # number of Monte Carlo iterations\n",
    "    # Visualization and output\n",
    "    show_plot=True,                                                      # display plot\n",
    "    core_name=CORE_A,                                                    # core name for plot title\n",
    "    export_csv=False,                                                    # export results to CSV\n",
    "    mute_mode=True\n",
    ")\n",
    "\n",
    "# # Print the interpolated ages\n",
    "# print(f\"\\nEstimated Ages for picked depths in {CORE_A}:\")\n",
    "# for i, depth in enumerate(pickeddepth_ages_a['depths']):\n",
    "#     print(f\"Depth: {depth:.2f} cm, Age: {pickeddepth_ages_a['ages'][i]:.1f} years BP (+{pickeddepth_ages_a['pos_uncertainties'][i]:.1f} ; -{pickeddepth_ages_a['neg_uncertainties'][i]:.1f})\")\n",
    "\n",
    "# Calculate interpolated ages for Core B using the function\n",
    "pickeddepth_ages_b = calculate_interpolated_ages(\n",
    "    # Input data\n",
    "    picked_depths=all_depths_b_cat1,                                     # depths to interpolate ages for\n",
    "    age_constraints_depths=age_data_b['depths'],                         # age constraint depths\n",
    "    age_constraints_ages=age_data_b['ages'],                             # age constraint ages\n",
    "    age_constraints_pos_errors=age_data_b['pos_errors'],                 # positive errors\n",
    "    age_constraints_neg_errors=age_data_b['neg_errors'],                 # negative errors\n",
    "    age_constraints_in_sequence_flags=age_data_b['in_sequence_flags'],   # in-sequence flags\n",
    "    age_constraint_source_core=age_data_b['core'],                       # source core for each constraint\n",
    "    # Core boundaries\n",
    "    top_bottom=True,                                                     # include top and bottom depths/ages\n",
    "    top_depth=0.0,                                                       # top of core depth\n",
    "    bottom_depth=md_b[-1],                                               # max depth of core b\n",
    "    top_age=0,                                                           # default age at top of core\n",
    "    top_age_pos_error=75,                                                # default positive uncertainty of top age\n",
    "    top_age_neg_error=75,                                                # default negative uncertainty of top age\n",
    "    # Uncertainty calculation\n",
    "    uncertainty_method=uncertainty_method,                               # uncertainty calculation method: 'MonteCarlo', 'Linear', or 'Gaussian'\n",
    "    n_monte_carlo=10000,                                                 # number of Monte Carlo sampling iterations\n",
    "    # Visualization and output\n",
    "    show_plot=True,                                                      # display plot\n",
    "    core_name=CORE_B,                                                    # core name for plot title\n",
    "    export_csv=False,                                                    # export results to CSV\n",
    "    mute_mode=True\n",
    ")\n",
    "\n",
    "# print(f\"\\nEstimated Ages for picked depths in {CORE_B}:\")\n",
    "# for i, depth in enumerate(pickeddepth_ages_b['depths']):\n",
    "#     print(f\"Depth: {depth:.2f} cm, Age: {pickeddepth_ages_b['ages'][i]:.1f} years BP (+{pickeddepth_ages_b['pos_uncertainties'][i]:.1f} ; -{pickeddepth_ages_b['neg_uncertainties'][i]:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load estimated boundary age data for both cores from CSV files\n",
    "# cores = [CORE_A, CORE_B]\n",
    "# pickeddepth_ages = {}\n",
    "\n",
    "# for core in cores:\n",
    "#     core_age_csv = f\"outputs/{core}_pickeddepth_age_{uncertainty_method}.csv\"\n",
    "#     if os.path.exists(core_age_csv):\n",
    "#         df_ages = pd.read_csv(core_age_csv)\n",
    "#         pickeddepth_ages[core] = {\n",
    "#             'depths': df_ages['picked_depths_cm'].values.astype('float32').tolist(),\n",
    "#             'ages': df_ages['est_age'].values.astype('float32').tolist(),\n",
    "#             'pos_uncertainties': df_ages['est_age_poserr'].values.astype('float32').tolist(),\n",
    "#             'neg_uncertainties': df_ages['est_age_negerr'].values.astype('float32').tolist()\n",
    "#         }\n",
    "#         print(f\"Loaded estimated boundary age data for {core} from CSV file\")\n",
    "#     else:\n",
    "#         print(f\"Warning: Could not find boundary age data CSV for {core}\")\n",
    "\n",
    "# # Assign to individual variables for backward compatibility\n",
    "# if CORE_A in pickeddepth_ages:\n",
    "#     pickeddepth_ages_a = pickeddepth_ages[CORE_A]\n",
    "# if CORE_B in pickeddepth_ages:\n",
    "#     pickeddepth_ages_b = pickeddepth_ages[CORE_B]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Compute r-value distribution for all stituation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Multi-Parameter Distribution Analysis\n",
    "\n",
    "test_age_constraint_removal = True  # Set to False to disable age constraint removal testing\n",
    "\n",
    "# Helper functions\n",
    "def generate_constraint_subsets(n_constraints):\n",
    "    \"\"\"Generate all possible subsets of constraints (2^n combinations)\"\"\"\n",
    "    all_subsets = []\n",
    "    for r in range(n_constraints + 1):  # 0 to n_constraints\n",
    "        for subset in combinations(range(n_constraints), r):\n",
    "            all_subsets.append(list(subset))\n",
    "    return all_subsets\n",
    "\n",
    "# Run all parameter combinations and plot distribution curves together\n",
    "\n",
    "# Define all parameter combinations to test\n",
    "parameter_combinations = [\n",
    "    # {'age_consideration': True, 'restricted_age_correlation': True, 'shortest_path_search': False},\n",
    "    {'age_consideration': True, 'restricted_age_correlation': True, 'shortest_path_search': True},\n",
    "    # {'age_consideration': True, 'restricted_age_correlation': False, 'shortest_path_search': False},\n",
    "    # {'age_consideration': True, 'restricted_age_correlation': False, 'shortest_path_search': True},\n",
    "    # {'age_consideration': False, 'restricted_age_correlation': None, 'shortest_path_search': False},\n",
    "    {'age_consideration': False, 'restricted_age_correlation': None, 'shortest_path_search': True}\n",
    "]\n",
    "\n",
    "# Define all quality indices to process\n",
    "quality_indices = ['corr_coef', 'norm_dtw', 'perc_diag']\n",
    "\n",
    "# Prepare master CSV files for incremental saving\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "print(f\"Running {len(parameter_combinations)} parameter combinations for {len(quality_indices)} quality indices...\")\n",
    "\n",
    "# Calculate additional scenarios if constraint removal is enabled\n",
    "# Reset variables at the beginning\n",
    "n_constraints_b = 0\n",
    "age_enabled_params = []\n",
    "total_additional_scenarios = 0\n",
    "\n",
    "if test_age_constraint_removal:\n",
    "    n_constraints_b = len(age_data_b['in_sequence_ages'])\n",
    "    age_enabled_params = [p for p in parameter_combinations if p['age_consideration']]\n",
    "    constraint_scenarios_per_param = (2 ** n_constraints_b) - 1  # Exclude empty set\n",
    "    total_additional_scenarios = len(age_enabled_params) * (constraint_scenarios_per_param - 1)  # Exclude original scenario\n",
    "    \n",
    "    print(f\"Age constraint removal testing enabled:\")\n",
    "    print(f\"- Core B has {n_constraints_b} age constraints\")\n",
    "    print(f\"- Additional scenarios to process: {total_additional_scenarios}\")\n",
    "\n",
    "# PHASE 1: Run original parameter combinations\n",
    "if test_age_constraint_removal:\n",
    "    print(\"\\n=== PHASE 1: Running original parameter combinations ===\")\n",
    "else:\n",
    "    print(\"\\nRunning parameter combinations...\")\n",
    "\n",
    "for idx, params in enumerate(tqdm(parameter_combinations, desc=\"Parameter combinations\" if not test_age_constraint_removal else \"Original parameter combinations\")):\n",
    "    \n",
    "    # Extract parameters\n",
    "    age_consideration = params['age_consideration']\n",
    "    restricted_age_correlation = params['restricted_age_correlation']\n",
    "    shortest_path_search = params['shortest_path_search']\n",
    "    \n",
    "    # Generate parameter labels\n",
    "    if age_consideration:\n",
    "        if restricted_age_correlation:\n",
    "            age_label = 'restricted_age'\n",
    "        else:\n",
    "            age_label = 'loose_age'\n",
    "    else:\n",
    "        age_label = 'no_age'\n",
    "    \n",
    "    search_label = 'optimal' if shortest_path_search else 'random'\n",
    "    combo_id = f\"{age_label}_{search_label}\"\n",
    "    \n",
    "    try:        \n",
    "        # Run comprehensive DTW analysis with original constraints\n",
    "        dtw_results, valid_dtw_pairs, segments_a, segments_b, depth_boundaries_a, depth_boundaries_b, dtw_distance_matrix_full = run_comprehensive_dtw_analysis(\n",
    "            log_a, log_b, md_a, md_b,\n",
    "            picked_depths_a=all_depths_a_cat1,\n",
    "            picked_depths_b=all_depths_b_cat1,\n",
    "            independent_dtw=False,\n",
    "            top_bottom=True,\n",
    "            top_depth=0.0,\n",
    "            exclude_deadend=True,\n",
    "            mute_mode=True,\n",
    "            age_consideration=age_consideration,\n",
    "            ages_a=pickeddepth_ages_a if age_consideration else None,\n",
    "            ages_b=pickeddepth_ages_b if age_consideration else None,\n",
    "            restricted_age_correlation=restricted_age_correlation if age_consideration else False,\n",
    "            all_constraint_ages_a=age_data_a['in_sequence_ages'] if age_consideration else None,\n",
    "            all_constraint_ages_b=age_data_b['in_sequence_ages'] if age_consideration else None,\n",
    "            all_constraint_depths_a=age_data_a['in_sequence_depths'] if age_consideration else None,\n",
    "            all_constraint_depths_b=age_data_b['in_sequence_depths'] if age_consideration else None,\n",
    "            all_constraint_pos_errors_a=age_data_a['in_sequence_pos_errors'] if age_consideration else None,\n",
    "            all_constraint_pos_errors_b=age_data_b['in_sequence_pos_errors'] if age_consideration else None,\n",
    "            all_constraint_neg_errors_a=age_data_a['in_sequence_neg_errors'] if age_consideration else None,\n",
    "            all_constraint_neg_errors_b=age_data_b['in_sequence_neg_errors'] if age_consideration else None\n",
    "        )\n",
    "        \n",
    "        # Find complete core paths\n",
    "        if shortest_path_search:\n",
    "            _ = find_complete_core_paths(\n",
    "                valid_dtw_pairs, segments_a, segments_b, log_a, log_b,\n",
    "                depth_boundaries_a, depth_boundaries_b, dtw_results,\n",
    "                output_csv=f'temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv',\n",
    "                start_from_top_only=True, batch_size=1000, n_jobs=-1,\n",
    "                shortest_path_search=True, shortest_path_level=2,\n",
    "                max_search_path=100000, mute_mode=True\n",
    "            )\n",
    "        else:\n",
    "            _ = find_complete_core_paths(\n",
    "                valid_dtw_pairs, segments_a, segments_b, log_a, log_b,\n",
    "                depth_boundaries_a, depth_boundaries_b, dtw_results,\n",
    "                output_csv=f'temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv',\n",
    "                start_from_top_only=True, batch_size=1000, n_jobs=-1,\n",
    "                shortest_path_search=False, shortest_path_level=2,\n",
    "                max_search_path=100000, mute_mode=True\n",
    "            )\n",
    "        \n",
    "        # Process quality indices\n",
    "        for quality_index in quality_indices:\n",
    "            \n",
    "            _, _, fit_params = plot_correlation_distribution(\n",
    "                csv_file=f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv',\n",
    "                quality_index=quality_index,\n",
    "                no_bins=30, save_png=False, pdf_method='normal',\n",
    "                kde_bandwidth=0.05, mute_mode=True\n",
    "            )\n",
    "            \n",
    "            if fit_params is not None:\n",
    "                fit_params_copy = fit_params.copy()\n",
    "                fit_params_copy['combination_id'] = combo_id\n",
    "                fit_params_copy['age_consideration'] = age_consideration\n",
    "                fit_params_copy['restricted_age_correlation'] = restricted_age_correlation\n",
    "                fit_params_copy['shortest_path_search'] = shortest_path_search\n",
    "                \n",
    "                # Add constraint tracking columns\n",
    "                fit_params_copy['core_a_constraints_count'] = len(age_data_a['in_sequence_ages']) if age_consideration else 0\n",
    "                fit_params_copy['core_b_constraints_count'] = len(age_data_b['in_sequence_ages']) if age_consideration else 0\n",
    "                fit_params_copy['constraint_scenario_description'] = 'all_original_constraints_remained' if age_consideration else 'no_age_constraints_used'\n",
    "                \n",
    "                # Save to CSV\n",
    "                # Load all fit_params from master CSV\n",
    "                if LOG_COLUMNS == ['hiresMS']:\n",
    "                    log_suffix = 'MSonly'\n",
    "                elif LOG_COLUMNS == ['hiresMS','CT', 'Lumin']:\n",
    "                    log_suffix = 'MSCTLumin'\n",
    "                else:\n",
    "                    log_suffix = 'unspecified'\n",
    "                \n",
    "                if quality_index == 'corr_coef':\n",
    "                    master_csv_filename = f'outputs/r-values_fit_params_{log_suffix}_{CORE_A}_{CORE_B}.csv'\n",
    "                else:\n",
    "                    master_csv_filename = f'outputs/{quality_index}_fit_params_{log_suffix}_{CORE_A}_{CORE_B}.csv'\n",
    "                \n",
    "                df_single = pd.DataFrame([fit_params_copy])\n",
    "                if idx == 0:\n",
    "                    df_single.to_csv(master_csv_filename, mode='w', index=False, header=True)\n",
    "                else:\n",
    "                    df_single.to_csv(master_csv_filename, mode='a', index=False, header=False)\n",
    "\n",
    "                del df_single, fit_params_copy\n",
    "            \n",
    "            del fit_params\n",
    "        \n",
    "        # Clean up\n",
    "        if os.path.exists(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv'):\n",
    "            os.remove(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv')\n",
    "        \n",
    "        del dtw_results, valid_dtw_pairs, segments_a, segments_b\n",
    "        del depth_boundaries_a, depth_boundaries_b, dtw_distance_matrix_full\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in {combo_id}: {str(e)}\")\n",
    "        if os.path.exists(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv'):\n",
    "            os.remove(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv')\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "print(\"✓ All original parameter combinations processed\" if test_age_constraint_removal else \"✓ All parameter combinations processed\")\n",
    "\n",
    "# PHASE 2: Run age constraint removal scenarios (if enabled)\n",
    "if test_age_constraint_removal:\n",
    "    print(\"\\n=== PHASE 2: Running age constraint removal scenarios ===\")\n",
    "    \n",
    "    # Calculate additional scenarios\n",
    "    n_constraints_b = len(age_data_b['in_sequence_ages'])\n",
    "    age_enabled_params = [p for p in parameter_combinations if p['age_consideration']]\n",
    "    constraint_scenarios_per_param = (2 ** n_constraints_b) - 1  # Exclude empty set\n",
    "    total_additional_scenarios = len(age_enabled_params) * (constraint_scenarios_per_param - 1)  # Exclude original scenario\n",
    "    \n",
    "    print(f\"- Core B has {n_constraints_b} age constraints\")\n",
    "    print(f\"- Processing {total_additional_scenarios} additional constraint removal scenarios\")\n",
    "    \n",
    "    additional_progress = tqdm(total=total_additional_scenarios, desc=\"Age constraint removal scenarios\")\n",
    "    \n",
    "    for param_idx, params in enumerate(age_enabled_params):\n",
    "        \n",
    "        # Reset and reload parameters correctly for each iteration\n",
    "        age_consideration = params['age_consideration']\n",
    "        restricted_age_correlation = params['restricted_age_correlation']\n",
    "        shortest_path_search = params['shortest_path_search']\n",
    "        \n",
    "        # Generate parameter labels\n",
    "        if restricted_age_correlation:\n",
    "            age_label = 'restricted_age'\n",
    "        else:\n",
    "            age_label = 'loose_age'\n",
    "        \n",
    "        search_label = 'optimal' if shortest_path_search else 'random'\n",
    "        combo_id = f\"{age_label}_{search_label}\"\n",
    "        \n",
    "        # Get indices of only in-sequence constraints from the original data\n",
    "        in_sequence_indices = [i for i, flag in enumerate(age_data_b['in_sequence_flags']) if flag == 1]\n",
    "        n_constraints_b = len(in_sequence_indices)  # Count of in-sequence constraints only\n",
    "\n",
    "        # Generate subsets from in-sequence constraint indices only\n",
    "        all_subsets = generate_constraint_subsets(n_constraints_b)\n",
    "        constraint_subsets = [subset for subset in all_subsets if 0 < len(subset) < n_constraints_b]\n",
    "\n",
    "        for constraint_subset in constraint_subsets:\n",
    "            \n",
    "            # Map subset indices to original constraint indices (in-sequence only)\n",
    "            original_indices = [in_sequence_indices[i] for i in constraint_subset]\n",
    "            \n",
    "            # Create modified age_data_b using original indices\n",
    "            age_data_b_current = {\n",
    "                'depths': [age_data_b['depths'][i] for i in original_indices],\n",
    "                'ages': [age_data_b['ages'][i] for i in original_indices],\n",
    "                'pos_errors': [age_data_b['pos_errors'][i] for i in original_indices],\n",
    "                'neg_errors': [age_data_b['neg_errors'][i] for i in original_indices],\n",
    "                'in_sequence_flags': [age_data_b['in_sequence_flags'][i] for i in original_indices],\n",
    "                'in_sequence_depths': [age_data_b['depths'][i] for i in original_indices],  # Same as depths since all are in-sequence\n",
    "                'in_sequence_ages': [age_data_b['ages'][i] for i in original_indices],\n",
    "                'in_sequence_pos_errors': [age_data_b['pos_errors'][i] for i in original_indices],\n",
    "                'in_sequence_neg_errors': [age_data_b['neg_errors'][i] for i in original_indices],\n",
    "                'out_sequence_depths': age_data_b['out_sequence_depths'],\n",
    "                'out_sequence_ages': age_data_b['out_sequence_ages'],\n",
    "                'out_sequence_pos_errors': age_data_b['out_sequence_pos_errors'],\n",
    "                'out_sequence_neg_errors': age_data_b['out_sequence_neg_errors'],\n",
    "                'core': [age_data_b['core'][i] for i in original_indices],\n",
    "                'interpreted_bed': [age_data_b['interpreted_bed'][i] for i in original_indices]\n",
    "            }\n",
    "            \n",
    "            # Recalculate ages for core B with modified constraints\n",
    "            pickeddepth_ages_b_current = calculate_interpolated_ages(\n",
    "                picked_depths=all_depths_b_cat1,\n",
    "                age_constraints_depths=age_data_b_current['depths'],\n",
    "                age_constraints_ages=age_data_b_current['ages'],\n",
    "                age_constraints_pos_errors=age_data_b_current['pos_errors'],\n",
    "                age_constraints_neg_errors=age_data_b_current['neg_errors'],\n",
    "                age_constraints_in_sequence_flags=age_data_b_current['in_sequence_flags'],\n",
    "                age_constraint_source_core=age_data_b_current['core'],\n",
    "                top_bottom=True,\n",
    "                top_depth=0.0,\n",
    "                bottom_depth=md_b[-1],\n",
    "                top_age=0,\n",
    "                top_age_pos_error=75,\n",
    "                top_age_neg_error=75,\n",
    "                uncertainty_method=uncertainty_method,\n",
    "                n_monte_carlo=10000,\n",
    "                show_plot=False,\n",
    "                core_name=CORE_B,\n",
    "                export_csv=False,\n",
    "                mute_mode=True\n",
    "            )\n",
    "            \n",
    "            scenario_id = f\"{combo_id}_b{len(constraint_subset)}of{n_constraints_b}\"\n",
    "            \n",
    "            try:\n",
    "                # Run DTW analysis with correctly loaded parameters\n",
    "                dtw_results, valid_dtw_pairs, segments_a, segments_b, depth_boundaries_a, depth_boundaries_b, dtw_distance_matrix_full = run_comprehensive_dtw_analysis(\n",
    "                    log_a, log_b, md_a, md_b,\n",
    "                    picked_depths_a=all_depths_a_cat1,\n",
    "                    picked_depths_b=all_depths_b_cat1,\n",
    "                    independent_dtw=False,\n",
    "                    top_bottom=True,\n",
    "                    top_depth=0.0,\n",
    "                    exclude_deadend=True,\n",
    "                    mute_mode=True,\n",
    "                    age_consideration=age_consideration,\n",
    "                    ages_a=pickeddepth_ages_a,  # Use original ages for core A\n",
    "                    ages_b=pickeddepth_ages_b_current,  # Use modified ages for core B\n",
    "                    restricted_age_correlation=restricted_age_correlation,\n",
    "                    all_constraint_ages_a=age_data_a['in_sequence_ages'],  # Original constraints for core A\n",
    "                    all_constraint_ages_b=age_data_b_current['in_sequence_ages'],  # Modified constraints for core B\n",
    "                    all_constraint_depths_a=age_data_a['in_sequence_depths'],  # Original depths for core A\n",
    "                    all_constraint_depths_b=age_data_b_current['in_sequence_depths'],  # Modified depths for core B\n",
    "                    all_constraint_pos_errors_a=age_data_a['in_sequence_pos_errors'],  # Original errors for core A\n",
    "                    all_constraint_pos_errors_b=age_data_b_current['in_sequence_pos_errors'],  # Modified errors for core B\n",
    "                    all_constraint_neg_errors_a=age_data_a['in_sequence_neg_errors'],  # Original errors for core A\n",
    "                    all_constraint_neg_errors_b=age_data_b_current['in_sequence_neg_errors']  # Modified errors for core B\n",
    "                )\n",
    "                \n",
    "                # Find paths with correct parameters\n",
    "                if shortest_path_search:\n",
    "                    _ = find_complete_core_paths(\n",
    "                        valid_dtw_pairs, segments_a, segments_b, log_a, log_b,\n",
    "                        depth_boundaries_a, depth_boundaries_b, dtw_results,\n",
    "                        output_csv=f'temp_mappings_{CORE_A}_{CORE_B}_{scenario_id}.csv',\n",
    "                        start_from_top_only=True, batch_size=1000, n_jobs=-1,\n",
    "                        shortest_path_search=True, shortest_path_level=2,\n",
    "                        max_search_path=100000, mute_mode=True\n",
    "                    )\n",
    "                else:\n",
    "                    _ = find_complete_core_paths(\n",
    "                        valid_dtw_pairs, segments_a, segments_b, log_a, log_b,\n",
    "                        depth_boundaries_a, depth_boundaries_b, dtw_results,\n",
    "                        output_csv=f'temp_mappings_{CORE_A}_{CORE_B}_{scenario_id}.csv',\n",
    "                        start_from_top_only=True, batch_size=1000, n_jobs=-1,\n",
    "                        shortest_path_search=False, shortest_path_level=2,\n",
    "                        max_search_path=100000, mute_mode=True\n",
    "                    )\n",
    "                \n",
    "                # Process quality indices\n",
    "                for quality_index in quality_indices:\n",
    "                    \n",
    "                    _, _, fit_params = plot_correlation_distribution(\n",
    "                        csv_file=f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{scenario_id}.csv',\n",
    "                        quality_index=quality_index,\n",
    "                        no_bins=30, save_png=False, pdf_method='normal',\n",
    "                        kde_bandwidth=0.05, mute_mode=True\n",
    "                    )\n",
    "                    \n",
    "                    if fit_params is not None:\n",
    "                        fit_params_copy = fit_params.copy()\n",
    "                        # Store the correct parameter values\n",
    "                        fit_params_copy['combination_id'] = combo_id\n",
    "                        fit_params_copy['age_consideration'] = age_consideration\n",
    "                        fit_params_copy['restricted_age_correlation'] = restricted_age_correlation\n",
    "                        fit_params_copy['shortest_path_search'] = shortest_path_search\n",
    "                        \n",
    "                        # Add constraint tracking with correct counts\n",
    "                        fit_params_copy['core_a_constraints_count'] = len(age_data_a['in_sequence_ages'])  # Original count for core A\n",
    "                        fit_params_copy['core_b_constraints_count'] = len(constraint_subset)  # Modified count for core B\n",
    "                        # Convert 0-based indices to 1-based for constraint description\n",
    "                        remaining_indices_1based = [i + 1 for i in sorted(constraint_subset)]\n",
    "                        fit_params_copy['constraint_scenario_description'] = f'constraints_{remaining_indices_1based}_remained'\n",
    "                        \n",
    "                        # Append to CSV with correct filename\n",
    "                        if quality_index == 'corr_coef':\n",
    "                            master_csv_filename = f'outputs/r-values_fit_params_{log_suffix}_{CORE_A}_{CORE_B}.csv'\n",
    "                        else:\n",
    "                            master_csv_filename = f'outputs/{quality_index}_fit_params_{log_suffix}_{CORE_A}_{CORE_B}.csv'\n",
    "                        \n",
    "                        df_single = pd.DataFrame([fit_params_copy])\n",
    "                        df_single.to_csv(master_csv_filename, mode='a', index=False, header=False)\n",
    "                        del df_single, fit_params_copy\n",
    "                    \n",
    "                    del fit_params\n",
    "                \n",
    "                # Clean up temporary files\n",
    "                if os.path.exists(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{scenario_id}.csv'):\n",
    "                    os.remove(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{scenario_id}.csv')\n",
    "                \n",
    "                del dtw_results, valid_dtw_pairs, segments_a, segments_b\n",
    "                del depth_boundaries_a, depth_boundaries_b, dtw_distance_matrix_full\n",
    "                del age_data_b_current, pickeddepth_ages_b_current\n",
    "                gc.collect()\n",
    "                \n",
    "                # Update progress\n",
    "                additional_progress.update(1)\n",
    "                additional_progress.set_description(f\"Constraint scenario: {len(constraint_subset)}/{n_constraints_b} constraints\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error in {scenario_id}: {str(e)}\")\n",
    "                if os.path.exists(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{scenario_id}.csv'):\n",
    "                    os.remove(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{scenario_id}.csv')\n",
    "                # Clean up variables in case of error\n",
    "                if 'age_data_b_current' in locals():\n",
    "                    del age_data_b_current\n",
    "                if 'pickeddepth_ages_b_current' in locals():\n",
    "                    del pickeddepth_ages_b_current\n",
    "                gc.collect()\n",
    "                additional_progress.update(1)\n",
    "                continue\n",
    "    \n",
    "    additional_progress.close()\n",
    "    print(\"✓ Phase 2 completed: All age constraint removal scenarios processed\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n=== Age constraint removal testing disabled ===\")\n",
    "\n",
    "print(f\"\\n✓ All processing completed\")\n",
    "for quality_index in quality_indices:\n",
    "    if LOG_COLUMNS == ['hiresMS']:\n",
    "        log_suffix = 'MSonly'\n",
    "    elif LOG_COLUMNS == ['hiresMS','CT', 'Lumin']:\n",
    "        log_suffix = 'MSCTLumin'\n",
    "    else:\n",
    "        log_suffix = 'unspecified'\n",
    "    \n",
    "    if quality_index == 'corr_coef':\n",
    "        filename = f'outputs/r-values_fit_params_{log_suffix}_{CORE_A}_{CORE_B}.csv'\n",
    "    else:\n",
    "        filename = f'outputs/{quality_index}_fit_params_{log_suffix}_{CORE_A}_{CORE_B}.csv'\n",
    "    print(f\"✓ {quality_index} fit_params saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell: Multi-Parameter Distribution Analysis\n",
    "# # Run all parameter combinations and plot distribution curves together\n",
    "\n",
    "# # Define all parameter combinations to test\n",
    "# parameter_combinations = [\n",
    "#     # {'age_consideration': True, 'restricted_age_correlation': True, 'shortest_path_search': False},\n",
    "#     {'age_consideration': True, 'restricted_age_correlation': True, 'shortest_path_search': True},\n",
    "#     # {'age_consideration': True, 'restricted_age_correlation': False, 'shortest_path_search': False},\n",
    "#     # {'age_consideration': True, 'restricted_age_correlation': False, 'shortest_path_search': True},\n",
    "#     # {'age_consideration': False, 'restricted_age_correlation': None, 'shortest_path_search': False},\n",
    "#     {'age_consideration': False, 'restricted_age_correlation': None, 'shortest_path_search': True}\n",
    "# ]\n",
    "\n",
    "# # Define all quality indices to process\n",
    "# quality_indices = ['corr_coef', 'norm_dtw', 'perc_diag']\n",
    "\n",
    "# # Prepare master CSV files for incremental saving\n",
    "# os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "# print(f\"Running {len(parameter_combinations)} parameter combinations for {len(quality_indices)} quality indices...\")\n",
    "\n",
    "# # Loop through all parameter combinations\n",
    "# for idx, params in enumerate(tqdm(parameter_combinations, desc=\"Parameter combinations\")):\n",
    "    \n",
    "#     # Extract parameters\n",
    "#     age_consideration = params['age_consideration']\n",
    "#     restricted_age_correlation = params['restricted_age_correlation']\n",
    "#     shortest_path_search = params['shortest_path_search']\n",
    "    \n",
    "#     # Generate parameter labels for file naming\n",
    "#     if age_consideration:\n",
    "#         if restricted_age_correlation:\n",
    "#             age_label = 'restricted_age'\n",
    "#         else:\n",
    "#             age_label = 'loose_age'\n",
    "#     else:\n",
    "#         age_label = 'no_age'\n",
    "    \n",
    "#     search_label = 'optimal' if shortest_path_search else 'random'\n",
    "    \n",
    "#     # Create unique identifier for this combination\n",
    "#     combo_id = f\"{age_label}_{search_label}\"\n",
    "    \n",
    "#     # print(f\"\\n--- Running combination {idx+1}/{len(parameter_combinations)}: {combo_id} ---\")\n",
    "#     # print(f\"age_consideration={age_consideration}, restricted_age_correlation={restricted_age_correlation}, shortest_path_search={shortest_path_search}\")\n",
    "    \n",
    "#     try:        \n",
    "#         # Run comprehensive DTW analysis\n",
    "#         dtw_results, valid_dtw_pairs, segments_a, segments_b, depth_boundaries_a, depth_boundaries_b, dtw_distance_matrix_full = run_comprehensive_dtw_analysis(\n",
    "#             log_a, log_b, md_a, md_b,\n",
    "#             picked_depths_a=all_depths_a_cat1,\n",
    "#             picked_depths_b=all_depths_b_cat1,\n",
    "#             independent_dtw=False,\n",
    "#             top_bottom=True,\n",
    "#             top_depth=0.0,\n",
    "#             exclude_deadend=True,\n",
    "#             mute_mode=True,\n",
    "#             age_consideration=age_consideration,\n",
    "#             ages_a=pickeddepth_ages_a if age_consideration else None,\n",
    "#             ages_b=pickeddepth_ages_b if age_consideration else None,\n",
    "#             restricted_age_correlation=restricted_age_correlation if age_consideration else False,\n",
    "#             all_constraint_ages_a=age_data_a['in_sequence_ages'] if age_consideration else None,\n",
    "#             all_constraint_ages_b=age_data_b['in_sequence_ages'] if age_consideration else None,\n",
    "#             all_constraint_depths_a=age_data_a['in_sequence_depths'] if age_consideration else None,\n",
    "#             all_constraint_depths_b=age_data_b['in_sequence_depths'] if age_consideration else None,\n",
    "#             all_constraint_pos_errors_a=age_data_a['in_sequence_pos_errors'] if age_consideration else None,\n",
    "#             all_constraint_pos_errors_b=age_data_b['in_sequence_pos_errors'] if age_consideration else None,\n",
    "#             all_constraint_neg_errors_a=age_data_a['in_sequence_neg_errors'] if age_consideration else None,\n",
    "#             all_constraint_neg_errors_b=age_data_b['in_sequence_neg_errors'] if age_consideration else None\n",
    "#         )\n",
    "        \n",
    "#         # Find complete core paths (only for shortest path search - as requested)\n",
    "#         if shortest_path_search:\n",
    "#             _ = find_complete_core_paths(\n",
    "#                 valid_dtw_pairs,\n",
    "#                 segments_a,\n",
    "#                 segments_b,\n",
    "#                 log_a,\n",
    "#                 log_b,\n",
    "#                 depth_boundaries_a,\n",
    "#                 depth_boundaries_b,\n",
    "#                 dtw_results,\n",
    "#                 output_csv=f'temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv',\n",
    "#                 start_from_top_only=True,\n",
    "#                 batch_size=1000,\n",
    "#                 n_jobs=-1,\n",
    "#                 shortest_path_search=True,\n",
    "#                 shortest_path_level=2,\n",
    "#                 max_search_path=100000,\n",
    "#                 mute_mode=True\n",
    "#             )\n",
    "#         else:\n",
    "#             # For random search, still run but with different parameters\n",
    "#             _ = find_complete_core_paths(\n",
    "#                 valid_dtw_pairs,\n",
    "#                 segments_a,\n",
    "#                 segments_b,\n",
    "#                 log_a,\n",
    "#                 log_b,\n",
    "#                 depth_boundaries_a,\n",
    "#                 depth_boundaries_b,\n",
    "#                 dtw_results,\n",
    "#                 output_csv=f'temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv',\n",
    "#                 start_from_top_only=True,\n",
    "#                 batch_size=1000,\n",
    "#                 n_jobs=-1,\n",
    "#                 shortest_path_search=False,\n",
    "#                 shortest_path_level=2,\n",
    "#                 max_search_path=100000,\n",
    "#                 mute_mode=True\n",
    "#             )\n",
    "        \n",
    "#         # Loop through all quality indices for this parameter combination\n",
    "#         for quality_index in quality_indices:\n",
    "            \n",
    "#             # Extract fit_params using plot_correlation_distribution in mute mode\n",
    "#             _, _, fit_params = plot_correlation_distribution(\n",
    "#                 csv_file=f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv',\n",
    "#                 quality_index=quality_index,\n",
    "#                 no_bins=30,\n",
    "#                 save_png=False,\n",
    "#                 pdf_method='normal',\n",
    "#                 kde_bandwidth=0.05,\n",
    "#                 mute_mode=True\n",
    "#             )\n",
    "            \n",
    "#             # Store fit_params with parameter combination info\n",
    "#             if fit_params is not None:\n",
    "#                 fit_params_copy = fit_params.copy()\n",
    "#                 fit_params_copy['combination_id'] = combo_id\n",
    "#                 fit_params_copy['age_consideration'] = age_consideration\n",
    "#                 fit_params_copy['restricted_age_correlation'] = restricted_age_correlation\n",
    "#                 fit_params_copy['shortest_path_search'] = shortest_path_search\n",
    "#                 fit_params_copy['combination_index'] = idx\n",
    "                \n",
    "#                 # Define master CSV filename based on quality index\n",
    "#                 if quality_index == 'corr_coef':\n",
    "#                     master_csv_filename = f'outputs/r-values_fit_params_{CORE_A}_{CORE_B}.csv'\n",
    "#                 else:\n",
    "#                     master_csv_filename = f'outputs/{quality_index}_fit_params_{CORE_A}_{CORE_B}.csv'\n",
    "                \n",
    "#                 # Incrementally save to master CSV\n",
    "#                 df_single = pd.DataFrame([fit_params_copy])\n",
    "#                 if idx == 0:\n",
    "#                     # Write header for first combination\n",
    "#                     df_single.to_csv(master_csv_filename, mode='w', index=False, header=True)\n",
    "#                 else:\n",
    "#                     # Append subsequent combinations without header\n",
    "#                     df_single.to_csv(master_csv_filename, mode='a', index=False, header=False)\n",
    "\n",
    "#                 del df_single, fit_params_copy\n",
    "#             else:\n",
    "#                 print(f\"✗ No fit_params generated for {combo_id} with {quality_index}\")\n",
    "            \n",
    "#             # Clean up fit_params for this quality index\n",
    "#             del fit_params\n",
    "        \n",
    "#         # Clean up temporary files\n",
    "#         if os.path.exists(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv'):\n",
    "#             os.remove(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv')\n",
    "        \n",
    "#         # Clean up variables and force garbage collection\n",
    "#         del dtw_results, valid_dtw_pairs, segments_a, segments_b\n",
    "#         del depth_boundaries_a, depth_boundaries_b, dtw_distance_matrix_full\n",
    "#         gc.collect()\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"✗ Error in combination {combo_id}: {str(e)}\")\n",
    "#         # Clean up on error\n",
    "#         if os.path.exists(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv'):\n",
    "#             os.remove(f'outputs/temp_mappings_{CORE_A}_{CORE_B}_{combo_id}.csv')\n",
    "#         gc.collect()\n",
    "#         continue\n",
    "\n",
    "# print(f\"\\n✓ Completed all parameter combinations for all quality indices\")\n",
    "# for quality_index in quality_indices:\n",
    "#     if quality_index == 'corr_coef':\n",
    "#         filename = f'outputs/r-values_fit_params_{CORE_A}_{CORE_B}.csv'\n",
    "#     else:\n",
    "#         filename = f'outputs/{quality_index}_fit_params_{CORE_A}_{CORE_B}.csv'\n",
    "#     print(f\"✓ {quality_index} fit_params saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions - moved outside the loop\n",
    "def reconstruct_raw_data_from_histogram(bins, hist_percentages, n_points):\n",
    "    \"\"\"Reconstruct raw data points from histogram bins and percentages\"\"\"\n",
    "    raw_data = []\n",
    "    \n",
    "    # Convert percentages to raw counts\n",
    "    raw_counts = (hist_percentages * n_points) / 100\n",
    "    \n",
    "    # Generate data points for each bin\n",
    "    for i, count in enumerate(raw_counts):\n",
    "        if count > 0:\n",
    "            n_samples = int(round(count))\n",
    "            if n_samples > 0:\n",
    "                # Sample uniformly within the bin\n",
    "                bin_samples = np.random.uniform(bins[i], bins[i+1], n_samples)\n",
    "                raw_data.extend(bin_samples)\n",
    "    \n",
    "    return np.array(raw_data)\n",
    "\n",
    "def cohens_d(x, y):\n",
    "    \"\"\"Calculate Cohen's d for effect size between two samples\"\"\"\n",
    "    n1, n2 = len(x), len(y)\n",
    "    s1, s2 = np.std(x, ddof=1), np.std(y, ddof=1)\n",
    "    # Pooled standard deviation\n",
    "    s_pooled = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n",
    "    # Cohen's d\n",
    "    d = (np.mean(x) - np.mean(y)) / s_pooled\n",
    "    return d\n",
    "\n",
    "# Loop through all quality indices\n",
    "quality_indices = ['corr_coef', 'norm_dtw', 'perc_diag']\n",
    "\n",
    "for quality_index in quality_indices:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROCESSING QUALITY INDEX: {quality_index}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Now create combined plot from master CSV\n",
    "    print(f\"\\n--- Creating combined distribution plot for {quality_index} ---\")\n",
    "\n",
    "    # Load all fit_params from master CSV\n",
    "    if LOG_COLUMNS == ['hiresMS']:\n",
    "        log_suffix = 'MSonly'\n",
    "    elif LOG_COLUMNS == ['hiresMS','CT', 'Lumin']:\n",
    "        log_suffix = 'MSCTLumin'\n",
    "    else:\n",
    "        log_suffix = 'unspecified'\n",
    "    \n",
    "    if quality_index == 'corr_coef':\n",
    "        master_csv_filename = f'outputs/r-values_fit_params_{log_suffix}_{CORE_A}_{CORE_B}.csv'\n",
    "    else:\n",
    "        master_csv_filename = f'outputs/{quality_index}_fit_params_{log_suffix}_{CORE_A}_{CORE_B}.csv'\n",
    "    \n",
    "    # Check if master CSV exists\n",
    "    if not os.path.exists(master_csv_filename):\n",
    "        print(f\"✗ Error: Master CSV file not found: {master_csv_filename}\")\n",
    "        print(f\"   Skipping {quality_index} and moving to next index...\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        df_all_params = pd.read_csv(master_csv_filename)\n",
    "        print(f\"✓ Loaded master CSV: {master_csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading master CSV {master_csv_filename}: {str(e)}\")\n",
    "        print(f\"   Skipping {quality_index} and moving to next index...\")\n",
    "        continue\n",
    "\n",
    "    # Define which categories to load - you can customize these filters\n",
    "    load_filters = {\n",
    "        'age_consideration': [True, False],  # Load both True and False, or specify [True] for only True\n",
    "        # 'restricted_age_correlation': [True, False, None],  # Load both True, False, and empty/NaN values\n",
    "        'restricted_age_correlation': [True, None],  # Load both True, False, and empty/NaN values\n",
    "        # 'shortest_path_search': [True, False]  # Load both, or specify [True] for only optimal search\n",
    "        'shortest_path_search': [True]  # Load both, or specify [True] for only optimal search\n",
    "    }\n",
    "\n",
    "    # Apply filters with NaN handling\n",
    "    mask = pd.Series([True] * len(df_all_params))\n",
    "    for column, values in load_filters.items():\n",
    "        if values is not None:  # Allow None to skip filtering on that parameter\n",
    "            # Handle NaN/empty values in the filter\n",
    "            if None in values:\n",
    "                # Include rows where the column is NaN or in the specified values\n",
    "                mask &= (df_all_params[column].isin([v for v in values if v is not None]) | \n",
    "                        df_all_params[column].isna())\n",
    "            else:\n",
    "                # Only include non-NaN values that match the filter\n",
    "                mask &= df_all_params[column].isin(values)\n",
    "\n",
    "    df_all_params = df_all_params[mask]\n",
    "    print(f\"Loaded {len(df_all_params)} rows after filtering\")\n",
    "\n",
    "    # Show what combinations were loaded (handle NaN display)\n",
    "    print(\"Loaded parameter combinations:\")\n",
    "    loaded_combos = df_all_params.groupby(['age_consideration', 'restricted_age_correlation', 'shortest_path_search'], dropna=False).size().reset_index(name='count')\n",
    "    for idx, row in loaded_combos.iterrows():\n",
    "        restricted_val = row['restricted_age_correlation'] if pd.notna(row['restricted_age_correlation']) else 'None/Empty'\n",
    "        print(f\"  age_consideration={row['age_consideration']}, restricted_age_correlation={restricted_val}, shortest_path_search={row['shortest_path_search']} ({row['count']} rows)\")\n",
    "\n",
    "    #####\n",
    "\n",
    "    # Load synthetic fit params from CSV for background\n",
    "    # Define input filename based on log columns and quality index\n",
    "    if LOG_COLUMNS == ['hiresMS']:\n",
    "        synthetic_csv_filename = f'outputs/synthetic_PDFs_MSonly_{quality_index}.csv'\n",
    "    elif LOG_COLUMNS == ['hiresMS','CT', 'Lumin']:\n",
    "        synthetic_csv_filename = f'outputs/synthetic_PDFs_MSCTLumin_{quality_index}.csv'\n",
    "    else:\n",
    "        synthetic_csv_filename = f'outputs/synthetic_PDFs_unspecified_{quality_index}.csv'\n",
    "    # Check if synthetic CSV exists\n",
    "    if not os.path.exists(synthetic_csv_filename):\n",
    "        print(f\"✗ Error: Synthetic CSV file not found: {synthetic_csv_filename}\")\n",
    "        print(f\"   Skipping {quality_index} and moving to next index...\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        df_synthetic_params = pd.read_csv(synthetic_csv_filename)\n",
    "        print(f\"✓ Loaded synthetic CSV: {synthetic_csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading synthetic CSV {synthetic_csv_filename}: {str(e)}\")\n",
    "        print(f\"   Skipping {quality_index} and moving to next index...\")\n",
    "        continue\n",
    "\n",
    "    # Reconstruct combined data from binned data (same as file_context_0)\n",
    "    all_raw_data = []\n",
    "\n",
    "    # Process each iteration to reconstruct raw data from binned data\n",
    "    for _, row in df_synthetic_params.iterrows():\n",
    "        # Extract binned data\n",
    "        bins = np.fromstring(row['bins'].strip('[]'), sep=' ') if 'bins' in row and pd.notna(row['bins']) else None\n",
    "        hist_percentages = np.fromstring(row['hist'].strip('[]'), sep=' ') if 'hist' in row and pd.notna(row['hist']) else None\n",
    "        n_points = row['n_points'] if 'n_points' in row and pd.notna(row['n_points']) else None\n",
    "        \n",
    "        if bins is not None and hist_percentages is not None and n_points is not None:\n",
    "            # Convert percentages back to raw counts\n",
    "            raw_counts = (hist_percentages * n_points) / 100\n",
    "            \n",
    "            # Reconstruct data points by sampling from each bin\n",
    "            bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "            bin_width = bins[1] - bins[0]\n",
    "            \n",
    "            for i, count in enumerate(raw_counts):\n",
    "                if count > 0:\n",
    "                    # Generate random points within each bin\n",
    "                    n_samples = int(round(count))\n",
    "                    if n_samples > 0:\n",
    "                        # Sample uniformly within the bin\n",
    "                        bin_samples = np.random.uniform(\n",
    "                            bins[i], bins[i+1], n_samples\n",
    "                        )\n",
    "                        all_raw_data.extend(bin_samples)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    combined_data = np.array(all_raw_data)\n",
    "\n",
    "    # Fit normal distribution to combined data\n",
    "    from scipy import stats\n",
    "    fitted_mean, fitted_std = stats.norm.fit(combined_data)\n",
    "\n",
    "    # Generate fitted curve\n",
    "    x_fitted = np.linspace(combined_data.min(), combined_data.max(), 1000)\n",
    "    y_fitted = stats.norm.pdf(x_fitted, fitted_mean, fitted_std)\n",
    "\n",
    "    # Create combined plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 4.5))\n",
    "\n",
    "    # Plot combined histogram in gray bars (same style as file_context_0)\n",
    "    n_bins = 30\n",
    "    ax.hist(combined_data, bins=n_bins, alpha=0.3, color='gray', density=True, label='Synthetic Data')\n",
    "\n",
    "    # Plot fitted normal curve as gray dotted line (same style as original synthetic curves)\n",
    "    ax.plot(x_fitted, y_fitted, color='gray', linestyle=':', linewidth=2, alpha=0.8,\n",
    "            label='Null Hypothesis PDFs')\n",
    "\n",
    "    # Get unique combinations available in the CSV\n",
    "    unique_combinations = df_all_params.drop_duplicates(\n",
    "        subset=['age_consideration', 'restricted_age_correlation', 'shortest_path_search']\n",
    "    )[['age_consideration', 'restricted_age_correlation', 'shortest_path_search']].to_dict('records')\n",
    "\n",
    "    print(f\"Found {len(unique_combinations)} unique parameter combinations in CSV:\")\n",
    "    for combo in unique_combinations:\n",
    "        print(f\"  {combo}\")\n",
    "\n",
    "    # Find max constraint counts for determining which curves to highlight\n",
    "    max_core_a_constraints = df_all_params['core_a_constraints_count'].max()\n",
    "    max_core_b_constraints = df_all_params['core_b_constraints_count'].max()\n",
    "    \n",
    "    print(f\"Max core_a_constraints_count: {max_core_a_constraints}\")\n",
    "    print(f\"Max core_b_constraints_count: {max_core_b_constraints}\")\n",
    "\n",
    "    # Set up colormap based on core_b_constraints_count\n",
    "    import matplotlib.cm as cm\n",
    "    import matplotlib.colors as colors\n",
    "    \n",
    "    # Get range of core_b_constraints_count values\n",
    "    min_core_b = df_all_params['core_b_constraints_count'].min()\n",
    "    max_core_b = df_all_params['core_b_constraints_count'].max()\n",
    "    \n",
    "    # Create colormap\n",
    "    cmap = cm.get_cmap('coolwarm')  # You can change this to other colormaps like 'viridis''plasma', 'inferno', etc.\n",
    "    norm = colors.Normalize(vmin=min_core_b, vmax=max_core_b)\n",
    "\n",
    "    # Separate combinations by search method and collect mean values for max constraint cases only\n",
    "    optimal_combinations = []  # For shortest_path_search=True\n",
    "    random_combinations = []   # For shortest_path_search=False\n",
    "\n",
    "    solid_mean_values = []  # For optimal search group (max constraints only)\n",
    "    dash_mean_values = []   # For random search group (max constraints only)\n",
    "\n",
    "    for combo in unique_combinations:\n",
    "        combo_data = df_all_params[\n",
    "            (df_all_params['age_consideration'] == combo['age_consideration']) &\n",
    "            (df_all_params['restricted_age_correlation'] == combo['restricted_age_correlation']) &\n",
    "            (df_all_params['shortest_path_search'] == combo['shortest_path_search'])\n",
    "        ]\n",
    "        \n",
    "        combo_mean_values = []\n",
    "        for idx, row in combo_data.iterrows():\n",
    "            if 'mean' in row and pd.notna(row['mean']):\n",
    "                # Only collect means for max constraint cases\n",
    "                if (row['core_a_constraints_count'] == max_core_a_constraints and \n",
    "                    row['core_b_constraints_count'] == max_core_b_constraints):\n",
    "                    combo_mean_values.append(row['mean'])\n",
    "        \n",
    "        # Determine which search method group this combination belongs to\n",
    "        if combo['shortest_path_search']:\n",
    "            optimal_combinations.append(combo)\n",
    "            solid_mean_values.extend(combo_mean_values)\n",
    "        else:\n",
    "            random_combinations.append(combo)\n",
    "            dash_mean_values.extend(combo_mean_values)\n",
    "\n",
    "    # Plot vertical lines for each search method group with text labels (max constraints only)\n",
    "    solid_mean_plotted = False\n",
    "    dash_mean_plotted = False\n",
    "\n",
    "    if solid_mean_values:\n",
    "        for mean_val in solid_mean_values:\n",
    "            label = 'Optimal-Search PDF Means' if not solid_mean_plotted else \"\"\n",
    "            ax.axvline(x=mean_val, color='brown', alpha=0.4, linewidth=4, label=label)\n",
    "            # Add text label for mean value\n",
    "            ax.text(mean_val, ax.get_ylim()[1] * 0.9, f'{mean_val:.3f}', \n",
    "                    rotation=90, ha='right', va='top', color='brown', fontweight='bold')\n",
    "            solid_mean_plotted = True\n",
    "\n",
    "    if dash_mean_values:\n",
    "        for mean_val in dash_mean_values:\n",
    "            label = 'Random-Search PDF Means' if not dash_mean_plotted else \"\"\n",
    "            ax.axvline(x=mean_val, color='green', alpha=0.4, linewidth=4, label=label)\n",
    "            # Add text label for mean value\n",
    "            ax.text(mean_val, ax.get_ylim()[1] * 0.9, f'{mean_val:.3f}', \n",
    "                    rotation=90, ha='right', va='top', color='green', fontweight='bold')\n",
    "            dash_mean_plotted = True\n",
    "\n",
    "    # Collect real data for statistical tests - separate by individual distributions\n",
    "    solid_real_data_by_combo = {}  # Dictionary to store data by combination\n",
    "    dash_real_data_by_combo = {}\n",
    "\n",
    "    # Plot each distribution curve and collect data for statistical tests\n",
    "    solid_count = 0\n",
    "    dash_count = 0\n",
    "\n",
    "    # Track which constraint levels have been plotted for legend\n",
    "    plotted_constraint_levels = set()\n",
    "\n",
    "    for idx, row in df_all_params.iterrows():\n",
    "        # Extract parameter values directly from CSV columns\n",
    "        age_consideration = row['age_consideration']\n",
    "        restricted_age_correlation = row['restricted_age_correlation']\n",
    "        shortest_path_search = row['shortest_path_search']\n",
    "        core_a_constraints = row['core_a_constraints_count']\n",
    "        core_b_constraints = row['core_b_constraints_count']\n",
    "        \n",
    "        # Parse x_range and y_values from stored strings\n",
    "        if 'x_range' in row and 'y_values' in row and pd.notna(row['x_range']) and pd.notna(row['y_values']):\n",
    "            try:\n",
    "                x_range = np.fromstring(row['x_range'].strip('[]'), sep=' ')\n",
    "                y_values = np.fromstring(row['y_values'].strip('[]'), sep=' ')\n",
    "                \n",
    "                # Create unique key for this parameter combination\n",
    "                combo_key = f\"age_{age_consideration}_restricted_{restricted_age_correlation}_shortest_{shortest_path_search}\"\n",
    "                \n",
    "                # Reconstruct raw data from histogram for statistical tests\n",
    "                if 'bins' in row and 'hist' in row and 'n_points' in row and \\\n",
    "                   pd.notna(row['bins']) and pd.notna(row['hist']) and pd.notna(row['n_points']):\n",
    "                    \n",
    "                    bins = np.fromstring(row['bins'].strip('[]'), sep=' ')\n",
    "                    hist_percentages = np.fromstring(row['hist'].strip('[]'), sep=' ')\n",
    "                    n_points = row['n_points']\n",
    "                    \n",
    "                    # Reconstruct raw data points\n",
    "                    raw_data_points = reconstruct_raw_data_from_histogram(bins, hist_percentages, n_points)\n",
    "                    \n",
    "                    # Group by search method based on shortest_path_search column\n",
    "                    if shortest_path_search:\n",
    "                        if combo_key not in solid_real_data_by_combo:\n",
    "                            solid_real_data_by_combo[combo_key] = []\n",
    "                        solid_real_data_by_combo[combo_key].extend(raw_data_points)\n",
    "                    else:\n",
    "                        if combo_key not in dash_real_data_by_combo:\n",
    "                            dash_real_data_by_combo[combo_key] = []\n",
    "                        dash_real_data_by_combo[combo_key].extend(raw_data_points)\n",
    "                \n",
    "                if len(x_range) > 0 and len(y_values) > 0:\n",
    "                    # Create descriptive label based on parameter combination\n",
    "                    # Determine age treatment description\n",
    "                    if age_consideration:\n",
    "                        if restricted_age_correlation:\n",
    "                            age_desc = \"Consider age strictly\"\n",
    "                        else:\n",
    "                            age_desc = \"Consider age loosely\"\n",
    "                    else:\n",
    "                        age_desc = \"Neglect age\"\n",
    "                    \n",
    "                    # Determine if this is a max constraint case\n",
    "                    is_max_constraints = (core_a_constraints == max_core_a_constraints and \n",
    "                                        core_b_constraints == max_core_b_constraints)\n",
    "                    \n",
    "                    # Get color from colormap based on core_b_constraints_count\n",
    "                    color = cmap(norm(core_b_constraints))\n",
    "                    \n",
    "                    # Determine line style and transparency based on constraint levels\n",
    "                    if is_max_constraints:\n",
    "                        line_style = '-'  # Solid line\n",
    "                        linewidth = 2\n",
    "                        alpha = 0.8\n",
    "                        # Create label for constraint level (for legend)\n",
    "                        constraint_label = f'{CORE_B} age constraints: {core_b_constraints}'\n",
    "                        if core_b_constraints not in plotted_constraint_levels:\n",
    "                            label = constraint_label\n",
    "                            plotted_constraint_levels.add(core_b_constraints)\n",
    "                        else:\n",
    "                            label = None  # Don't repeat in legend\n",
    "                    else:\n",
    "                        line_style = '--'  # Dashed line\n",
    "                        linewidth = 1  # Thinner dashed lines\n",
    "                        alpha = 0.5\n",
    "                        # Create label for constraint level (for legend)\n",
    "                        constraint_label = f'{CORE_B} age constraints: {core_b_constraints}'\n",
    "                        if core_b_constraints not in plotted_constraint_levels:\n",
    "                            label = constraint_label\n",
    "                            plotted_constraint_levels.add(core_b_constraints)\n",
    "                        else:\n",
    "                            label = None  # Don't repeat in legend\n",
    "                    \n",
    "                    # Plot distribution curve\n",
    "                    ax.plot(x_range, y_values, \n",
    "                           color=color, \n",
    "                           linestyle=line_style,\n",
    "                           linewidth=linewidth, alpha=alpha, \n",
    "                           label=label)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not plot curve for row {idx}: {str(e)}\")\n",
    "\n",
    "    # Perform statistical tests for each individual distribution (only for max constraints)\n",
    "    solid_stats_by_combo = {}\n",
    "    dash_stats_by_combo = {}\n",
    "\n",
    "    # Filter data to only include max constraint cases for statistical analysis\n",
    "    max_constraint_data = df_all_params[\n",
    "        (df_all_params['core_a_constraints_count'] == max_core_a_constraints) &\n",
    "        (df_all_params['core_b_constraints_count'] == max_core_b_constraints)\n",
    "    ]\n",
    "\n",
    "    # Recalculate real data collections for max constraints only\n",
    "    solid_real_data_by_combo_max = {}\n",
    "    dash_real_data_by_combo_max = {}\n",
    "\n",
    "    for idx, row in max_constraint_data.iterrows():\n",
    "        age_consideration = row['age_consideration']\n",
    "        restricted_age_correlation = row['restricted_age_correlation']\n",
    "        shortest_path_search = row['shortest_path_search']\n",
    "        \n",
    "        combo_key = f\"age_{age_consideration}_restricted_{restricted_age_correlation}_shortest_{shortest_path_search}\"\n",
    "        \n",
    "        if 'bins' in row and 'hist' in row and 'n_points' in row and \\\n",
    "           pd.notna(row['bins']) and pd.notna(row['hist']) and pd.notna(row['n_points']):\n",
    "            \n",
    "            bins = np.fromstring(row['bins'].strip('[]'), sep=' ')\n",
    "            hist_percentages = np.fromstring(row['hist'].strip('[]'), sep=' ')\n",
    "            n_points = row['n_points']\n",
    "            \n",
    "            raw_data_points = reconstruct_raw_data_from_histogram(bins, hist_percentages, n_points)\n",
    "            \n",
    "            if shortest_path_search:\n",
    "                if combo_key not in solid_real_data_by_combo_max:\n",
    "                    solid_real_data_by_combo_max[combo_key] = []\n",
    "                solid_real_data_by_combo_max[combo_key].extend(raw_data_points)\n",
    "            else:\n",
    "                if combo_key not in dash_real_data_by_combo_max:\n",
    "                    dash_real_data_by_combo_max[combo_key] = []\n",
    "                dash_real_data_by_combo_max[combo_key].extend(raw_data_points)\n",
    "\n",
    "    # Calculate statistics for solid (optimal search) combinations - max constraints only\n",
    "    for combo_key, data in solid_real_data_by_combo_max.items():\n",
    "        if len(data) > 1:  # Need at least 2 data points for statistical tests\n",
    "            data_array = np.array(data)\n",
    "            t_stat, p_value = stats.ttest_ind(data_array, combined_data)\n",
    "            \n",
    "            # Replace z_test with one-sample t-test but keep variable name as z_stat\n",
    "            null_mean = np.mean(combined_data)\n",
    "            z_stat, _ = stats.ttest_1samp(data_array, null_mean)\n",
    "            \n",
    "            cohens_d_val = cohens_d(data_array, combined_data)\n",
    "            \n",
    "            solid_stats_by_combo[combo_key] = {\n",
    "                't_stat': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'z_stat': z_stat,  # This is actually a one-sample t-stat now\n",
    "                'cohens_d': cohens_d_val,\n",
    "                'n_samples': len(data_array),\n",
    "                'mean': np.mean(data_array),\n",
    "                'std': np.std(data_array)\n",
    "            }\n",
    "\n",
    "    # Calculate statistics for dash (random search) combinations - max constraints only\n",
    "    for combo_key, data in dash_real_data_by_combo_max.items():\n",
    "        if len(data) > 1:  # Need at least 2 data points for statistical tests\n",
    "            data_array = np.array(data)\n",
    "            t_stat, p_value = stats.ttest_ind(data_array, combined_data)\n",
    "            \n",
    "            # Replace z_test with one-sample t-test but keep variable name as z_stat\n",
    "            null_mean = np.mean(combined_data)\n",
    "            z_stat, _ = stats.ttest_1samp(data_array, null_mean)\n",
    "            \n",
    "            cohens_d_val = cohens_d(data_array, combined_data)\n",
    "            \n",
    "            dash_stats_by_combo[combo_key] = {\n",
    "                't_stat': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'z_stat': z_stat,  # This is actually a one-sample t-stat now\n",
    "                'cohens_d': cohens_d_val,\n",
    "                'n_samples': len(data_array),\n",
    "                'mean': np.mean(data_array),\n",
    "                'std': np.std(data_array)\n",
    "            }\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlabel(f\"{quality_index}\")\n",
    "    ax.set_ylabel('Probability Density (%)')\n",
    "    ax.set_title(f'{quality_index} Distribution Comparison\\n{CORE_A} vs {CORE_B}')\n",
    "\n",
    "    # Create grouped legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    # Separate handles and labels by groups\n",
    "    synthetic_handles = []\n",
    "    synthetic_labels = []\n",
    "    constraint_handles = []\n",
    "    constraint_labels = []\n",
    "    mean_handles = []\n",
    "    mean_labels = []\n",
    "\n",
    "    for handle, label in zip(handles, labels):\n",
    "        if 'Null' in label or 'Synthetic' in label:\n",
    "            synthetic_handles.append(handle)\n",
    "            synthetic_labels.append(label)\n",
    "        elif 'age constraints' in label:\n",
    "            constraint_handles.append(handle)\n",
    "            constraint_labels.append(label)\n",
    "        elif 'PDF Means' in label:\n",
    "            mean_handles.append(handle)\n",
    "            mean_labels.append(label)\n",
    "\n",
    "    # Create grouped legend with titles and detailed statistics\n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    legend_elements = []\n",
    "    legend_labels = []\n",
    "\n",
    "    # Add synthetic group\n",
    "    if synthetic_handles:\n",
    "        # Add group title (invisible line)\n",
    "        legend_elements.append(Line2D([0], [0], color='white', linewidth=0, alpha=0))\n",
    "        legend_labels.append('Null Hypotheses (Synthetic Data)')\n",
    "        # Add actual legend items\n",
    "        legend_elements.extend(synthetic_handles)\n",
    "        legend_labels.extend(synthetic_labels)\n",
    "        # Add spacer\n",
    "        legend_elements.append(Line2D([0], [0], color='white', linewidth=0, alpha=0))\n",
    "        legend_labels.append('')\n",
    "\n",
    "    # Add constraint level group\n",
    "    if constraint_handles:\n",
    "        # Add group title (invisible line)\n",
    "        legend_elements.append(Line2D([0], [0], color='white', linewidth=0, alpha=0))\n",
    "        legend_labels.append('Real Data by Age Constraints')\n",
    "        \n",
    "        # Sort constraint handles and labels by constraint count\n",
    "        constraint_pairs = list(zip(constraint_handles, constraint_labels))\n",
    "        constraint_pairs.sort(key=lambda x: int(x[1].split(': ')[-1]))\n",
    "        \n",
    "        for handle, label in constraint_pairs:\n",
    "            legend_elements.append(handle)\n",
    "            legend_labels.append(label)\n",
    "        \n",
    "        # Add spacer\n",
    "        legend_elements.append(Line2D([0], [0], color='white', linewidth=0, alpha=0))\n",
    "        legend_labels.append('')\n",
    "\n",
    "    # Add mean lines group\n",
    "    if mean_handles:\n",
    "        # Add group title (invisible line)\n",
    "        legend_elements.append(Line2D([0], [0], color='white', linewidth=0, alpha=0))\n",
    "        legend_labels.append('PDF Mean Values')\n",
    "        # Add actual legend items\n",
    "        legend_elements.extend(mean_handles)\n",
    "        legend_labels.extend(mean_labels)\n",
    "\n",
    "    # Apply legend with grouping\n",
    "    legend = ax.legend(legend_elements, legend_labels, bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "\n",
    "    # Style the group title labels\n",
    "    for i, label in enumerate(legend_labels):\n",
    "        if (label.startswith('Null Hypotheses') or \n",
    "            label.startswith('Real Data by Age Constraints') or \n",
    "            label.startswith('PDF Mean Values')):\n",
    "            legend.get_texts()[i].set_weight('bold')\n",
    "            legend.get_texts()[i].set_fontsize(10)\n",
    "            legend.get_texts()[i].set_ha('left')  # Align to left\n",
    "\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if quality_index == 'corr_coef':\n",
    "        ax.set_xlim(0, 1.0)\n",
    "    # For other indices, let matplotlib auto-scale\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if LOG_COLUMNS == ['hiresMS']:\n",
    "        log_suffix = 'MSonly'\n",
    "    elif LOG_COLUMNS == ['hiresMS','CT', 'Lumin']:\n",
    "        log_suffix = 'MSCTLumin'\n",
    "    else:\n",
    "        log_suffix = 'unspecified'\n",
    "    \n",
    "    if quality_index == 'corr_coef':\n",
    "        plt.savefig(f'outputs/r-values_comparison_{log_suffix}_{CORE_A}_{CORE_B}.png', dpi=150, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'outputs/{quality_index}_comparison_{log_suffix}_{CORE_A}_{CORE_B}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed statistical results\n",
    "    print(f\"\\n=== DETAILED STATISTICAL ANALYSIS FOR {quality_index} ===\")\n",
    "    print(f\"Null Hypothesis Distribution:\")\n",
    "    print(f\"  Mean: {fitted_mean:.1f}, SD: {fitted_std:.1f}\")\n",
    "    print(f\"  Sample size: {len(combined_data)}\")\n",
    "    print(f\"  Interpretation: Baseline distribution from synthetic data representing no true correlation\")\n",
    "    print()\n",
    "\n",
    "    print(f\"--- Optimal Search Results (Max Constraints Only) ---\")\n",
    "    for combo_key, stats in solid_stats_by_combo.items():\n",
    "        # Parse combo_key to get descriptive name\n",
    "        if \"age_True_restricted_True\" in combo_key:\n",
    "            desc = \"Consider age strictly\"\n",
    "        elif \"age_True_restricted_False\" in combo_key:\n",
    "            desc = \"Consider age loosely\"\n",
    "        elif \"age_False\" in combo_key:\n",
    "            desc = \"Neglect age\"\n",
    "        else:\n",
    "            desc = combo_key\n",
    "        \n",
    "        print(f\"{desc}:\")\n",
    "        print(f\"  Mean: {stats['mean']:.1f}, SD: {stats['std']:.1f}\")\n",
    "        print(f\"  t-statistic: {stats['t_stat']:.1f} (measures difference between means relative to variation)\")\n",
    "        print(f\"  z-statistic: {stats['z_stat']:.1f} (standard deviations from null hypothesis mean)\")\n",
    "        print(f\"  p-value: {stats['p_value']:.2g}\")\n",
    "        print(f\"  Cohen's d: {stats['cohens_d']:.1f}\")\n",
    "        print(f\"  Sample size: {stats['n_samples']}\")\n",
    "        \n",
    "        # Interpret effect size\n",
    "        if abs(stats['cohens_d']) < 0.2:\n",
    "            effect_size = \"negligible\"\n",
    "        elif abs(stats['cohens_d']) < 0.5:\n",
    "            effect_size = \"small\"\n",
    "        elif abs(stats['cohens_d']) < 0.8:\n",
    "            effect_size = \"medium\"\n",
    "        else:\n",
    "            effect_size = \"large\"\n",
    "        \n",
    "        # Statistical interpretation\n",
    "        if stats['t_stat'] > 0:\n",
    "            direction = \"higher than\"\n",
    "        else:\n",
    "            direction = \"lower than\"\n",
    "        \n",
    "        print(f\"  Effect size: {effect_size} difference between distributions\")\n",
    "        \n",
    "        # Only use \"significantly\" if p-value indicates statistical significance\n",
    "        if stats['p_value'] < 0.05:\n",
    "            print(f\"  Interpretation: Significantly {direction} null hypothesis with {effect_size} effect size\")\n",
    "        else:\n",
    "            print(f\"  Interpretation: no statistical significance (p-value = {stats['p_value']:.2e})\")\n",
    "        print()\n",
    "        \n",
    "\n",
    "    print(f\"--- Random Search Results (Max Constraints Only) ---\")\n",
    "    for combo_key, stats in dash_stats_by_combo.items():\n",
    "        # Parse combo_key to get descriptive name\n",
    "        if \"age_True_restricted_True\" in combo_key:\n",
    "            desc = \"Consider age strictly\"\n",
    "        elif \"age_True_restricted_False\" in combo_key:\n",
    "            desc = \"Consider age loosely\"\n",
    "        elif \"age_False\" in combo_key:\n",
    "            desc = \"Neglect age\"\n",
    "        else:\n",
    "            desc = combo_key\n",
    "        \n",
    "        print(f\"{desc}:\")\n",
    "        print(f\"  Mean: {stats['mean']:.1f}, SD: {stats['std']:.1f}\")\n",
    "        print(f\"  t-statistic: {stats['t_stat']:.1f} (measures difference between means relative to variation)\")\n",
    "        print(f\"  z-statistic: {stats['z_stat']:.1f} (standard deviations from null hypothesis mean)\")\n",
    "        print(f\"  p-value: {stats['p_value']:.2g}\")\n",
    "        print(f\"  Cohen's d: {stats['cohens_d']:.1f}\")\n",
    "        print(f\"  Sample size: {stats['n_samples']}\")\n",
    "        \n",
    "        # Interpret effect size\n",
    "        if abs(stats['cohens_d']) < 0.2:\n",
    "            effect_size = \"negligible\"\n",
    "        elif abs(stats['cohens_d']) < 0.5:\n",
    "            effect_size = \"small\"\n",
    "        elif abs(stats['cohens_d']) < 0.8:\n",
    "            effect_size = \"medium\"\n",
    "        else:\n",
    "            effect_size = \"large\"\n",
    "        \n",
    "        # Statistical interpretation\n",
    "        if stats['t_stat'] > 0:\n",
    "            direction = \"higher than\"\n",
    "        else:\n",
    "            direction = \"lower than\"\n",
    "        \n",
    "        print(f\"  Effect size: {effect_size} difference between distributions\")\n",
    "        \n",
    "        # Only use \"significantly\" if p-value indicates statistical significance\n",
    "        if stats['p_value'] < 0.05:\n",
    "            print(f\"  Interpretation: Significantly {direction} null hypothesis with {effect_size} effect size\")\n",
    "        else:\n",
    "            print(f\"  Interpretation: no statistical significance (p-value = {stats['p_value']:.2e})\")\n",
    "        print()\n",
    "    \n",
    "    if quality_index == 'corr_coef':\n",
    "        print(f\"\\n✓ Combined plot saved as: outputs/r-values_comparison_{log_suffix}_{CORE_A}_{CORE_B}.png\")\n",
    "    else:\n",
    "        print(f\"\\n✓ Combined plot saved as: outputs/{quality_index}_comparison_{log_suffix}_{CORE_A}_{CORE_B}.png\")\n",
    "    print(f\"✓ Analysis complete for {quality_index}!\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ALL QUALITY INDICES PROCESSING COMPLETED\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "11-TimeSeriesCorrelation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
