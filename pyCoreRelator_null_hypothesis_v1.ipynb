{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Required Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import pyCoreRelator functions\n",
    "from pyCoreRelator import (\n",
    "    create_segment_pool_from_available_cores,\n",
    "    generate_synthetic_core_pair,\n",
    "    compute_pycorerelator_null_hypothesis,\n",
    "    plot_correlation_distribution,\n",
    "    run_comprehensive_dtw_analysis,\n",
    "    load_log_data\n",
    ")\n",
    "\n",
    "print(\"Packages imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Core Data\n",
    "# Define core names and log columns\n",
    "CORE_A = \"M9907-23PC\"\n",
    "CORE_B = \"M9907-25PC\"\n",
    "LOG_COLUMNS = ['hiresMS']  # Single log for simplicity\n",
    "DEPTH_COLUMN = 'SB_DEPTH_cm'\n",
    "\n",
    "# Define directory paths\n",
    "mother_dir = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/'\n",
    "\n",
    "# Define paths for Core A\n",
    "core_a_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_hiresMS_MLfilled.csv'\n",
    "}\n",
    "\n",
    "# Define paths for Core B\n",
    "core_b_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_hiresMS_MLfilled.csv'\n",
    "}\n",
    "\n",
    "# Load data for both cores\n",
    "log_a, md_a, available_columns_a, _, _ = load_log_data(\n",
    "    core_a_log_paths,\n",
    "    {},  # No images needed\n",
    "    LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "log_b, md_b, available_columns_b, _, _ = load_log_data(\n",
    "    core_b_log_paths,\n",
    "    {},  # No images needed\n",
    "    LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "print(f\"Loaded {CORE_A}: {len(log_a)} points, columns: {available_columns_a}\")\n",
    "print(f\"Loaded {CORE_B}: {len(log_b)} points, columns: {available_columns_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Boundaries and Run DTW Analysis to Get Segments\n",
    "# Load picked depths from CSV files (or define them manually if needed)\n",
    "pickeddepth_a_csv = f'pickeddepth/{CORE_A}_pickeddepth.csv'\n",
    "pickeddepth_b_csv = f'pickeddepth/{CORE_B}_pickeddepth.csv'\n",
    "\n",
    "# Extract picked depths (category 1 only for simplicity)\n",
    "if os.path.exists(pickeddepth_a_csv):\n",
    "    picked_data_a = pd.read_csv(pickeddepth_a_csv)\n",
    "    all_depths_a_cat1 = picked_data_a[picked_data_a['category'] == 1]['picked_depths_cm'].values.astype('float32')\n",
    "else:\n",
    "    # Define some default boundaries if CSV doesn't exist\n",
    "    all_depths_a_cat1 = np.linspace(0, md_a[-1], 10)  # 10 evenly spaced boundaries\n",
    "\n",
    "if os.path.exists(pickeddepth_b_csv):\n",
    "    picked_data_b = pd.read_csv(pickeddepth_b_csv)\n",
    "    all_depths_b_cat1 = picked_data_b[picked_data_b['category'] == 1]['picked_depths_cm'].values.astype('float32')\n",
    "else:\n",
    "    # Define some default boundaries if CSV doesn't exist\n",
    "    all_depths_b_cat1 = np.linspace(0, md_b[-1], 10)  # 10 evenly spaced boundaries\n",
    "\n",
    "# Run DTW analysis to get segments (no age constraints for null hypothesis)\n",
    "print(\"Running DTW analysis to identify segments...\")\n",
    "dtw_results, valid_dtw_pairs, segments_a, segments_b, depth_boundaries_a, depth_boundaries_b, _ = run_comprehensive_dtw_analysis(\n",
    "    log_a, log_b, md_a, md_b, \n",
    "    picked_depths_a=all_depths_a_cat1, \n",
    "    picked_depths_b=all_depths_b_cat1,\n",
    "    top_bottom=True,\n",
    "    independent_dtw=False,\n",
    "    exclude_deadend=True,\n",
    "    create_dtw_matrix=False,  # Skip visualization for speed\n",
    "    creategif=False,  # Skip animation for speed\n",
    "    age_consideration=False,  # No age constraints for null hypothesis\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(f\"Found {len(valid_dtw_pairs)} valid segment pairs\")\n",
    "print(f\"Core A segments: {len(segments_a)}\")\n",
    "print(f\"Core B segments: {len(segments_b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create Segment Pool from Available Cores\n",
    "print(\"=== Creating Segment Pool ===\")\n",
    "\n",
    "# Prepare data structures for segment pool creation\n",
    "all_cores_data = {\n",
    "    CORE_A: {\n",
    "        'log_data': log_a,\n",
    "        'md_data': md_a\n",
    "    },\n",
    "    CORE_B: {\n",
    "        'log_data': log_b,\n",
    "        'md_data': md_b\n",
    "    }\n",
    "}\n",
    "\n",
    "all_boundaries_data = {\n",
    "    CORE_A: {\n",
    "        'depth_boundaries': depth_boundaries_a,\n",
    "        'segments': segments_a\n",
    "    },\n",
    "    CORE_B: {\n",
    "        'depth_boundaries': depth_boundaries_b,\n",
    "        'segments': segments_b\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create segment pool from available cores\n",
    "segment_pool = create_segment_pool_from_available_cores(all_cores_data, all_boundaries_data)\n",
    "\n",
    "print(f\"Created segment pool with {len(segment_pool)} segments\")\n",
    "\n",
    "# Display summary statistics\n",
    "if segment_pool:\n",
    "    lengths = [seg['length'] for seg in segment_pool]\n",
    "    depth_spans = [seg['depth_span'] for seg in segment_pool]\n",
    "    print(f\"Segment lengths: min={min(lengths)}, max={max(lengths)}, mean={np.mean(lengths):.1f}\")\n",
    "    print(f\"Depth spans: min={min(depth_spans):.1f}, max={max(depth_spans):.1f}, mean={np.mean(depth_spans):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Compute Null Hypothesis Distribution\n",
    "print(\"=== Computing Null Hypothesis Distribution ===\")\n",
    "\n",
    "# Configure target characteristics for synthetic cores\n",
    "target_dimensions = log_a.shape[1] if log_a.ndim > 1 else 1\n",
    "\n",
    "core_a_config = {\n",
    "    'target_length': len(log_a),\n",
    "    'target_dimensions': target_dimensions\n",
    "}\n",
    "\n",
    "core_b_config = {\n",
    "    'target_length': len(log_b), \n",
    "    'target_dimensions': target_dimensions\n",
    "}\n",
    "\n",
    "print(f\"Target Core A: {core_a_config['target_length']} points, {core_a_config['target_dimensions']} dimensions\")\n",
    "print(f\"Target Core B: {core_b_config['target_length']} points, {core_b_config['target_dimensions']} dimensions\")\n",
    "\n",
    "# Compute null hypothesis distribution with 10,000 iterations\n",
    "print(\"Computing null hypothesis with 10,000 synthetic core pairs...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "null_hypothesis_results = compute_pycorerelator_null_hypothesis(\n",
    "    segment_pool=segment_pool,\n",
    "    core_a_config=core_a_config,\n",
    "    core_b_config=core_b_config,\n",
    "    n_iterations=10000,\n",
    "    exponent=0.3,  # Match typical analysis parameters\n",
    "    dtw_distance_threshold=None,\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "# Extract results\n",
    "r_values_null = null_hypothesis_results['r_values_distribution']\n",
    "distribution_stats = null_hypothesis_results['distribution_stats']\n",
    "\n",
    "print(f\"\\n=== NULL HYPOTHESIS RESULTS ===\")\n",
    "print(f\"Successful iterations: {null_hypothesis_results['successful_iterations']}\")\n",
    "print(f\"Failed iterations: {null_hypothesis_results['failed_iterations']}\")\n",
    "print(f\"Mean r-value: {distribution_stats['mean']:.4f} Â± {distribution_stats['std']:.4f}\")\n",
    "print(f\"95th percentile threshold: {distribution_stats['percentile_95']:.4f}\")\n",
    "print(f\"97.5th percentile threshold: {distribution_stats['percentile_97_5']:.4f}\")\n",
    "print(f\"99th percentile threshold: {distribution_stats['percentile_99']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Plot R-value Distribution\n",
    "print(\"=== Plotting R-value Distribution ===\")\n",
    "\n",
    "# Create temporary CSV for plot_correlation_distribution compatibility\n",
    "temp_csv = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)\n",
    "temp_csv_path = temp_csv.name\n",
    "\n",
    "# Write data in the format expected by plot_correlation_distribution\n",
    "with open(temp_csv_path, 'w') as f:\n",
    "    f.write(\"mapping_id,corr_coef\\n\")\n",
    "    for i, r_val in enumerate(r_values_null):\n",
    "        f.write(f\"{i},{r_val}\\n\")\n",
    "\n",
    "temp_csv.close()\n",
    "\n",
    "# Plot null hypothesis distribution\n",
    "try:\n",
    "    fig, ax, fit_params = plot_correlation_distribution(\n",
    "        csv_file=temp_csv_path,\n",
    "        quality_index='corr_coef',\n",
    "        save_png=True,\n",
    "        png_filename=f'Null_Hypothesis_Distribution_{CORE_A}_{CORE_B}.png',\n",
    "        core_a_name=CORE_A,\n",
    "        core_b_name=CORE_B,\n",
    "        pdf_method='skew-normal',  # Use skew-normal for better fit\n",
    "        no_bins=50\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "finally:\n",
    "    # Clean up temporary file\n",
    "    if os.path.exists(temp_csv_path):\n",
    "        os.unlink(temp_csv_path)\n",
    "\n",
    "# Save null hypothesis results for future use\n",
    "results_filename = f'Null_Hypothesis_Results_{CORE_A}_{CORE_B}.csv'\n",
    "null_df = pd.DataFrame({\n",
    "    'iteration': range(len(r_values_null)),\n",
    "    'r_value': r_values_null\n",
    "})\n",
    "null_df.to_csv(results_filename, index=False)\n",
    "print(f\"Null hypothesis r-values saved to: {results_filename}\")\n",
    "\n",
    "print(\"=== NULL HYPOTHESIS TESTING COMPLETE ===\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
