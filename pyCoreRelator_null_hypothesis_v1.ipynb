{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Required Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Extract Core Lengths and Load Segment Pool\n",
    "from pyCoreRelator import load_log_data\n",
    "\n",
    "# Define core names and target parameters\n",
    "CORE_A = \"M9907-23PC\"\n",
    "CORE_B = \"M9907-25PC\"\n",
    "LOG_COLUMNS = ['hiresMS']  # Choose one log column for segment pool\n",
    "DEPTH_COLUMN = 'SB_DEPTH_cm'\n",
    "\n",
    "# Define directory paths\n",
    "mother_dir = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites'\n",
    "\n",
    "# Function to extract core length from depth data\n",
    "def get_core_length(core_name, depth_column='SB_DEPTH_cm'):\n",
    "    \"\"\"Extract maximum depth from core data\"\"\"\n",
    "    # Try hiresMS file first (most common)\n",
    "    depth_file = f'{mother_dir}/Cascadia_core_data/OSU_dataset/_compiled_logs/{core_name}/ML_filled/{core_name}_hiresMS_MLfilled.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(depth_file)\n",
    "        return df[depth_column].max()\n",
    "    except:\n",
    "        print(f\"Warning: Could not read depth from {depth_file}\")\n",
    "        return None\n",
    "\n",
    "# Extract core lengths\n",
    "core_a_length = get_core_length(CORE_A, DEPTH_COLUMN)\n",
    "core_b_length = get_core_length(CORE_B, DEPTH_COLUMN)\n",
    "\n",
    "print(f\"Core A ({CORE_A}) length: {core_a_length} cm\")\n",
    "print(f\"Core B ({CORE_B}) length: {core_b_length} cm\")\n",
    "\n",
    "# Define all cores for segment pool\n",
    "SEGMENT_POOL_CORES = [\"M9907-22PC\", \"M9907-23PC\", \"M9907-25PC\"]\n",
    "\n",
    "# Load segment pool data (turbidite database)\n",
    "segment_pool_cores_data = {}\n",
    "turb_logs = []\n",
    "depth_logs = []\n",
    "\n",
    "print(\"Loading segment pool from available cores...\")\n",
    "\n",
    "for core_name in SEGMENT_POOL_CORES:\n",
    "    print(f\"Processing {core_name}...\")\n",
    "    \n",
    "    # Define log paths for this core\n",
    "    core_log_paths = {\n",
    "        'hiresMS': f'{mother_dir}/Cascadia_core_data/OSU_dataset/_compiled_logs/{core_name}/ML_filled/{core_name}_hiresMS_MLfilled.csv',\n",
    "    }\n",
    "    \n",
    "    # Define column alternatives\n",
    "    column_alternatives = {\n",
    "        'hiresMS': ['MS'],\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load data for segment pool\n",
    "        log_data, md_data, available_columns, _, _ = load_log_data(\n",
    "            core_log_paths,\n",
    "            {},  # No images needed\n",
    "            LOG_COLUMNS,\n",
    "            depth_column=DEPTH_COLUMN,\n",
    "            normalize=True,\n",
    "            column_alternatives=column_alternatives\n",
    "        )\n",
    "        \n",
    "        # Store core data\n",
    "        segment_pool_cores_data[core_name] = {\n",
    "            'log_data': log_data,\n",
    "            'md_data': md_data,\n",
    "            'available_columns': available_columns\n",
    "        }\n",
    "        \n",
    "        # Load turbidite boundaries for this core\n",
    "        picked_file = f'{mother_dir}/pyCoreRelator/pickeddepth/{core_name}_pickeddepth.csv'\n",
    "        try:\n",
    "            picked_df = pd.read_csv(picked_file)\n",
    "            # Filter for category 1 boundaries only\n",
    "            category_1_depths = picked_df[picked_df['category'] == 1]['picked_depths_cm'].values\n",
    "            category_1_depths = np.sort(category_1_depths)  # Ensure sorted order\n",
    "            \n",
    "            # Create turbidite segments (from boundary to boundary)\n",
    "            for i in range(len(category_1_depths) - 1):\n",
    "                start_depth = category_1_depths[i]\n",
    "                end_depth = category_1_depths[i + 1]\n",
    "                \n",
    "                # Find indices corresponding to these depths\n",
    "                start_idx = np.argmin(np.abs(md_data - start_depth))\n",
    "                end_idx = np.argmin(np.abs(md_data - end_depth))\n",
    "                \n",
    "                if end_idx > start_idx:\n",
    "                    # Extract turbidite segment\n",
    "                    turb_segment = log_data[start_idx:end_idx]\n",
    "                    turb_depth = md_data[start_idx:end_idx] - md_data[start_idx]  # Relative depths\n",
    "                    \n",
    "                    turb_logs.append(turb_segment)\n",
    "                    depth_logs.append(turb_depth)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load turbidite boundaries for {core_name}: {e}\")\n",
    "        \n",
    "        print(f\"  Loaded: {len(log_data)} points, columns: {available_columns}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {core_name}: {e}\")\n",
    "\n",
    "print(f\"Segment pool created with {len(turb_logs)} turbidites\")\n",
    "print(f\"Total cores processed: {len(segment_pool_cores_data)}\")\n",
    "\n",
    "# Set target dimensions based on segment pool\n",
    "target_dimensions = turb_logs[0].shape[1] if len(turb_logs) > 0 and turb_logs[0].ndim > 1 else 1\n",
    "\n",
    "print(f\"Target dimensions: {target_dimensions}\")\n",
    "print(f\"Core A target length: {core_a_length} cm\")\n",
    "print(f\"Core B target length: {core_b_length} cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Plot All Turbidite Segments from Pool\n",
    "print(f\"Plotting {len(turb_logs)} turbidite segments from the pool...\")\n",
    "\n",
    "# Create subplot grid\n",
    "n_segments = len(turb_logs)\n",
    "n_cols = 8\n",
    "n_rows = int(np.ceil(n_segments / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_rows))\n",
    "axes = axes.flatten() if n_segments > 1 else [axes]\n",
    "\n",
    "for i, (turb_segment, turb_depth) in enumerate(zip(turb_logs, depth_logs)):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot segment\n",
    "    if turb_segment.ndim > 1:\n",
    "        # Multi-dimensional data - plot first column\n",
    "        ax.plot(turb_segment[:, 0], turb_depth, 'b-', linewidth=1)\n",
    "        ax.set_xlabel(f'{LOG_COLUMNS[0]} (normalized)')\n",
    "    else:\n",
    "        # 1D data\n",
    "        ax.plot(turb_segment, turb_depth, 'b-', linewidth=1)\n",
    "        ax.set_xlabel(f'{LOG_COLUMNS[0]} (normalized)')\n",
    "    \n",
    "    ax.set_ylabel('Relative Depth (cm)')\n",
    "    ax.set_title(f'Segment {i+1}\\n({len(turb_segment)} pts, {turb_depth[-1]:.1f} cm)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.invert_yaxis()  # Depth increases downward\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(n_segments, len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Turbidite Segment Pool ({len(turb_logs)} segments)', y=1.02, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "segment_lengths = [len(seg) for seg in turb_logs]\n",
    "segment_depths = [depth[-1] for depth in depth_logs]\n",
    "\n",
    "print(f\"\\nSegment Pool Summary:\")\n",
    "print(f\"  Total segments: {len(turb_logs)}\")\n",
    "print(f\"  Length range: {min(segment_lengths)}-{max(segment_lengths)} points\")\n",
    "print(f\"  Depth range: {min(segment_depths):.1f}-{max(segment_depths):.1f} cm\")\n",
    "print(f\"  Mean depth: {np.mean(segment_depths):.1f} cm\")\n",
    "print(f\"  Target dimensions: {target_dimensions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create and Plot Synthetic Core Pair\n",
    "import random\n",
    "\n",
    "def create_synthetic_log_with_depths(thickness, turb_logs, depth_logs, exclude_inds=None):\n",
    "    \"\"\"Create synthetic log using turbidite database approach with picked depths at turbidite bases.\"\"\"\n",
    "    fake_log = np.array([]).reshape(0, target_dimensions) if target_dimensions > 1 else np.array([])\n",
    "    md_log = np.array([])\n",
    "    max_depth = 0\n",
    "    inds = []\n",
    "    picked_depths = []\n",
    "    \n",
    "    # Add initial boundary\n",
    "    picked_depths.append((0, 1))\n",
    "    \n",
    "    while max_depth <= thickness:\n",
    "        ind = random.choices(np.arange(len(turb_logs)), k=1)[0]\n",
    "        \n",
    "        # Skip if this index should be excluded\n",
    "        if exclude_inds is not None and ind in exclude_inds:\n",
    "            continue\n",
    "            \n",
    "        inds.append(ind)\n",
    "        \n",
    "        # Get turbidite segment from database\n",
    "        turb_segment = turb_logs[ind]\n",
    "        turb_depths = depth_logs[ind]\n",
    "        \n",
    "        # Ensure turbidite has proper dimensions\n",
    "        if turb_segment.ndim == 1:\n",
    "            turb_segment = turb_segment.reshape(-1, 1)\n",
    "        \n",
    "        # Ensure proper dimensions match target\n",
    "        if turb_segment.shape[1] < target_dimensions:\n",
    "            # Pad with noise if needed\n",
    "            padding = np.random.normal(0, 0.1, (len(turb_segment), target_dimensions - turb_segment.shape[1]))\n",
    "            turb_segment = np.hstack([turb_segment, padding])\n",
    "        elif turb_segment.shape[1] > target_dimensions:\n",
    "            # Truncate if needed\n",
    "            turb_segment = turb_segment[:, :target_dimensions]\n",
    "        \n",
    "        # Append log data\n",
    "        if target_dimensions > 1:\n",
    "            if len(fake_log) == 0:\n",
    "                fake_log = turb_segment.copy()\n",
    "            else:\n",
    "                fake_log = np.vstack((fake_log, turb_segment))\n",
    "        else:\n",
    "            fake_log = np.hstack((fake_log, turb_segment.flatten()))\n",
    "        \n",
    "        # Append depth data\n",
    "        if len(md_log) == 0:\n",
    "            md_log = np.hstack((md_log, 1 + turb_depths))\n",
    "        else:\n",
    "            md_log = np.hstack((md_log, 1 + md_log[-1] + turb_depths))\n",
    "            \n",
    "        max_depth = md_log[-1]\n",
    "        \n",
    "        # Add picked depth at the base of this turbidite (current max_depth)\n",
    "        if max_depth <= thickness:\n",
    "            picked_depths.append((max_depth, 1))\n",
    "    \n",
    "    # Truncate to target thickness\n",
    "    valid_indices = md_log <= thickness\n",
    "    if target_dimensions > 1:\n",
    "        log = fake_log[valid_indices]\n",
    "    else:\n",
    "        log = fake_log[valid_indices]\n",
    "    d = md_log[valid_indices]\n",
    "    \n",
    "    # Filter picked depths to only include those within the valid range\n",
    "    valid_picked_depths = [(depth, category) for depth, category in picked_depths if depth <= thickness]\n",
    "    \n",
    "    # Ensure we have an end boundary\n",
    "    if len(valid_picked_depths) == 0 or valid_picked_depths[-1][0] != d[-1]:\n",
    "        valid_picked_depths.append((d[-1], 1))\n",
    "    \n",
    "    return log, d, inds, valid_picked_depths\n",
    "\n",
    "# Generate synthetic logs for cores A and B\n",
    "print(\"Generating synthetic core pair...\")\n",
    "\n",
    "synthetic_log_a, synthetic_md_a, inds_a, synthetic_picked_a_tuples = create_synthetic_log_with_depths(\n",
    "    core_a_length, turb_logs, depth_logs, exclude_inds=None\n",
    ")\n",
    "synthetic_log_b, synthetic_md_b, inds_b, synthetic_picked_b_tuples = create_synthetic_log_with_depths(\n",
    "    core_b_length, turb_logs, depth_logs, exclude_inds=None\n",
    ")\n",
    "\n",
    "# Extract just the depths from the tuples\n",
    "synthetic_picked_a = [depth for depth, category in synthetic_picked_a_tuples]\n",
    "synthetic_picked_b = [depth for depth, category in synthetic_picked_b_tuples]\n",
    "\n",
    "# Plot synthetic core pair\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(4, 8))\n",
    "\n",
    "# Plot synthetic core A\n",
    "if synthetic_log_a.ndim > 1:\n",
    "    ax1.plot(synthetic_log_a[:, 0], synthetic_md_a, 'b-', linewidth=1)\n",
    "else:\n",
    "    ax1.plot(synthetic_log_a, synthetic_md_a, 'b-', linewidth=1)\n",
    "\n",
    "# Add picked depths as horizontal lines\n",
    "for depth in synthetic_picked_a:\n",
    "    ax1.axhline(y=depth, color='red', linestyle='--', alpha=0.7, linewidth=1)\n",
    "\n",
    "ax1.set_xlabel(f'{LOG_COLUMNS[0]}\\n(normalized)')\n",
    "ax1.set_ylabel('Depth (cm)')\n",
    "ax1.set_title(f'Synthetic Core A\\n({len(inds_a)} turbidites)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Plot synthetic core B\n",
    "if synthetic_log_b.ndim > 1:\n",
    "    ax2.plot(synthetic_log_b[:, 0], synthetic_md_b, 'g-', linewidth=1)\n",
    "else:\n",
    "    ax2.plot(synthetic_log_b, synthetic_md_b, 'g-', linewidth=1)\n",
    "\n",
    "# Add picked depths as horizontal lines\n",
    "for depth in synthetic_picked_b:\n",
    "    ax2.axhline(y=depth, color='red', linestyle='--', alpha=0.7, linewidth=1)\n",
    "\n",
    "ax2.set_xlabel(f'{LOG_COLUMNS[0]}\\n(normalized)')\n",
    "ax2.set_ylabel('Depth (cm)')\n",
    "ax2.set_title(f'Synthetic Core B\\n({len(inds_b)} turbidites)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Synthetic Core A: {len(synthetic_log_a)} points, {len(inds_a)} turbidites, {len(synthetic_picked_a)} boundaries\")\n",
    "print(f\"Synthetic Core B: {len(synthetic_log_b)} points, {len(inds_b)} turbidites, {len(synthetic_picked_b)} boundaries\")\n",
    "print(f\"Turbidite indices used in A: {[int(x) for x in inds_a[:10]]}...\" if len(inds_a) > 10 else f\"Turbidite indices used in A: {[int(x) for x in inds_a]}\")\n",
    "print(f\"Turbidite indices used in B: {[int(x) for x in inds_b[:10]]}...\" if len(inds_b) > 10 else f\"Turbidite indices used in B: {[int(x) for x in inds_b]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: DTW Analysis on Synthetic Pair\n",
    "from pyCoreRelator import run_comprehensive_dtw_analysis, find_complete_core_paths\n",
    "\n",
    "# Run DTW analysis\n",
    "dtw_results, valid_dtw_pairs, segments_a, segments_b, _, _, _ = run_comprehensive_dtw_analysis(\n",
    "    synthetic_log_a, synthetic_log_b, synthetic_md_a, synthetic_md_b,\n",
    "    picked_depths_a=synthetic_picked_a,\n",
    "    picked_depths_b=synthetic_picked_b,\n",
    "    independent_dtw=False,\n",
    "    top_bottom=False,\n",
    "    mute_mode=False\n",
    ")\n",
    "\n",
    "# Find complete core paths and extract r-values\n",
    "_ = find_complete_core_paths(\n",
    "    valid_dtw_pairs,\n",
    "    segments_a, \n",
    "    segments_b, \n",
    "    synthetic_log_a, \n",
    "    synthetic_log_b,\n",
    "    synthetic_picked_a, \n",
    "    synthetic_picked_b,\n",
    "    dtw_results,\n",
    "    output_csv=\"synthetic_core_pair_metrics.csv\",\n",
    "    output_metric_only=True,\n",
    "    shortest_path_search=True,\n",
    "    shortest_path_level=2,\n",
    "    max_search_path=50000,\n",
    "    mute_mode=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Plot R-Values Distribution from Synthetic Pair\n",
    "from pyCoreRelator import plot_correlation_distribution\n",
    "\n",
    "# Define quality index and parameters\n",
    "targeted_quality_index = 'corr_coef'\n",
    "csv_filename = 'outputs/synthetic_core_pair_metrics.csv'\n",
    "\n",
    "# Plot correlation distribution\n",
    "_, _, fit_params = plot_correlation_distribution(\n",
    "    csv_file=csv_filename,\n",
    "    quality_index=targeted_quality_index,\n",
    "    no_bins=30,\n",
    "    save_png=False,\n",
    "    pdf_method='normal',  # 'KDE', 'skew-normal', 'normal'\n",
    "    kde_bandwidth=0.05,\n",
    "    mute_mode=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Re-plot Distribution Using Fit Parameters\n",
    "if 'fit_params' in locals() and fit_params is not None:\n",
    "    print(\"Re-plotting fitted curve only from 'fit_params'...\")\n",
    "    \n",
    "    # Create new figure\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    # Plot only the fitted distribution curve\n",
    "    if 'x_range' in fit_params and 'y_values' in fit_params:\n",
    "        x = fit_params['x_range']\n",
    "        y = fit_params['y_values']\n",
    "        \n",
    "        method = fit_params.get('method', 'unknown')\n",
    "        \n",
    "        if method == 'normal':\n",
    "            mean_val = fit_params['mean']\n",
    "            std_val = fit_params['std']\n",
    "            n_points = fit_params['n_points']\n",
    "            ax.plot(x, y, 'r-', linewidth=2, alpha=0.8,\n",
    "                    label=f'Normal Fit\\n(mean = {mean_val:.3f})\\n(σ = {std_val:.3f})\\nn = {n_points:,}')\n",
    "                      \n",
    "        elif method == 'skew-normal':\n",
    "            shape = fit_params['shape']\n",
    "            location = fit_params['location']\n",
    "            scale = fit_params['scale']\n",
    "            n_points = fit_params['n_points']\n",
    "            ax.plot(x, y, 'r-', linewidth=2, alpha=0.8,\n",
    "                    label=f'Skew-Normal Fit\\n(α = {shape:.3f})\\n(μ = {location:.3f})\\n(σ = {scale:.3f})\\nn = {n_points:,}')\n",
    "                      \n",
    "        elif method == 'KDE':\n",
    "            bandwidth = fit_params['bandwidth']\n",
    "            n_points = fit_params['n_points']\n",
    "            ax.plot(x, y, 'r-', linewidth=2, alpha=0.8,\n",
    "                    label=f'KDE\\n(bandwidth = {bandwidth})\\nn = {n_points:,}')\n",
    "    \n",
    "    # Add median line\n",
    "    if 'median' in fit_params:\n",
    "        median_val = fit_params['median']\n",
    "        ax.axvline(median_val, color='green', linestyle='dashed', linewidth=2,\n",
    "                  label=f'Median: {median_val:.3f}')\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel(f'{targeted_quality_index}')\n",
    "    ax.set_ylabel('Density (%)')\n",
    "    ax.set_title(f'Fitted Distribution Curve\\nSynthetic Cores {CORE_A} vs {CORE_B}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Set appropriate x-axis limits\n",
    "    if targeted_quality_index == 'corr_coef':\n",
    "        ax.set_xlim(0, 1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "                \n",
    "else:\n",
    "    print(\"No fit_params available. Please run Cell 6 first to generate the distribution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Run 10000 Iterations for Synthetic Logs R-Value Findings\n",
    "# Prepare CSV file for incremental saving\n",
    "output_csv_filename = 'outputs/synthetic_iterations_fit_params.csv'\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "# Run 10000 iterations with progress bar\n",
    "for iteration in tqdm(range(5), desc=\"Running synthetic analysis\"):\n",
    "    \n",
    "    # Generate synthetic core pair (from cell 4)\n",
    "    synthetic_log_a, synthetic_md_a, inds_a, synthetic_picked_a_tuples = create_synthetic_log_with_depths(\n",
    "        core_a_length, turb_logs, depth_logs, exclude_inds=None\n",
    "    )\n",
    "    synthetic_log_b, synthetic_md_b, inds_b, synthetic_picked_b_tuples = create_synthetic_log_with_depths(\n",
    "        core_b_length, turb_logs, depth_logs, exclude_inds=None\n",
    "    )\n",
    "    \n",
    "    # Extract depths from tuples\n",
    "    synthetic_picked_a = [depth for depth, category in synthetic_picked_a_tuples]\n",
    "    synthetic_picked_b = [depth for depth, category in synthetic_picked_b_tuples]\n",
    "    \n",
    "    # Run DTW analysis (from cell 5)\n",
    "    dtw_results, valid_dtw_pairs, segments_a, segments_b, _, _, _ = run_comprehensive_dtw_analysis(\n",
    "        synthetic_log_a, synthetic_log_b, synthetic_md_a, synthetic_md_b,\n",
    "        picked_depths_a=synthetic_picked_a,\n",
    "        picked_depths_b=synthetic_picked_b,\n",
    "        independent_dtw=False,\n",
    "        top_bottom=False,\n",
    "        mute_mode=True\n",
    "    )\n",
    "    \n",
    "    # Find complete core paths\n",
    "    _ = find_complete_core_paths(\n",
    "        valid_dtw_pairs,\n",
    "        segments_a, \n",
    "        segments_b, \n",
    "        synthetic_log_a, \n",
    "        synthetic_log_b,\n",
    "        synthetic_picked_a, \n",
    "        synthetic_picked_b,\n",
    "        dtw_results,\n",
    "        output_csv=\"synthetic_core_pair_metrics.csv\",\n",
    "        output_metric_only=True,\n",
    "        shortest_path_search=True,\n",
    "        shortest_path_level=2,\n",
    "        max_search_path=50000,\n",
    "        mute_mode=True\n",
    "    )\n",
    "    \n",
    "    # Extract fit_params (from cell 6) - suppress all plotting\n",
    "    \n",
    "    # Plot correlation distribution to get fit_params only\n",
    "    _, _, fit_params = plot_correlation_distribution(\n",
    "        csv_file=\"outputs/synthetic_core_pair_metrics.csv\",\n",
    "        quality_index='corr_coef',\n",
    "        no_bins=30,\n",
    "        save_png=False,\n",
    "        pdf_method='normal',\n",
    "        kde_bandwidth=0.05,\n",
    "        mute_mode=True\n",
    "    )\n",
    "    \n",
    "    # Store fit_params with iteration number and incrementally save to CSV\n",
    "    if fit_params is not None:\n",
    "        fit_params_copy = fit_params.copy()\n",
    "        fit_params_copy['iteration'] = iteration\n",
    "        \n",
    "        # Incrementally save to CSV\n",
    "        df_single = pd.DataFrame([fit_params_copy])\n",
    "        if iteration == 0:\n",
    "            # Write header for first iteration\n",
    "            df_single.to_csv(output_csv_filename, mode='w', index=False, header=True)\n",
    "        else:\n",
    "            # Append subsequent iterations without header\n",
    "            df_single.to_csv(output_csv_filename, mode='a', index=False, header=False)\n",
    "        \n",
    "        del df_single, fit_params_copy\n",
    "    \n",
    "    # Clear memory after each iteration\n",
    "    del synthetic_log_a, synthetic_md_a, inds_a, synthetic_picked_a_tuples\n",
    "    del synthetic_log_b, synthetic_md_b, inds_b, synthetic_picked_b_tuples\n",
    "    del synthetic_picked_a, synthetic_picked_b\n",
    "    del dtw_results, valid_dtw_pairs, segments_a, segments_b\n",
    "    del fit_params\n",
    "    \n",
    "    # Force garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nCompleted 3 iterations\")\n",
    "print(f\"All distribution curves parameters saved to: {output_csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Plot all distribution curves\n",
    "\n",
    "# Load fit params from CSV\n",
    "output_csv_filename = 'outputs/synthetic_iterations_fit_params.csv'\n",
    "df_fit_params = pd.read_csv(output_csv_filename)\n",
    "\n",
    "# Convert to list of dictionaries containing only necessary columns\n",
    "all_fit_params = []\n",
    "for _, row in df_fit_params.iterrows():\n",
    "    fit_params = {\n",
    "        'x_range': np.fromstring(row['x_range'].strip('[]'), sep=' ') if 'x_range' in row and pd.notna(row['x_range']) else None,\n",
    "        'y_values': np.fromstring(row['y_values'].strip('[]'), sep=' ') if 'y_values' in row and pd.notna(row['y_values']) else None\n",
    "    }\n",
    "    all_fit_params.append(fit_params)\n",
    "\n",
    "# Plot all distribution curves (adapted from cell 7)\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Plot all curves as transparent red lines\n",
    "for fit_params in all_fit_params:\n",
    "    if 'x_range' in fit_params and 'y_values' in fit_params:\n",
    "        x = fit_params['x_range']\n",
    "        y = fit_params['y_values']\n",
    "        if x is not None and y is not None:\n",
    "            ax.plot(x, y, 'r-', linewidth=1, alpha=0.3)\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel(f\"Pearson's r\\n(Correlation Coefficient)\")\n",
    "ax.set_ylabel('Probability Density (%)')\n",
    "ax.set_title(f'Synthetic Core Correlation: {len(all_fit_params)} Iterations')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Set appropriate x-axis limits\n",
    "if targeted_quality_index == 'corr_coef':\n",
    "    ax.set_xlim(0, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('synthetic_iterations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
