{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NULL HYPOTHESIS TESTING FOR pyCoreRelator\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "import tempfile\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "# Import pyCoreRelator null hypothesis functions\n",
    "from pyCoreRelator import (\n",
    "    create_segment_pool_from_available_cores,\n",
    "    generate_synthetic_core_pair,\n",
    "    compute_pycorerelator_null_hypothesis,\n",
    "    plot_correlation_distribution\n",
    ")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== NULL HYPOTHESIS TESTING FOR pyCoreRelator ===\")\n",
    "print(\"This notebook implements null hypothesis testing for DTW correlation significance\")\n",
    "print(\"Required inputs from main analysis: log_a, log_b, md_a, md_b, segments, boundaries, dtw_results\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: LOAD REQUIRED DATA FROM MAIN ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "from pyCoreRelator import (\n",
    "    load_log_data,\n",
    "    run_comprehensive_dtw_analysis,\n",
    "    calculate_interpolated_ages\n",
    ")\n",
    "\n",
    "print(\"=== LOADING REQUIRED DATA FROM MAIN ANALYSIS ===\")\n",
    "\n",
    "# Check if required variables exist from main analysis\n",
    "required_vars = [\n",
    "    'log_a', 'log_b', 'md_a', 'md_b',\n",
    "    'segments_a', 'segments_b', \n",
    "    'depth_boundaries_a', 'depth_boundaries_b',\n",
    "    'dtw_results', 'valid_dtw_pairs',\n",
    "    'CORE_A', 'CORE_B'\n",
    "]\n",
    "\n",
    "missing_vars = []\n",
    "for var_name in required_vars:\n",
    "    if var_name not in globals():\n",
    "        missing_vars.append(var_name)\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"Missing required variables: {missing_vars}\")\n",
    "    print(\"Loading data from scratch...\")\n",
    "    \n",
    "    # =================================================================\n",
    "    # STANDALONE DATA LOADING (modify paths and parameters as needed)\n",
    "    # =================================================================\n",
    "    \n",
    "    # Define core names\n",
    "    CORE_A = \"M9907-23PC\"  # Modify these\n",
    "    CORE_B = \"M9907-25PC\"\n",
    "    \n",
    "    # Define log columns and paths\n",
    "    LOG_COLUMNS = ['hiresMS']  # Modify as needed\n",
    "    DEPTH_COLUMN = 'SB_DEPTH_cm'\n",
    "    \n",
    "    # Define directory paths (MODIFY THESE PATHS)\n",
    "    mother_dir = '/path/to/your/data/'  # UPDATE THIS PATH\n",
    "    \n",
    "    # Core A paths\n",
    "    core_a_log_paths = {\n",
    "        'hiresMS': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_hiresMS_MLfilled.csv',\n",
    "        # Add more log types as needed\n",
    "    }\n",
    "    \n",
    "    # Core B paths  \n",
    "    core_b_log_paths = {\n",
    "        'hiresMS': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_hiresMS_MLfilled.csv',\n",
    "        # Add more log types as needed\n",
    "    }\n",
    "    \n",
    "    # Column alternatives\n",
    "    column_alternatives = {\n",
    "        'hiresMS': ['MS'],\n",
    "        'CT': ['CT_value'],\n",
    "        # Add more alternatives as needed\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load data for Core A\n",
    "        print(f\"Loading {CORE_A}...\")\n",
    "        log_a, md_a, available_columns_a, _, _ = load_log_data(\n",
    "            core_a_log_paths,\n",
    "            {},  # No images for null hypothesis\n",
    "            LOG_COLUMNS,\n",
    "            depth_column=DEPTH_COLUMN,\n",
    "            normalize=True,\n",
    "            column_alternatives=column_alternatives\n",
    "        )\n",
    "        \n",
    "        # Load data for Core B\n",
    "        print(f\"Loading {CORE_B}...\")\n",
    "        log_b, md_b, available_columns_b, _, _ = load_log_data(\n",
    "            core_b_log_paths,\n",
    "            {},  # No images for null hypothesis\n",
    "            LOG_COLUMNS,\n",
    "            depth_column=DEPTH_COLUMN,\n",
    "            normalize=True,\n",
    "            column_alternatives=column_alternatives\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Log data loaded successfully\")\n",
    "        \n",
    "        # Load picked depths (modify paths as needed)\n",
    "        try:\n",
    "            picked_data_a = pd.read_csv(f'pickeddepth/{CORE_A}_pickeddepth.csv')\n",
    "            picked_depths_a = picked_data_a['picked_depths_cm'].values.tolist()\n",
    "            print(f\"✓ Loaded {len(picked_depths_a)} picked depths for {CORE_A}\")\n",
    "        except:\n",
    "            picked_depths_a = None\n",
    "            print(f\"⚠️  No picked depths found for {CORE_A}\")\n",
    "            \n",
    "        try:\n",
    "            picked_data_b = pd.read_csv(f'pickeddepth/{CORE_B}_pickeddepth.csv')\n",
    "            picked_depths_b = picked_data_b['picked_depths_cm'].values.tolist()\n",
    "            print(f\"✓ Loaded {len(picked_depths_b)} picked depths for {CORE_B}\")\n",
    "        except:\n",
    "            picked_depths_b = None\n",
    "            print(f\"⚠️  No picked depths found for {CORE_B}\")\n",
    "        \n",
    "        # Run DTW analysis to get segments and boundaries\n",
    "        print(\"Running DTW analysis...\")\n",
    "        dtw_results, valid_dtw_pairs, segments_a, segments_b, depth_boundaries_a, depth_boundaries_b, _ = run_comprehensive_dtw_analysis(\n",
    "            log_a, log_b, md_a, md_b,\n",
    "            picked_depths_a=picked_depths_a,\n",
    "            picked_depths_b=picked_depths_b,\n",
    "            top_bottom=True,\n",
    "            top_depth=0.0,\n",
    "            independent_dtw=False,\n",
    "            exclude_deadend=True,\n",
    "            create_dtw_matrix=False,  # Skip visualization for null hypothesis\n",
    "            creategif=False,\n",
    "            age_consideration=False,  # No age constraints for null hypothesis\n",
    "            debug=False\n",
    "        )\n",
    "        \n",
    "        print(\"✓ DTW analysis completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading data: {e}\")\n",
    "        print(\"Please modify the paths and parameters in this cell\")\n",
    "        raise\n",
    "\n",
    "else:\n",
    "    print(\"✓ All required variables found!\")\n",
    "\n",
    "# Display final status\n",
    "print(f\"\\nFinal status:\")\n",
    "print(f\"Core A: {CORE_A} - {len(log_a)} data points\")\n",
    "print(f\"Core B: {CORE_B} - {len(log_b)} data points\") \n",
    "print(f\"Valid segment pairs: {len(valid_dtw_pairs)}\")\n",
    "print(f\"Segments A: {len(segments_a)}, Segments B: {len(segments_b)}\")\n",
    "\n",
    "# Determine dimensionality\n",
    "target_dimensions = log_a.shape[1] if log_a.ndim > 1 else 1\n",
    "print(f\"Log dimensions: {target_dimensions}\")\n",
    "\n",
    "print(\"Data loading complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: CREATE SEGMENT POOL FROM AVAILABLE CORES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== CREATING SEGMENT POOL FROM AVAILABLE CORES ===\")\n",
    "\n",
    "# Step 1: Prepare data structures for segment pool creation\n",
    "# Add all available cores and their boundaries\n",
    "all_cores_data = {\n",
    "    CORE_A: {\n",
    "        'log_data': log_a,\n",
    "        'md_data': md_a\n",
    "    },\n",
    "    CORE_B: {\n",
    "        'log_data': log_b,\n",
    "        'md_data': md_b\n",
    "    }\n",
    "    # Add more cores here if available\n",
    "    # 'CORE_C': {'log_data': log_c, 'md_data': md_c},\n",
    "}\n",
    "\n",
    "all_boundaries_data = {\n",
    "    CORE_A: {\n",
    "        'depth_boundaries': depth_boundaries_a,\n",
    "        'segments': segments_a\n",
    "    },\n",
    "    CORE_B: {\n",
    "        'depth_boundaries': depth_boundaries_b,\n",
    "        'segments': segments_b\n",
    "    }\n",
    "    # Add corresponding boundaries for additional cores\n",
    "}\n",
    "\n",
    "print(f\"Available cores: {list(all_cores_data.keys())}\")\n",
    "print(f\"Total cores in pool: {len(all_cores_data)}\")\n",
    "\n",
    "# Step 2: Create segment pool from available cores\n",
    "print(\"\\nCreating segment pool from available cores...\")\n",
    "segment_pool = create_segment_pool_from_available_cores(all_cores_data, all_boundaries_data)\n",
    "\n",
    "print(f\"\\n=== SEGMENT POOL STATISTICS ===\")\n",
    "print(f\"Total segments extracted: {len(segment_pool)}\")\n",
    "\n",
    "if segment_pool:\n",
    "    # Analyze segment characteristics\n",
    "    lengths = [seg['length'] for seg in segment_pool]\n",
    "    depth_spans = [seg['depth_span'] for seg in segment_pool]\n",
    "    dimensions = [seg['dimensions'] for seg in segment_pool]\n",
    "    source_cores = [seg['source_core'] for seg in segment_pool]\n",
    "    \n",
    "    print(f\"Segment lengths: min={min(lengths)}, max={max(lengths)}, mean={np.mean(lengths):.1f}\")\n",
    "    print(f\"Depth spans: min={min(depth_spans):.1f}, max={max(depth_spans):.1f}, mean={np.mean(depth_spans):.1f}\")\n",
    "    print(f\"Dimensions: {set(dimensions)}\")\n",
    "    \n",
    "    # Count segments by source core\n",
    "    from collections import Counter\n",
    "    core_counts = Counter(source_cores)\n",
    "    print(f\"Segments by core: {dict(core_counts)}\")\n",
    "    \n",
    "    # Show a few example segments\n",
    "    print(f\"\\nExample segments:\")\n",
    "    for i, seg in enumerate(segment_pool[:3]):\n",
    "        print(f\"  {seg['segment_id']}: {seg['length']} points, span={seg['depth_span']:.1f}, dims={seg['dimensions']}\")\n",
    "        \n",
    "else:\n",
    "    print(\"ERROR: No segments were extracted!\")\n",
    "    \n",
    "print(\"Segment pool creation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: CONFIGURE TARGET CHARACTERISTICS FOR SYNTHETIC CORES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== CONFIGURING TARGET CHARACTERISTICS ===\")\n",
    "\n",
    "# Step 3: Configure target characteristics for synthetic cores\n",
    "# Match the characteristics of your actual analysis\n",
    "target_dimensions = log_a.shape[1] if log_a.ndim > 1 else 1\n",
    "\n",
    "core_a_config = {\n",
    "    'target_length': len(log_a),\n",
    "    'target_dimensions': target_dimensions\n",
    "}\n",
    "\n",
    "core_b_config = {\n",
    "    'target_length': len(log_b), \n",
    "    'target_dimensions': target_dimensions\n",
    "}\n",
    "\n",
    "print(f\"Target Core A: {core_a_config['target_length']} points, {core_a_config['target_dimensions']} dimensions\")\n",
    "print(f\"Target Core B: {core_b_config['target_length']} points, {core_b_config['target_dimensions']} dimensions\")\n",
    "\n",
    "# Verify compatibility with segment pool\n",
    "compatible_segments = [seg for seg in segment_pool if seg['dimensions'] == target_dimensions]\n",
    "print(f\"\\nCompatible segments in pool: {len(compatible_segments)}/{len(segment_pool)}\")\n",
    "\n",
    "if len(compatible_segments) == 0:\n",
    "    print(\"ERROR: No segments in pool match the target dimensions!\")\n",
    "else:\n",
    "    print(\"✓ Segment pool is compatible with target characteristics\")\n",
    "\n",
    "# Optional: Test synthetic core generation\n",
    "print(\"\\n=== TESTING SYNTHETIC CORE GENERATION ===\")\n",
    "try:\n",
    "    print(\"Generating test synthetic core pair...\")\n",
    "    test_log_a, test_log_b, test_md_a, test_md_b, test_bounds_a, test_bounds_b = generate_synthetic_core_pair(\n",
    "        segment_pool, \n",
    "        core_a_config['target_length'], \n",
    "        core_b_config['target_length'],\n",
    "        target_dimensions\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Test successful!\")\n",
    "    print(f\"  Generated Core A: {len(test_log_a)} points, shape={test_log_a.shape}\")\n",
    "    print(f\"  Generated Core B: {len(test_log_b)} points, shape={test_log_b.shape}\")\n",
    "    print(f\"  Boundaries A: {len(test_bounds_a)} boundaries\")\n",
    "    print(f\"  Boundaries B: {len(test_bounds_b)} boundaries\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR in synthetic core generation: {e}\")\n",
    "    \n",
    "print(\"Configuration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: CONFIGURE NULL HYPOTHESIS PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== CONFIGURING NULL HYPOTHESIS PARAMETERS ===\")\n",
    "\n",
    "# Configuration parameters for null hypothesis testing\n",
    "# These should match your actual DTW analysis parameters\n",
    "\n",
    "# Number of iterations for null hypothesis distribution\n",
    "n_iterations = 1000  # Adjust based on computational resources\n",
    "# Recommended: 1000 for testing, 10000+ for publication\n",
    "\n",
    "# DTW parameters (should match your main analysis)\n",
    "exponent = 0.3  # DTW exponent parameter\n",
    "dtw_distance_threshold = None  # Set to match your main analysis if used\n",
    "\n",
    "# Display progress bar during computation\n",
    "progress_bar = True\n",
    "\n",
    "print(f\"Null hypothesis configuration:\")\n",
    "print(f\"  Iterations: {n_iterations}\")\n",
    "print(f\"  DTW exponent: {exponent}\")\n",
    "print(f\"  DTW distance threshold: {dtw_distance_threshold}\")\n",
    "print(f\"  Progress bar: {progress_bar}\")\n",
    "\n",
    "# Estimate computation time\n",
    "segments_per_core = len(segment_pool) / len(all_cores_data)\n",
    "estimated_time_per_iteration = 0.5  # seconds (rough estimate)\n",
    "estimated_total_time = n_iterations * estimated_time_per_iteration\n",
    "\n",
    "print(f\"\\nEstimated computation time: {estimated_total_time/60:.1f} minutes\")\n",
    "print(f\"Average segments per core: {segments_per_core:.1f}\")\n",
    "\n",
    "if n_iterations > 5000:\n",
    "    print(\"WARNING: Large number of iterations may take significant time\")\n",
    "    print(\"Consider reducing n_iterations for initial testing\")\n",
    "\n",
    "print(\"Parameter configuration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: COMPUTE NULL HYPOTHESIS DISTRIBUTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== COMPUTING NULL HYPOTHESIS DISTRIBUTION ===\")\n",
    "print(\"This may take several minutes depending on n_iterations...\")\n",
    "print(f\"Processing {n_iterations} synthetic core pairs...\")\n",
    "\n",
    "# Step 4: Compute null hypothesis distribution\n",
    "null_hypothesis_results = compute_pycorerelator_null_hypothesis(\n",
    "    segment_pool=segment_pool,\n",
    "    core_a_config=core_a_config,\n",
    "    core_b_config=core_b_config,\n",
    "    n_iterations=n_iterations,\n",
    "    exponent=exponent,\n",
    "    dtw_distance_threshold=dtw_distance_threshold,\n",
    "    progress_bar=progress_bar\n",
    ")\n",
    "\n",
    "# Step 5: Extract and analyze null hypothesis distribution\n",
    "r_values_null = null_hypothesis_results['r_values_distribution']\n",
    "distribution_stats = null_hypothesis_results['distribution_stats']\n",
    "\n",
    "print(f\"\\n=== NULL HYPOTHESIS RESULTS ===\")\n",
    "print(f\"Successful iterations: {null_hypothesis_results['successful_iterations']}\")\n",
    "print(f\"Failed iterations: {null_hypothesis_results['failed_iterations']}\")\n",
    "success_rate = null_hypothesis_results['successful_iterations'] / n_iterations * 100\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "if len(r_values_null) > 0:\n",
    "    print(f\"\\nDistribution statistics:\")\n",
    "    print(f\"  Mean r-value: {distribution_stats['mean']:.4f} ± {distribution_stats['std']:.4f}\")\n",
    "    print(f\"  Median r-value: {distribution_stats['median']:.4f}\")\n",
    "    print(f\"  Range: [{distribution_stats['min']:.4f}, {distribution_stats['max']:.4f}]\")\n",
    "    print(f\"  95th percentile: {distribution_stats['percentile_95']:.4f}\")\n",
    "    print(f\"  97.5th percentile: {distribution_stats['percentile_97_5']:.4f}\")\n",
    "    print(f\"  99th percentile: {distribution_stats['percentile_99']:.4f}\")\n",
    "    \n",
    "    # Quick visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(r_values_null, bins=50, alpha=0.7, density=True, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(distribution_stats['mean'], color='red', linestyle='-', linewidth=2, label=f\"Mean: {distribution_stats['mean']:.3f}\")\n",
    "    plt.axvline(distribution_stats['percentile_95'], color='orange', linestyle='--', linewidth=2, label=f\"95th %ile: {distribution_stats['percentile_95']:.3f}\")\n",
    "    plt.axvline(distribution_stats['percentile_97_5'], color='green', linestyle='--', linewidth=2, label=f\"97.5th %ile: {distribution_stats['percentile_97_5']:.3f}\")\n",
    "    plt.xlabel('Correlation Coefficient (r)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Null Hypothesis Distribution\\n{CORE_A} vs {CORE_B} (n={len(r_values_null)})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: No successful iterations - cannot create null hypothesis distribution\")\n",
    "\n",
    "print(\"Null hypothesis computation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: EXTRACT ACTUAL CORRELATIONS FROM DTW ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== EXTRACTING ACTUAL CORRELATIONS ===\")\n",
    "\n",
    "# Step 6: Extract actual correlation coefficients from your DTW analysis\n",
    "actual_correlations = {}\n",
    "\n",
    "# Extract correlation coefficients from valid segment pairs\n",
    "segment_correlations = []\n",
    "for pair_key in valid_dtw_pairs:\n",
    "    if pair_key in dtw_results:\n",
    "        paths, _, quality_metrics = dtw_results[pair_key]\n",
    "        if quality_metrics and len(quality_metrics) > 0:\n",
    "            qi = quality_metrics[0]  # Get first quality indicator\n",
    "            if 'corr_coef' in qi and not np.isnan(qi['corr_coef']):\n",
    "                pair_name = f\"Seg_{pair_key[0]+1}-{pair_key[1]+1}\"\n",
    "                actual_correlations[pair_name] = qi['corr_coef']\n",
    "                segment_correlations.append(qi['corr_coef'])\n",
    "\n",
    "print(f\"Found {len(actual_correlations)} valid segment correlations\")\n",
    "\n",
    "# Compute overall correlation (mean of segment correlations)\n",
    "if actual_correlations:\n",
    "    overall_actual_correlation = np.mean(list(actual_correlations.values()))\n",
    "    actual_correlations['Overall'] = overall_actual_correlation\n",
    "    \n",
    "    print(f\"Segment correlations:\")\n",
    "    for pair_name, r_value in actual_correlations.items():\n",
    "        if pair_name != 'Overall':\n",
    "            print(f\"  {pair_name}: r = {r_value:.4f}\")\n",
    "    \n",
    "    print(f\"\\nOverall correlation: {overall_actual_correlation:.4f}\")\n",
    "    print(f\"Range of segment correlations: [{min(segment_correlations):.4f}, {max(segment_correlations):.4f}]\")\n",
    "    print(f\"Standard deviation: {np.std(segment_correlations):.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"WARNING: No valid correlations found in DTW results\")\n",
    "    print(\"This may indicate issues with the DTW analysis or segment pairs\")\n",
    "\n",
    "print(\"Actual correlation extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: PLOT NULL HYPOTHESIS DISTRIBUTION WITH ACTUAL CORRELATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== PLOTTING NULL HYPOTHESIS DISTRIBUTION ===\")\n",
    "\n",
    "# Step 7: Create temporary CSV for plot_correlation_distribution compatibility\n",
    "temp_csv = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)\n",
    "temp_csv_path = temp_csv.name\n",
    "\n",
    "# Write data in the format expected by plot_correlation_distribution\n",
    "with open(temp_csv_path, 'w') as f:\n",
    "    f.write(\"mapping_id,corr_coef\\n\")\n",
    "    for i, r_val in enumerate(r_values_null):\n",
    "        f.write(f\"{i},{r_val}\\n\")\n",
    "\n",
    "temp_csv.close()\n",
    "\n",
    "# Step 8: Plot null hypothesis distribution with actual correlations\n",
    "try:\n",
    "    fig, ax, fit_params = plot_correlation_distribution(\n",
    "        csv_file=temp_csv_path,\n",
    "        quality_index='corr_coef',\n",
    "        save_png=True,\n",
    "        png_filename=f'Null_Hypothesis_Distribution_{CORE_A}_{CORE_B}.png',\n",
    "        core_a_name=CORE_A,\n",
    "        core_b_name=CORE_B,\n",
    "        pdf_method='KDE',\n",
    "        kde_bandwidth=0.05\n",
    "    )\n",
    "    \n",
    "    # Add actual correlations as vertical lines\n",
    "    if actual_correlations:\n",
    "        colors = ['red', 'orange', 'blue', 'green', 'purple']\n",
    "        color_idx = 0\n",
    "        \n",
    "        for pair_name, r_value in actual_correlations.items():\n",
    "            if not np.isnan(r_value):\n",
    "                # Calculate percentile and p-value\n",
    "                percentile = (r_values_null < r_value).mean() * 100\n",
    "                p_value = (r_values_null >= r_value).mean()\n",
    "                \n",
    "                # Choose color and style based on significance and pair type\n",
    "                if pair_name == 'Overall':\n",
    "                    color = 'red'\n",
    "                    linestyle = 'solid'\n",
    "                    linewidth = 3\n",
    "                    alpha = 0.9\n",
    "                else:\n",
    "                    color = colors[color_idx % len(colors)]\n",
    "                    linestyle = 'dashed'\n",
    "                    linewidth = 2\n",
    "                    alpha = 0.7\n",
    "                    color_idx += 1\n",
    "                \n",
    "                # Determine significance marker\n",
    "                if p_value < 0.001:\n",
    "                    sig_marker = '***'\n",
    "                elif p_value < 0.01:\n",
    "                    sig_marker = '**'\n",
    "                elif p_value < 0.05:\n",
    "                    sig_marker = '*'\n",
    "                else:\n",
    "                    sig_marker = ''\n",
    "                \n",
    "                # Plot vertical line\n",
    "                ax.axvline(r_value, color=color, linestyle=linestyle, alpha=alpha, linewidth=linewidth)\n",
    "                \n",
    "                # Add text label\n",
    "                y_pos = ax.get_ylim()[1] * (0.85 if pair_name == 'Overall' else 0.75 - color_idx * 0.05)\n",
    "                ax.text(r_value, y_pos, f'{pair_name}\\nr={r_value:.3f}\\np={p_value:.3f}{sig_marker}', \n",
    "                       rotation=90, ha='right', va='top', fontsize=8,\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Add legend for significance levels\n",
    "        legend_elements = [\n",
    "            plt.Line2D([0], [0], color='black', linestyle='-', label='*** p < 0.001'),\n",
    "            plt.Line2D([0], [0], color='black', linestyle='-', label='** p < 0.01'),\n",
    "            plt.Line2D([0], [0], color='black', linestyle='-', label='* p < 0.05')\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='upper right', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"No actual correlations to overlay on the plot\")\n",
    "        \n",
    "finally:\n",
    "    # Clean up temporary file\n",
    "    if os.path.exists(temp_csv_path):\n",
    "        os.unlink(temp_csv_path)\n",
    "\n",
    "print(\"Null hypothesis distribution plotting complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: SIGNIFICANCE TESTING RESULTS AND INTERPRETATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== SIGNIFICANCE TESTING RESULTS ===\")\n",
    "\n",
    "if len(r_values_null) > 0 and actual_correlations:\n",
    "    \n",
    "    # Print detailed significance results\n",
    "    print(f\"Null hypothesis distribution (n={len(r_values_null)}):\")\n",
    "    print(f\"  Mean: {distribution_stats['mean']:.4f} ± {distribution_stats['std']:.4f}\")\n",
    "    print(f\"  Significance thresholds:\")\n",
    "    print(f\"    p < 0.05: r > {distribution_stats['percentile_95']:.4f}\")\n",
    "    print(f\"    p < 0.025: r > {distribution_stats['percentile_97_5']:.4f}\")\n",
    "    print(f\"    p < 0.01: r > {distribution_stats['percentile_99']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nActual correlations vs null hypothesis:\")\n",
    "    print(f\"{'Segment':<12} {'r-value':<8} {'Percentile':<10} {'p-value':<8} {'Significance':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    significant_segments = []\n",
    "    non_significant_segments = []\n",
    "    \n",
    "    for pair_name, r_value in actual_correlations.items():\n",
    "        if not np.isnan(r_value):\n",
    "            percentile = (r_values_null < r_value).mean() * 100\n",
    "            p_value = (r_values_null >= r_value).mean()\n",
    "            \n",
    "            if p_value < 0.001:\n",
    "                significance = \"***\"\n",
    "                status = \"Highly Sig.\"\n",
    "            elif p_value < 0.01:\n",
    "                significance = \"**\"\n",
    "                status = \"Very Sig.\"\n",
    "            elif p_value < 0.05:\n",
    "                significance = \"*\"\n",
    "                status = \"Significant\"\n",
    "            else:\n",
    "                significance = \"\"\n",
    "                status = \"Not Sig.\"\n",
    "            \n",
    "            print(f\"{pair_name:<12} {r_value:<8.4f} {percentile:<10.1f} {p_value:<8.4f} {status:<12} {significance}\")\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                significant_segments.append(pair_name)\n",
    "            else:\n",
    "                non_significant_segments.append(pair_name)\n",
    "    \n",
    "    # Summary\n",
    "    total_segments = len(actual_correlations) - (1 if 'Overall' in actual_correlations else 0)\n",
    "    print(f\"\\n=== SUMMARY ===\")\n",
    "    print(f\"Total segment correlations tested: {total_segments}\")\n",
    "    print(f\"Significant correlations (p < 0.05): {len([s for s in significant_segments if s != 'Overall'])}\")\n",
    "    print(f\"Non-significant correlations: {len(non_significant_segments)}\")\n",
    "    \n",
    "    if 'Overall' in actual_correlations:\n",
    "        overall_p = (r_values_null >= actual_correlations['Overall']).mean()\n",
    "        overall_sig = \"SIGNIFICANT\" if overall_p < 0.05 else \"NOT SIGNIFICANT\"\n",
    "        print(f\"Overall correlation: {overall_sig} (p = {overall_p:.4f})\")\n",
    "    \n",
    "    # Effect size interpretation\n",
    "    if 'Overall' in actual_correlations:\n",
    "        r_val = actual_correlations['Overall']\n",
    "        if abs(r_val) >= 0.7:\n",
    "            effect_size = \"Large\"\n",
    "        elif abs(r_val) >= 0.3:\n",
    "            effect_size = \"Medium\"\n",
    "        elif abs(r_val) >= 0.1:\n",
    "            effect_size = \"Small\"\n",
    "        else:\n",
    "            effect_size = \"Negligible\"\n",
    "        \n",
    "        print(f\"Effect size: {effect_size} (|r| = {abs(r_val):.3f})\")\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: Cannot perform significance testing\")\n",
    "    print(\"Missing null hypothesis distribution or actual correlations\")\n",
    "\n",
    "print(\"\\nSignificance testing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: SAVE RESULTS AND SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== SAVING RESULTS ===\")\n",
    "\n",
    "# Step 9: Save null hypothesis results for future use\n",
    "results_filename = f'Null_Hypothesis_Results_{CORE_A}_{CORE_B}.csv'\n",
    "null_df = pd.DataFrame({\n",
    "    'iteration': range(len(r_values_null)),\n",
    "    'r_value': r_values_null\n",
    "})\n",
    "null_df.to_csv(results_filename, index=False)\n",
    "print(f\"Null hypothesis r-values saved to: {results_filename}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_filename = f'Null_Hypothesis_Summary_{CORE_A}_{CORE_B}.txt'\n",
    "with open(summary_filename, 'w') as f:\n",
    "    f.write(f\"=== NULL HYPOTHESIS TESTING SUMMARY ===\\n\")\n",
    "    f.write(f\"Cores: {CORE_A} vs {CORE_B}\\n\")\n",
    "    f.write(f\"Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"ANALYSIS PARAMETERS:\\n\")\n",
    "    f.write(f\"  Iterations: {n_iterations}\\n\")\n",
    "    f.write(f\"  Success rate: {null_hypothesis_results['successful_iterations']}/{n_iterations} ({null_hypothesis_results['successful_iterations']/n_iterations*100:.1f}%)\\n\")\n",
    "    f.write(f\"  DTW exponent: {exponent}\\n\")\n",
    "    f.write(f\"  Segment pool size: {len(segment_pool)}\\n\\n\")\n",
    "    \n",
    "    if len(r_values_null) > 0:\n",
    "        f.write(f\"NULL HYPOTHESIS DISTRIBUTION:\\n\")\n",
    "        f.write(f\"  Mean: {distribution_stats['mean']:.4f} ± {distribution_stats['std']:.4f}\\n\")\n",
    "        f.write(f\"  Median: {distribution_stats['median']:.4f}\\n\")\n",
    "        f.write(f\"  Range: [{distribution_stats['min']:.4f}, {distribution_stats['max']:.4f}]\\n\")\n",
    "        f.write(f\"  95th percentile: {distribution_stats['percentile_95']:.4f}\\n\")\n",
    "        f.write(f\"  97.5th percentile: {distribution_stats['percentile_97_5']:.4f}\\n\")\n",
    "        f.write(f\"  99th percentile: {distribution_stats['percentile_99']:.4f}\\n\\n\")\n",
    "    \n",
    "    if actual_correlations:\n",
    "        f.write(f\"ACTUAL CORRELATIONS:\\n\")\n",
    "        for pair_name, r_value in actual_correlations.items():\n",
    "            if not np.isnan(r_value):\n",
    "                percentile = (r_values_null < r_value).mean() * 100\n",
    "                p_value = (r_values_null >= r_value).mean()\n",
    "                significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
    "                f.write(f\"  {pair_name}: r = {r_value:.4f}, p = {p_value:.4f} {significance}\\n\")\n",
    "\n",
    "print(f\"Summary saved to: {summary_filename}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata_filename = f'Null_Hypothesis_Metadata_{CORE_A}_{CORE_B}.json'\n",
    "import json\n",
    "\n",
    "metadata = {\n",
    "    'analysis_date': pd.Timestamp.now().isoformat(),\n",
    "    'cores': [CORE_A, CORE_B],\n",
    "    'parameters': {\n",
    "        'n_iterations': n_iterations,\n",
    "        'exponent': exponent,\n",
    "        'dtw_distance_threshold': dtw_distance_threshold,\n",
    "        'target_dimensions': target_dimensions\n",
    "    },\n",
    "    'results': {\n",
    "        'successful_iterations': null_hypothesis_results['successful_iterations'],\n",
    "        'failed_iterations': null_hypothesis_results['failed_iterations'],\n",
    "        'distribution_stats': distribution_stats if len(r_values_null) > 0 else None\n",
    "    },\n",
    "    'segment_pool': {\n",
    "        'total_segments': len(segment_pool),\n",
    "        'cores_included': list(all_cores_data.keys())\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Metadata saved to: {metadata_filename}\")\n",
    "\n",
    "# Display final summary\n",
    "print(f\"\\n=== FINAL SUMMARY ===\")\n",
    "print(f\"✓ Null hypothesis testing completed for {CORE_A} vs {CORE_B}\")\n",
    "print(f\"✓ {len(r_values_null)} successful iterations out of {n_iterations}\")\n",
    "if len(r_values_null) > 0:\n",
    "    print(f\"✓ Null hypothesis mean: {distribution_stats['mean']:.4f} ± {distribution_stats['std']:.4f}\")\n",
    "    print(f\"✓ Significance threshold (p<0.05): r > {distribution_stats['percentile_95']:.4f}\")\n",
    "\n",
    "if actual_correlations and 'Overall' in actual_correlations:\n",
    "    overall_p = (r_values_null >= actual_correlations['Overall']).mean()\n",
    "    print(f\"✓ Overall correlation: r = {actual_correlations['Overall']:.4f}, p = {overall_p:.4f}\")\n",
    "\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  - {results_filename}\")\n",
    "print(f\"  - {summary_filename}\")\n",
    "print(f\"  - {metadata_filename}\")\n",
    "print(f\"  - Null_Hypothesis_Distribution_{CORE_A}_{CORE_B}.png\")\n",
    "\n",
    "print(\"\\n=== NULL HYPOTHESIS TESTING COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: OPTIONAL ANALYSIS AND TROUBLESHOOTING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== OPTIONAL ANALYSIS AND TROUBLESHOOTING ===\")\n",
    "\n",
    "# This cell contains optional analyses and troubleshooting code\n",
    "# Run sections as needed based on your results\n",
    "\n",
    "# =============================================================================\n",
    "# OPTION 1: ANALYZE FAILED ITERATIONS\n",
    "# =============================================================================\n",
    "if null_hypothesis_results['failed_iterations'] > 0:\n",
    "    print(f\"\\n--- ANALYZING FAILED ITERATIONS ---\")\n",
    "    failure_rate = null_hypothesis_results['failed_iterations'] / n_iterations * 100\n",
    "    print(f\"Failure rate: {failure_rate:.1f}% ({null_hypothesis_results['failed_iterations']}/{n_iterations})\")\n",
    "    \n",
    "    if failure_rate > 20:\n",
    "        print(\"WARNING: High failure rate may indicate:\")\n",
    "        print(\"  - Insufficient segments in pool\")\n",
    "        print(\"  - Incompatible segment dimensions\")\n",
    "        print(\"  - DTW analysis parameter issues\")\n",
    "        print(\"  - Target core lengths too large for available segments\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTION 2: COMPARE WITH DIFFERENT AGGREGATION METHODS\n",
    "# =============================================================================\n",
    "if len(r_values_null) > 0 and actual_correlations:\n",
    "    print(f\"\\n--- TESTING DIFFERENT AGGREGATION METHODS ---\")\n",
    "    \n",
    "    # Extract individual segment correlations (excluding 'Overall')\n",
    "    segment_r_values = [r for name, r in actual_correlations.items() \n",
    "                       if name != 'Overall' and not np.isnan(r)]\n",
    "    \n",
    "    if len(segment_r_values) > 1:\n",
    "        # Different ways to aggregate segment correlations\n",
    "        aggregation_methods = {\n",
    "            'Mean': np.mean(segment_r_values),\n",
    "            'Median': np.median(segment_r_values),\n",
    "            'Max': np.max(segment_r_values),\n",
    "            'Min': np.min(segment_r_values),\n",
    "            'RMS': np.sqrt(np.mean(np.array(segment_r_values)**2))\n",
    "        }\n",
    "        \n",
    "        print(\"Different aggregation methods:\")\n",
    "        for method, value in aggregation_methods.items():\n",
    "            percentile = (r_values_null < value).mean() * 100\n",
    "            p_value = (r_values_null >= value).mean()\n",
    "            sig = \"*\" if p_value < 0.05 else \"\"\n",
    "            print(f\"  {method}: r = {value:.4f}, p = {p_value:.4f} {sig}\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTION 3: DISTRIBUTION DIAGNOSTICS\n",
    "# =============================================================================\n",
    "if len(r_values_null) > 0:\n",
    "    print(f\"\\n--- DISTRIBUTION DIAGNOSTICS ---\")\n",
    "    \n",
    "    # Test for normality\n",
    "    from scipy.stats import shapiro, anderson\n",
    "    \n",
    "    # Shapiro-Wilk test (for small samples)\n",
    "    if len(r_values_null) <= 5000:\n",
    "        stat, p_val = shapiro(r_values_null)\n",
    "        print(f\"Shapiro-Wilk normality test: p = {p_val:.4f}\")\n",
    "        print(f\"  Normal distribution: {'Yes' if p_val > 0.05 else 'No'}\")\n",
    "    \n",
    "    # Anderson-Darling test\n",
    "    ad_result = anderson(r_values_null, dist='norm')\n",
    "    print(f\"Anderson-Darling test statistic: {ad_result.statistic:.4f}\")\n",
    "    \n",
    "    # Check for outliers\n",
    "    q1, q3 = np.percentile(r_values_null, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    outliers = r_values_null[(r_values_null < lower_bound) | (r_values_null > upper_bound)]\n",
    "    print(f\"Outliers (IQR method): {len(outliers)} ({len(outliers)/len(r_values_null)*100:.1f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTION 4: SEGMENT POOL ANALYSIS\n",
    "# =============================================================================\n",
    "if segment_pool:\n",
    "    print(f\"\\n--- SEGMENT POOL ANALYSIS ---\")\n",
    "    \n",
    "    # Analyze segment characteristics in detail\n",
    "    lengths = [seg['length'] for seg in segment_pool]\n",
    "    depth_spans = [seg['depth_span'] for seg in segment_pool]\n",
    "    \n",
    "    print(f\"Segment length distribution:\")\n",
    "    print(f\"  Mean: {np.mean(lengths):.1f} ± {np.std(lengths):.1f}\")\n",
    "    print(f\"  Median: {np.median(lengths):.1f}\")\n",
    "    print(f\"  Quartiles: Q1={np.percentile(lengths, 25):.1f}, Q3={np.percentile(lengths, 75):.1f}\")\n",
    "    \n",
    "    print(f\"Depth span distribution:\")\n",
    "    print(f\"  Mean: {np.mean(depth_spans):.1f} ± {np.std(depth_spans):.1f}\")\n",
    "    print(f\"  Median: {np.median(depth_spans):.1f}\")\n",
    "    \n",
    "    # Check for segment diversity\n",
    "    unique_lengths = len(set(lengths))\n",
    "    print(f\"Segment diversity: {unique_lengths} unique lengths from {len(segment_pool)} segments\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTION 5: POWER ANALYSIS\n",
    "# =============================================================================\n",
    "if len(r_values_null) > 0:\n",
    "    print(f\"\\n--- POWER ANALYSIS ---\")\n",
    "    \n",
    "    # Estimate power to detect different effect sizes\n",
    "    null_std = np.std(r_values_null)\n",
    "    null_mean = np.mean(r_values_null)\n",
    "    alpha = 0.05\n",
    "    threshold = np.percentile(r_values_null, 95)\n",
    "    \n",
    "    effect_sizes = [0.1, 0.3, 0.5, 0.7]\n",
    "    print(\"Power to detect effect sizes (assuming normal distribution):\")\n",
    "    \n",
    "    for effect_size in effect_sizes:\n",
    "        # Simulate distribution under alternative hypothesis\n",
    "        alt_mean = null_mean + effect_size\n",
    "        # Simple power approximation\n",
    "        z_score = (threshold - alt_mean) / null_std\n",
    "        power = 1 - stats.norm.cdf(z_score)\n",
    "        print(f\"  Effect size r = {effect_size}: Power ≈ {power:.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTION 6: RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "print(f\"\\n--- RECOMMENDATIONS ---\")\n",
    "\n",
    "if len(r_values_null) == 0:\n",
    "    print(\"⚠️  No successful iterations - Check segment pool and parameters\")\n",
    "elif null_hypothesis_results['failed_iterations'] / n_iterations > 0.5:\n",
    "    print(\"⚠️  High failure rate - Consider:\")\n",
    "    print(\"   - Increasing segment pool size\")\n",
    "    print(\"   - Reducing target core lengths\")\n",
    "    print(\"   - Checking DTW parameters\")\n",
    "elif len(r_values_null) < 1000:\n",
    "    print(\"⚠️  Small sample size - Consider increasing n_iterations for more reliable results\")\n",
    "else:\n",
    "    print(\"✓ Analysis appears successful\")\n",
    "\n",
    "if actual_correlations and len(r_values_null) > 0:\n",
    "    overall_p = (r_values_null >= actual_correlations.get('Overall', 0)).mean()\n",
    "    if overall_p < 0.001:\n",
    "        print(\"✓ Very strong evidence against null hypothesis\")\n",
    "    elif overall_p < 0.01:\n",
    "        print(\"✓ Strong evidence against null hypothesis\")\n",
    "    elif overall_p < 0.05:\n",
    "        print(\"✓ Moderate evidence against null hypothesis\")\n",
    "    else:\n",
    "        print(\"⚠️  Weak evidence against null hypothesis - correlation may not be significant\")\n",
    "\n",
    "print(\"\\nOptional analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
