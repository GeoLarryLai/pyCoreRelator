{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Required Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import pyCoreRelator functions\n",
    "from pyCoreRelator import (\n",
    "    create_segment_pool_from_available_cores,\n",
    "    generate_synthetic_core_pair,\n",
    "    compute_pycorerelator_null_hypothesis,\n",
    "    create_synthetic_picked_depths,\n",
    "    run_comprehensive_dtw_analysis,\n",
    "    load_log_data,\n",
    "    plot_correlation_distribution\n",
    ")   \n",
    "\n",
    "print(\"Packages imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load ALL Available Core Data\n",
    "# Define core names and ALL available log columns\n",
    "CORE_A = \"M9907-23PC\"\n",
    "CORE_B = \"M9907-25PC\"\n",
    "# LOG_COLUMNS = ['hiresMS', 'CT', 'R', 'G', 'B', 'Lumin', 'Den_gm/cc']  # Choose available logs\n",
    "LOG_COLUMNS = ['hiresMS']  # Choose one log column\n",
    "DEPTH_COLUMN = 'SB_DEPTH_cm'\n",
    "\n",
    "# Define directory paths\n",
    "mother_dir = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/'\n",
    "\n",
    "# Define paths for Core A - ALL available log types\n",
    "core_a_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_Den_MLfilled.csv'\n",
    "}\n",
    "\n",
    "# Define paths for Core B - ALL available log types\n",
    "core_b_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_Den_MLfilled.csv'\n",
    "}\n",
    "\n",
    "# Define column mapping for alternative column names\n",
    "column_alternatives = {\n",
    "    'hiresMS': ['MS'],\n",
    "    'CT': ['CT_value'],\n",
    "    'R': ['R', 'red', 'Red'],\n",
    "    'G': ['G', 'green', 'Green'],\n",
    "    'B': ['B', 'blue', 'Blue'],\n",
    "    'Lumin': ['luminance', 'Luminance'],\n",
    "    'Den_gm/cc': ['Density', 'density']\n",
    "}\n",
    "\n",
    "# Load data for Core A - ALL available logs\n",
    "log_a, md_a, available_columns_a, _, _ = load_log_data(\n",
    "    core_a_log_paths,\n",
    "    {},  # No images needed for null hypothesis\n",
    "    LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")\n",
    "\n",
    "# Load data for Core B - ALL available logs\n",
    "log_b, md_b, available_columns_b, _, _ = load_log_data(\n",
    "    core_b_log_paths,\n",
    "    {},  # No images needed for null hypothesis\n",
    "    LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")\n",
    "\n",
    "print(f\"Loaded {CORE_A}: {len(log_a)} points, columns: {available_columns_a}\")\n",
    "print(f\"Loaded {CORE_B}: {len(log_b)} points, columns: {available_columns_b}\")\n",
    "print(f\"Log A shape: {log_a.shape}\")\n",
    "print(f\"Log B shape: {log_b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.5: Define All Cores for Segment Pool\n",
    "# Define all available cores to be included in the segment pool\n",
    "SEGMENT_POOL_CORES = [\"M9907-24PC\", \"M9907-26PC\", \"M9907-27PC\"]  # Add any other available cores\n",
    "\n",
    "# Initialize data structure for all cores that will contribute to segment pool\n",
    "segment_pool_cores_data = {\n",
    "    CORE_A: {\n",
    "        'log_data': log_a,\n",
    "        'md_data': md_a,\n",
    "        'available_columns': available_columns_a\n",
    "    },\n",
    "    CORE_B: {\n",
    "        'log_data': log_b,\n",
    "        'md_data': md_b,\n",
    "        'available_columns': available_columns_b\n",
    "    }\n",
    "}\n",
    "\n",
    "# Try to load segment pool cores if they exist\n",
    "for core_name in SEGMENT_POOL_CORES:\n",
    "    # Check if core files exist before attempting to load\n",
    "    core_hiresMS_path = f'{mother_dir}_compiled_logs/{core_name}/ML_filled/{core_name}_hiresMS_MLfilled.csv'\n",
    "    \n",
    "    if not os.path.exists(core_hiresMS_path):\n",
    "        print(f\"Core files for {core_name} do not exist, skipping...\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        # Define paths for segment pool core\n",
    "        core_log_paths = {\n",
    "            'hiresMS': f'{mother_dir}_compiled_logs/{core_name}/ML_filled/{core_name}_hiresMS_MLfilled.csv',\n",
    "            'CT': f'{mother_dir}_compiled_logs/{core_name}/ML_filled/{core_name}_CT_MLfilled.csv',\n",
    "            'Lumin': f'{mother_dir}_compiled_logs/{core_name}/ML_filled/{core_name}_RGB_MLfilled.csv',\n",
    "            'R': f'{mother_dir}_compiled_logs/{core_name}/ML_filled/{core_name}_RGB_MLfilled.csv',\n",
    "            'G': f'{mother_dir}_compiled_logs/{core_name}/ML_filled/{core_name}_RGB_MLfilled.csv',\n",
    "            'B': f'{mother_dir}_compiled_logs/{core_name}/ML_filled/{core_name}_RGB_MLfilled.csv',\n",
    "            'Den_gm/cc': f'{mother_dir}_compiled_logs/{core_name}/ML_filled/{core_name}_Den_MLfilled.csv'\n",
    "        }\n",
    "        \n",
    "        # Load segment pool core data\n",
    "        log_core, md_core, available_columns_core, _, _ = load_log_data(\n",
    "            core_log_paths,\n",
    "            {},\n",
    "            LOG_COLUMNS,\n",
    "            depth_column=DEPTH_COLUMN,\n",
    "            normalize=True,\n",
    "            column_alternatives=column_alternatives\n",
    "        )\n",
    "        \n",
    "        # Define this core for segment pool\n",
    "        segment_pool_cores_data[core_name] = {\n",
    "            'log_data': log_core,\n",
    "            'md_data': md_core,\n",
    "            'available_columns': available_columns_core\n",
    "        }\n",
    "        \n",
    "        print(f\"Successfully loaded {core_name}: {len(log_core)} points, columns: {available_columns_core}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not load {core_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nTotal cores defined for segment pool: {len(segment_pool_cores_data)}\")\n",
    "print(f\"Core names: {list(segment_pool_cores_data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load ALL Picked Depths for ALL Segment Pool Cores\n",
    "print(\"=== Loading Picked Depths for ALL Segment Pool Cores ===\")\n",
    "\n",
    "# Store picked depths for all cores (including segment pool cores)\n",
    "picked_depths_info = {}\n",
    "\n",
    "# Load picked depths for ALL cores in segment pool\n",
    "for core_name in segment_pool_cores_data.keys():\n",
    "    pickeddepth_csv = f'pickeddepth/{core_name}_pickeddepth.csv'\n",
    "    \n",
    "    if os.path.exists(pickeddepth_csv):\n",
    "        try:\n",
    "            picked_data = pd.read_csv(pickeddepth_csv)\n",
    "            # Combine all categories to get maximum segment diversity\n",
    "            all_depths = picked_data['picked_depths_cm'].values.astype('float32')\n",
    "            all_categories = picked_data['category'].values.astype('int')\n",
    "            picked_depths_info[core_name] = list(zip(all_depths.tolist(), all_categories.tolist()))\n",
    "            print(f\"Loaded {len(all_depths)} picked depths for {core_name} (categories: {np.unique(all_categories)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading picked depths for {core_name}: {e}\")\n",
    "            # Create default boundaries if CSV loading fails\n",
    "            core_md = segment_pool_cores_data[core_name]['md_data']\n",
    "            default_depths = np.linspace(core_md[0], core_md[-1], 15)\n",
    "            default_categories = np.ones(len(default_depths), dtype=int)\n",
    "            picked_depths_info[core_name] = list(zip(default_depths.tolist(), default_categories.tolist()))\n",
    "            print(f\"Created {len(default_depths)} default boundaries for {core_name}\")\n",
    "    else:\n",
    "        print(f\"Picked depths file for {core_name} does not exist, creating default boundaries...\")\n",
    "        # Create default boundaries if CSV doesn't exist\n",
    "        core_md = segment_pool_cores_data[core_name]['md_data']\n",
    "        default_depths = np.linspace(core_md[0], core_md[-1], 15)\n",
    "        default_categories = np.ones(len(default_depths), dtype=int)\n",
    "        picked_depths_info[core_name] = list(zip(default_depths.tolist(), default_categories.tolist()))\n",
    "        print(f\"Created {len(default_depths)} default boundaries for {core_name}\")\n",
    "\n",
    "print(f\"\\nPicked depths loaded for {len(picked_depths_info)} cores:\")\n",
    "for core_name, depths in picked_depths_info.items():\n",
    "    print(f\"  {core_name}: {len(depths)} depth boundaries\")\n",
    "\n",
    "print(\"\\nThese picked depths will be used:\")\n",
    "print(f\"- For segment extraction from all cores during segment pool creation\")\n",
    "print(f\"- For DTW analysis with synthetic cores (using {CORE_A} and {CORE_B} boundaries)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create Comprehensive Segment Pool Using Picked Depths\n",
    "print(\"=== Creating Comprehensive Segment Pool Using Actual Picked Depths ===\")\n",
    "\n",
    "# Initialize boundaries data structure\n",
    "all_boundaries_data = {}\n",
    "\n",
    "# Create segments for ALL defined cores using their actual picked depths\n",
    "for core_name in segment_pool_cores_data.keys():\n",
    "    if core_name in picked_depths_info:\n",
    "        # Use actual picked depths to create segments\n",
    "        picked_depths = [depth for depth, category in picked_depths_info[core_name]]\n",
    "        picked_depths = sorted(picked_depths)  # Ensure depths are sorted\n",
    "        \n",
    "        core_md = segment_pool_cores_data[core_name]['md_data']\n",
    "        \n",
    "        # Create segments based on picked depths\n",
    "        segments = []\n",
    "        for i in range(len(picked_depths) - 1):\n",
    "            start_depth = picked_depths[i]\n",
    "            end_depth = picked_depths[i + 1]\n",
    "            \n",
    "            start_idx = np.searchsorted(core_md, start_depth)\n",
    "            end_idx = np.searchsorted(core_md, end_depth)\n",
    "            \n",
    "            if end_idx > start_idx:\n",
    "                segments.append((start_idx, end_idx, f\"seg_{i}\"))\n",
    "        \n",
    "        all_boundaries_data[core_name] = {\n",
    "            'depth_boundaries': np.array(picked_depths),\n",
    "            'segments': segments\n",
    "        }\n",
    "        \n",
    "        print(f\"Created {len(segments)} segments for {core_name} using {len(picked_depths)} picked depths\")\n",
    "    else:\n",
    "        # Fallback to evenly spaced boundaries if no picked depths available\n",
    "        core_md = segment_pool_cores_data[core_name]['md_data']\n",
    "        n_boundaries = max(15, len(core_md) // 100)\n",
    "        boundaries = np.linspace(core_md[0], core_md[-1], n_boundaries)\n",
    "        \n",
    "        simple_segments = []\n",
    "        for i in range(len(boundaries) - 1):\n",
    "            start_idx = np.searchsorted(core_md, boundaries[i])\n",
    "            end_idx = np.searchsorted(core_md, boundaries[i + 1])\n",
    "            if end_idx > start_idx:\n",
    "                simple_segments.append((start_idx, end_idx, f\"seg_{i}\"))\n",
    "        \n",
    "        all_boundaries_data[core_name] = {\n",
    "            'depth_boundaries': boundaries,\n",
    "            'segments': simple_segments\n",
    "        }\n",
    "        \n",
    "        print(f\"Created {len(simple_segments)} segments for {core_name} using default boundaries\")\n",
    "\n",
    "# Create comprehensive segment pool from ALL defined cores\n",
    "segment_pool = create_segment_pool_from_available_cores(segment_pool_cores_data, all_boundaries_data)\n",
    "\n",
    "print(f\"\\nCreated comprehensive segment pool with {len(segment_pool)} segments from {len(segment_pool_cores_data)} cores\")\n",
    "\n",
    "# Display detailed segment pool statistics\n",
    "if segment_pool:\n",
    "    lengths = [seg['length'] for seg in segment_pool]\n",
    "    depth_spans = [seg['depth_span'] for seg in segment_pool]\n",
    "    dimensions = [seg['log_data'].shape[1] if seg['log_data'].ndim > 1 else 1 for seg in segment_pool]\n",
    "    \n",
    "    print(f\"\\n=== Segment Pool Statistics ===\")\n",
    "    print(f\"Segment lengths: min={min(lengths)}, max={max(lengths)}, mean={np.mean(lengths):.1f}\")\n",
    "    print(f\"Depth spans: min={min(depth_spans):.1f}, max={max(depth_spans):.1f}, mean={np.mean(depth_spans):.1f}\")\n",
    "    print(f\"Log dimensions: min={min(dimensions)}, max={max(dimensions)}, mean={np.mean(dimensions):.1f}\")\n",
    "    \n",
    "    # Show core distribution in segment pool\n",
    "    core_counts = {}\n",
    "    for seg in segment_pool:\n",
    "        core_name = seg.get('core_name', 'unknown')\n",
    "        core_counts[core_name] = core_counts.get(core_name, 0) + 1\n",
    "    \n",
    "    print(f\"\\nSegment distribution by core:\")\n",
    "    for core_name, count in core_counts.items():\n",
    "        print(f\"  {core_name}: {count} segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Custom Null Hypothesis with DTW Analysis on Synthetic Cores\n",
    "print(\"=== Computing Null Hypothesis Distribution with DTW Analysis ===\")\n",
    "\n",
    "# Configure target characteristics for synthetic cores\n",
    "target_dimensions = log_a.shape[1] if log_a.ndim > 1 else 1\n",
    "\n",
    "core_a_config = {\n",
    "    'target_length': len(log_a),\n",
    "    'target_dimensions': target_dimensions\n",
    "}\n",
    "\n",
    "core_b_config = {\n",
    "    'target_length': len(log_b), \n",
    "    'target_dimensions': target_dimensions\n",
    "}\n",
    "\n",
    "print(f\"Target Core A: {core_a_config['target_length']} points, {core_a_config['target_dimensions']} dimensions\")\n",
    "print(f\"Target Core B: {core_b_config['target_length']} points, {core_b_config['target_dimensions']} dimensions\")\n",
    "\n",
    "# Manual null hypothesis computation with DTW analysis\n",
    "print(\"Computing null hypothesis with 10,000 synthetic core pairs and DTW analysis...\")\n",
    "print(\"This may take considerable time...\")\n",
    "\n",
    "n_iterations = 10000\n",
    "r_values_null = []\n",
    "successful_iterations = 0\n",
    "failed_iterations = 0\n",
    "\n",
    "for i in tqdm(range(n_iterations), desc=\"Null hypothesis iterations\"):\n",
    "    try:\n",
    "        # Generate synthetic core pair - need to get segment info for picked depths\n",
    "        synthetic_log_a, synthetic_md_a, segment_info_a = generate_synthetic_core_pair(\n",
    "            segment_pool, core_a_config['target_length'], core_a_config['target_dimensions'],\n",
    "            return_segment_info=True  # Get segment info to calculate picked depths\n",
    "        )\n",
    "        synthetic_log_b, synthetic_md_b, segment_info_b = generate_synthetic_core_pair(\n",
    "            segment_pool, core_b_config['target_length'], core_b_config['target_dimensions'],\n",
    "            return_segment_info=True  # Get segment info to calculate picked depths\n",
    "        )\n",
    "        \n",
    "        # Create synthetic picked depths based on segment boundaries\n",
    "        synthetic_picked_a = create_synthetic_picked_depths(synthetic_md_a, segment_info_a)\n",
    "        synthetic_picked_b = create_synthetic_picked_depths(synthetic_md_b, segment_info_b)\n",
    "        \n",
    "        # Run DTW analysis on synthetic cores using synthetic picked depths\n",
    "        dtw_results, valid_dtw_pairs, _, _, _, _, _ = run_comprehensive_dtw_analysis(\n",
    "            synthetic_log_a, synthetic_log_b, synthetic_md_a, synthetic_md_b,\n",
    "            picked_depths_a=synthetic_picked_a,\n",
    "            picked_depths_b=synthetic_picked_b,\n",
    "            top_bottom=True,\n",
    "            independent_dtw=False,\n",
    "            exclude_deadend=True,\n",
    "            create_dtw_matrix=False,  # Skip visualization for speed\n",
    "            creategif=False,  # Skip animation for speed\n",
    "            age_consideration=False,  # No age constraints\n",
    "            debug=False\n",
    "        )\n",
    "        \n",
    "        # Extract correlation coefficients from DTW results\n",
    "        correlations = []\n",
    "        for pair_key in valid_dtw_pairs:\n",
    "            if pair_key in dtw_results:\n",
    "                _, _, quality_metrics = dtw_results[pair_key]\n",
    "                if 'corr_coef' in quality_metrics and not np.isnan(quality_metrics['corr_coef']):\n",
    "                    correlations.append(quality_metrics['corr_coef'])\n",
    "        \n",
    "        # Use mean correlation if we have valid correlations\n",
    "        if correlations:\n",
    "            mean_correlation = np.mean(correlations)\n",
    "            r_values_null.append(mean_correlation)\n",
    "            successful_iterations += 1\n",
    "        else:\n",
    "            failed_iterations += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        failed_iterations += 1\n",
    "        if i < 10:  # Only print first few errors\n",
    "            print(f\"Error in iteration {i}: {e}\")\n",
    "\n",
    "# Calculate distribution statistics\n",
    "r_values_null = np.array(r_values_null)\n",
    "distribution_stats = {\n",
    "    'mean': np.mean(r_values_null),\n",
    "    'std': np.std(r_values_null),\n",
    "    'percentile_95': np.percentile(r_values_null, 95),\n",
    "    'percentile_97_5': np.percentile(r_values_null, 97.5),\n",
    "    'percentile_99': np.percentile(r_values_null, 99)\n",
    "}\n",
    "\n",
    "print(f\"\\n=== NULL HYPOTHESIS RESULTS ===\")\n",
    "print(f\"Successful iterations: {successful_iterations}\")\n",
    "print(f\"Failed iterations: {failed_iterations}\")\n",
    "print(f\"Mean r-value: {distribution_stats['mean']:.4f} Â± {distribution_stats['std']:.4f}\")\n",
    "print(f\"95th percentile threshold: {distribution_stats['percentile_95']:.4f}\")\n",
    "print(f\"97.5th percentile threshold: {distribution_stats['percentile_97_5']:.4f}\")\n",
    "print(f\"99th percentile threshold: {distribution_stats['percentile_99']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Plot R-value Distribution\n",
    "print(\"=== Plotting R-value Distribution ===\")\n",
    "\n",
    "# Create temporary CSV for plot_correlation_distribution compatibility\n",
    "temp_csv = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)\n",
    "temp_csv_path = temp_csv.name\n",
    "\n",
    "# Write data in the format expected by plot_correlation_distribution\n",
    "with open(temp_csv_path, 'w') as f:\n",
    "    f.write(\"mapping_id,corr_coef\\n\")\n",
    "    for i, r_val in enumerate(r_values_null):\n",
    "        f.write(f\"{i},{r_val}\\n\")\n",
    "\n",
    "temp_csv.close()\n",
    "\n",
    "# Plot null hypothesis distribution\n",
    "try:\n",
    "    fig, ax, fit_params = plot_correlation_distribution(\n",
    "        csv_file=temp_csv_path,\n",
    "        quality_index='corr_coef',\n",
    "        save_png=True,\n",
    "        png_filename=f'Null_Hypothesis_Distribution_{CORE_A}_{CORE_B}.png',\n",
    "        core_a_name=CORE_A,\n",
    "        core_b_name=CORE_B,\n",
    "        pdf_method='skew-normal',  # Use skew-normal for better fit\n",
    "        no_bins=50\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "finally:\n",
    "    # Clean up temporary file\n",
    "    if os.path.exists(temp_csv_path):\n",
    "        os.unlink(temp_csv_path)\n",
    "\n",
    "# Save null hypothesis results for future use\n",
    "results_filename = f'Null_Hypothesis_Results_{CORE_A}_{CORE_B}.csv'\n",
    "null_df = pd.DataFrame({\n",
    "    'iteration': range(len(r_values_null)),\n",
    "    'r_value': r_values_null\n",
    "})\n",
    "null_df.to_csv(results_filename, index=False)\n",
    "print(f\"Null hypothesis r-values saved to: {results_filename}\")\n",
    "\n",
    "print(\"=== NULL HYPOTHESIS TESTING COMPLETE ===\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
