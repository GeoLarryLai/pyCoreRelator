{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25kTe2nS84Tf"
   },
   "source": [
    "# Correlating well log pairs: Complex Dynamic Time Warping with boundary constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4awuK6ffO2t"
   },
   "source": [
    "## Introduction to dynamic time warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47Kk07X_84Th"
   },
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyCoreRelator import (\n",
    "    load_log_data,\n",
    "    load_core_age_constraints,\n",
    "    load_pickeddepth_ages_from_csv,\n",
    "    run_multi_parameter_analysis,\n",
    "    calculate_quality_comparison_t_statistics,\n",
    "    plot_quality_comparison_t_statistics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Define basic parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define core pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define core names as variables for easy reference\n",
    "# CORE_A = \"M9907-31PC\"\n",
    "# CORE_A = \"M9907-23PC\"\n",
    "CORE_A = \"M9907-25PC\"\n",
    "# CORE_A = \"M9907-12PC\"\n",
    "# CORE_A = \"M9907-11PC\"\n",
    "# CORE_A = \"RR0207-56PC\" \n",
    "\n",
    "CORE_B = \"M9907-23PC\"\n",
    "# CORE_B = \"M9907-11PC\"\n",
    "# CORE_B = \"M9907-25PC\"\n",
    "# CORE_B = \"RR0207-56PC\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log data paths and column name structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log columns to extract\n",
    "# LOG_COLUMNS = ['hiresMS', 'CT', 'Lumin']  # Choose which logs to include\n",
    "LOG_COLUMNS = ['hiresMS']  # Choose which logs to include\n",
    "\n",
    "# Define depth column\n",
    "DEPTH_COLUMN = 'SB_DEPTH_cm'\n",
    "\n",
    "# For multidimensional DTW, choose DTW method:\n",
    "pca_for_dependent_dtw=False\n",
    "\n",
    "# Define directory paths\n",
    "mother_dir = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/'\n",
    "\n",
    "# Define paths for Core A\n",
    "core_a_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_MST_MLfilled.csv'\n",
    "}\n",
    "\n",
    "# Define paths for Core B\n",
    "core_b_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_MST_MLfilled.csv'\n",
    "}\n",
    "\n",
    "# Define column mapping for alternative column names\n",
    "column_alternatives = {\n",
    "    'hiresMS': ['MS'],\n",
    "    'CT': ['CT_value'],\n",
    "    'R': ['R', 'red', 'Red'],\n",
    "    'G': ['G', 'green', 'Green'],\n",
    "    'B': ['B', 'blue', 'Blue'],\n",
    "    'Lumin': ['luminance', 'Luminance'],\n",
    "    'Den_gm/cc': ['Density', 'density']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Load log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for Core A\n",
    "log_a, md_a, _, _, _ = load_log_data(\n",
    "    core_a_log_paths,\n",
    "    log_columns=LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")\n",
    "\n",
    "# Load data for Core B\n",
    "log_b, md_b, _, _, _ = load_log_data(\n",
    "    core_b_log_paths,\n",
    "    log_columns=LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load picked depth boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Define paths to the CSV files\n",
    "pickeddepth_a_csv = f'pickeddepth/{CORE_A}_pickeddepth.csv'\n",
    "pickeddepth_b_csv = f'pickeddepth/{CORE_B}_pickeddepth.csv'\n",
    "\n",
    "# Load picked depths and extract category 1 depths\n",
    "if os.path.exists(pickeddepth_a_csv):\n",
    "    picked_data_a = pd.read_csv(pickeddepth_a_csv)\n",
    "    all_depths_a_cat1 = picked_data_a[picked_data_a['category'] == 1]['picked_depths_cm'].values.astype('float32')\n",
    "    intepreted_bed_a_cat1 = picked_data_a[picked_data_a['category'] == 1]['interpreted_bed'].fillna('').values.astype('str')\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_a_csv} not found. Using empty array for all_depths_a_cat1.\")\n",
    "    all_depths_a_cat1 = np.array([]).astype('float32')\n",
    "    intepreted_bed_a_cat1 = np.array([]).astype('str').fillna('')\n",
    "\n",
    "if os.path.exists(pickeddepth_b_csv):\n",
    "    picked_data_b = pd.read_csv(pickeddepth_b_csv)\n",
    "    all_depths_b_cat1 = picked_data_b[picked_data_b['category'] == 1]['picked_depths_cm'].values.astype('float32')\n",
    "    intepreted_bed_b_cat1 = picked_data_b[picked_data_b['category'] == 1]['interpreted_bed'].fillna('').values.astype('str')\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_b_csv} not found. Using empty array for all_depths_b_cat1.\")\n",
    "    all_depths_b_cat1 = np.array([]).astype('float32')\n",
    "    intepreted_bed_b_cat1 = np.array([]).astype('str').fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load age data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load age constraints for both cores\n",
    "consider_adjacent_core = True\n",
    "\n",
    "data_columns = {                                # Define age constraint data columns in the imported csv file   \n",
    "    'age': 'calib502_agebp',                    # Calibrated radiocarbon age in years before present\n",
    "    'pos_error': 'calib502_2sigma_pos',         # Positive 2-sigma uncertainty in age\n",
    "    'neg_error': 'calib502_2sigma_neg',         # Negative 2-sigma uncertainty in age\n",
    "    'min_depth': 'mindepth_cm',                 # Minimum depth of age constraint in cm\n",
    "    'max_depth': 'maxdepth_cm',                 # Maximum depth of age constraint in cm\n",
    "    'in_sequence': 'in_sequence',               # Flag indicating if constraint is in stratigraphic sequence\n",
    "    'core': 'core',                             # Core identifier for the age constraint\n",
    "    'interpreted_bed': 'interpreted_bed'        # Interpreted bed/layer identifier\n",
    "}\n",
    "\n",
    "# Configuration: Define the path to the age constraints csv file\n",
    "age_base_path = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/Age constraints/Goldfinger2012'\n",
    "\n",
    "# Load age constraints for both cores: Load the age constraints for both cores from the csv file\n",
    "age_data_a = load_core_age_constraints(CORE_A, age_base_path, consider_adjacent_core, data_columns, mute_mode=True)\n",
    "age_data_b = load_core_age_constraints(CORE_B, age_base_path, consider_adjacent_core, data_columns, mute_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load estimated ages for each picked depth boundary from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load estimated boundary age data for both cores from CSV files using the imported function\n",
    "cores = [CORE_A, CORE_B]\n",
    "pickeddepth_ages = {}\n",
    "\n",
    "# Define the uncertainty method: 'MonteCarlo', 'Linear', or 'Gaussian'\n",
    "uncertainty_method='MonteCarlo'   \n",
    "\n",
    "for core in cores:\n",
    "    core_age_csv = f\"pickeddepth_ages/{core}_pickeddepth_ages_{uncertainty_method}.csv\"\n",
    "    pickeddepth_ages[core] = load_pickeddepth_ages_from_csv(core_age_csv)\n",
    "\n",
    "# Assign to individual variables for backward compatibility\n",
    "if CORE_A in pickeddepth_ages:\n",
    "    pickeddepth_ages_a = pickeddepth_ages[CORE_A]\n",
    "if CORE_B in pickeddepth_ages:\n",
    "    pickeddepth_ages_b = pickeddepth_ages[CORE_B]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Compute quality metric distribution for all stituation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Multi-Parameter Distribution Analysis\n",
    "\n",
    "test_age_constraint_removal = True  # Set to False to disable age constraint removal testing\n",
    "\n",
    "# For multidimensional DTW, choose DTW method:\n",
    "pca_for_dependent_dtw=False\n",
    "\n",
    "# Run all parameter combinations and plot distribution curves together\n",
    "\n",
    "# Define all parameter combinations to test\n",
    "parameter_combinations = [\n",
    "    # {'age_consideration': True, 'restricted_age_correlation': True, 'shortest_path_search': False},\n",
    "    {'age_consideration': True, 'restricted_age_correlation': True, 'shortest_path_search': True},\n",
    "    # {'age_consideration': True, 'restricted_age_correlation': False, 'shortest_path_search': False},\n",
    "    # {'age_consideration': True, 'restricted_age_correlation': False, 'shortest_path_search': True},\n",
    "    # {'age_consideration': False, 'restricted_age_correlation': False, 'shortest_path_search': False},\n",
    "    {'age_consideration': False, 'restricted_age_correlation': False, 'shortest_path_search': True}\n",
    "]\n",
    "\n",
    "# Define all quality indices to process\n",
    "target_quality_indices = ['corr_coef', 'norm_dtw']\n",
    "\n",
    "output_csv_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    output_csv_filenames[quality_index] = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_fit_params.csv'\n",
    "\n",
    "# Define synthetic CSV filenames for consistent bin sizing\n",
    "synthetic_csv_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    synthetic_csv_filenames[quality_index] = f'outputs/synthetic_PDFs_{\"_\".join(LOG_COLUMNS)}_{quality_index}.csv'\n",
    "\n",
    "# Execute the analysis function\n",
    "run_multi_parameter_analysis(\n",
    "    # Core data inputs\n",
    "    log_a=log_a, \n",
    "    log_b=log_b, \n",
    "    md_a=md_a, \n",
    "    md_b=md_b,\n",
    "    all_depths_a_cat1=all_depths_a_cat1,\n",
    "    all_depths_b_cat1=all_depths_b_cat1,\n",
    "    pickeddepth_ages_a=pickeddepth_ages_a,\n",
    "    pickeddepth_ages_b=pickeddepth_ages_b,\n",
    "    age_data_a=age_data_a,\n",
    "    age_data_b=age_data_b,\n",
    "    uncertainty_method=uncertainty_method,\n",
    "    \n",
    "    # Analysis parameters\n",
    "    parameter_combinations=parameter_combinations,\n",
    "    target_quality_indices=target_quality_indices,\n",
    "    test_age_constraint_removal=test_age_constraint_removal,\n",
    "    \n",
    "    # Core identifiers\n",
    "    core_a_name=CORE_A,\n",
    "    core_b_name=CORE_B,\n",
    "    \n",
    "    # Output configuration\n",
    "    output_csv_filenames=output_csv_filenames,\n",
    "    \n",
    "    # Optional parameter\n",
    "    pca_for_dependent_dtw=pca_for_dependent_dtw,\n",
    "\n",
    "    # Maximum number of scenarios per constraint removal layer\n",
    "    max_search_per_layer= 100,\n",
    "\n",
    "    # Number of cores used in parallel processing\n",
    "    n_jobs=-1     # -1: uses all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting: compare the quality metric to the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file names outside the function\n",
    "target_quality_indices = ['corr_coef', 'norm_dtw']\n",
    "\n",
    "# Define input path for master CSV filenames\n",
    "master_csv_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    master_csv_filenames[quality_index] = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_fit_params.csv'\n",
    "\n",
    "# Define input path for synthetic CSV filenames\n",
    "synthetic_csv_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    synthetic_csv_filenames[quality_index] = f'outputs/synthetic_PDFs_{\"_\".join(LOG_COLUMNS)}_{quality_index}.csv'\n",
    "\n",
    "# Define output figure filenames\n",
    "output_figure_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    output_figure_filenames[quality_index] = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_compare2null.png'\n",
    "\n",
    "# Define animated gifs showing progressive constraint addition\n",
    "output_gif_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    output_gif_filenames[quality_index] = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_compare2null.gif'\n",
    "        \n",
    "# Step 1: Calculate statistics (run once)\n",
    "calculate_quality_comparison_t_statistics(\n",
    "    target_quality_indices=target_quality_indices,\n",
    "    master_csv_filenames=master_csv_filenames,\n",
    "    synthetic_csv_filenames=synthetic_csv_filenames,\n",
    "    CORE_A=CORE_A,\n",
    "    CORE_B=CORE_B,\n",
    "    mute_mode=False\n",
    ")\n",
    "\n",
    "# Step 2: Plot results\n",
    "plot_quality_comparison_t_statistics(\n",
    "    target_quality_indices=target_quality_indices,\n",
    "    master_csv_filenames=master_csv_filenames,\n",
    "    synthetic_csv_filenames=synthetic_csv_filenames,\n",
    "    CORE_A=CORE_A,\n",
    "    CORE_B=CORE_B,\n",
    "    mute_mode=True,\n",
    "    save_fig=True,\n",
    "    output_figure_filenames=output_figure_filenames,\n",
    "    save_gif=False, \n",
    "    max_frames = 40,\n",
    "    output_gif_filenames=output_gif_filenames,\n",
    "    plot_real_data_histogram=True,\n",
    "    plot_age_removal_step_pdf=False\n",
    "    )\n",
    "\n",
    "plot_quality_comparison_t_statistics(\n",
    "    target_quality_indices=target_quality_indices,\n",
    "    master_csv_filenames=master_csv_filenames,\n",
    "    synthetic_csv_filenames=synthetic_csv_filenames,\n",
    "    CORE_A=CORE_A,\n",
    "    CORE_B=CORE_B,\n",
    "    mute_mode=True,\n",
    "    save_fig=False,\n",
    "    output_figure_filenames=output_figure_filenames,\n",
    "    save_gif=True, \n",
    "    max_frames = 40,\n",
    "    output_gif_filenames=output_gif_filenames,\n",
    "    plot_real_data_histogram=False,\n",
    "    plot_age_removal_step_pdf=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Loop Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all core options\n",
    "from re import T\n",
    "\n",
    "\n",
    "CORE_A_OPTIONS = [\"M9907-11PC\", \"M9907-12PC\", \"M9907-23PC\", \"M9907-25PC\", \"M9907-30PC\", \"M9907-31PC\", \"RR0207-56PC\"]\n",
    "CORE_B_OPTIONS = [\"M9907-11PC\", \"M9907-23PC\", \"M9907-30PC\"]\n",
    "\n",
    "# Define log columns to extract\n",
    "# LOG_COLUMNS = ['hiresMS', 'CT', 'Lumin']  # Choose which logs to include\n",
    "LOG_COLUMNS = ['hiresMS']  # Choose which logs to include\n",
    "# LOG_COLUMNS = ['CT']  # Choose which logs to include\n",
    "# LOG_COLUMNS = ['Lumin']  # Choose which logs to include\n",
    "\n",
    "# Define depth column\n",
    "DEPTH_COLUMN = 'SB_DEPTH_cm'\n",
    "\n",
    "# For multidimensional DTW, choose DTW method:\n",
    "pca_for_dependent_dtw=False\n",
    "\n",
    "# Define directory paths\n",
    "mother_dir = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/'\n",
    "\n",
    "# Define all quality indices to process\n",
    "target_quality_indices = ['corr_coef', 'norm_dtw', 'perc_diag']\n",
    "# target_quality_indices = ['dtw_warp_eff'] \n",
    "\n",
    "# Define all parameter combinations to test\n",
    "parameter_combinations = [\n",
    "    {'age_consideration': True, 'restricted_age_correlation': True, 'shortest_path_search': True},\n",
    "    {'age_consideration': False, 'restricted_age_correlation': False, 'shortest_path_search': True}\n",
    "]\n",
    "\n",
    "# Configuration\n",
    "age_base_path = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/Age constraints/Goldfinger2012'\n",
    "uncertainty_method='MonteCarlo'   # 'MonteCarlo', 'Linear', or 'Gaussian'\n",
    "test_age_constraint_removal = True  # Set to False to disable age constraint removal testing\n",
    "\n",
    "def force_cleanup():\n",
    "    \"\"\"Force aggressive garbage collection and memory cleanup\"\"\"\n",
    "    plt.close('all')\n",
    "    for _ in range(3):\n",
    "        gc.collect()\n",
    "\n",
    "######\n",
    "# Loop through all valid combinations of CORE_A and CORE_B\n",
    "for CORE_A in CORE_A_OPTIONS:\n",
    "    for CORE_B in CORE_B_OPTIONS:\n",
    "        # Skip if CORE_A and CORE_B are the same\n",
    "        if CORE_A == CORE_B:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing core pair: {CORE_A} vs {CORE_B}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Define paths for Core A\n",
    "            core_a_log_paths = {\n",
    "                'hiresMS': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_hiresMS_MLfilled.csv',\n",
    "                'CT': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_CT_MLfilled.csv',\n",
    "                'Lumin': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "                'R': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "                'G': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "                'B': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "                'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_MST_MLfilled.csv'\n",
    "            }\n",
    "\n",
    "            # Define paths for Core B\n",
    "            core_b_log_paths = {\n",
    "                'hiresMS': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_hiresMS_MLfilled.csv',\n",
    "                'CT': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_CT_MLfilled.csv',\n",
    "                'Lumin': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "                'R': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "                'G': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "                'B': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "                'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_MST_MLfilled.csv'\n",
    "            }\n",
    "\n",
    "            # Define column mapping for alternative column names\n",
    "            column_alternatives = {\n",
    "                'hiresMS': ['MS'],\n",
    "                'CT': ['CT_value'],\n",
    "                'R': ['R', 'red', 'Red'],\n",
    "                'G': ['G', 'green', 'Green'],\n",
    "                'B': ['B', 'blue', 'Blue'],\n",
    "                'Lumin': ['luminance', 'Luminance'],\n",
    "                'Den_gm/cc': ['Density', 'density']\n",
    "            }\n",
    "\n",
    "            # Load data for Core A\n",
    "            log_a, md_a, _, _, _ = load_log_data(\n",
    "                core_a_log_paths,\n",
    "                log_columns=LOG_COLUMNS,\n",
    "                depth_column=DEPTH_COLUMN,\n",
    "                normalize=True,\n",
    "                column_alternatives=column_alternatives\n",
    "            )\n",
    "\n",
    "            # Load data for Core B\n",
    "            log_b, md_b, _, _, _ = load_log_data(\n",
    "                core_b_log_paths,\n",
    "                log_columns=LOG_COLUMNS,\n",
    "                depth_column=DEPTH_COLUMN,\n",
    "                normalize=True,\n",
    "                column_alternatives=column_alternatives\n",
    "            )\n",
    "\n",
    "            # Define paths to the CSV files\n",
    "            pickeddepth_a_csv = f'pickeddepth/{CORE_A}_pickeddepth.csv'\n",
    "            pickeddepth_b_csv = f'pickeddepth/{CORE_B}_pickeddepth.csv'\n",
    "\n",
    "            # Load picked depths and extract category 1 depths\n",
    "            if os.path.exists(pickeddepth_b_csv):\n",
    "                picked_data_b = pd.read_csv(pickeddepth_b_csv)\n",
    "                all_depths_b_cat1 = picked_data_b[picked_data_b['category'] == 1]['picked_depths_cm'].values.astype('float32')\n",
    "            else:\n",
    "                print(f\"Warning: {pickeddepth_b_csv} not found. Using empty array for all_depths_b_cat1.\")\n",
    "                all_depths_b_cat1 = np.array([]).astype('float32')\n",
    "\n",
    "            if os.path.exists(pickeddepth_a_csv):\n",
    "                picked_data_a = pd.read_csv(pickeddepth_a_csv)\n",
    "                all_depths_a_cat1 = picked_data_a[picked_data_a['category'] == 1]['picked_depths_cm'].values.astype('float32')\n",
    "            else:\n",
    "                print(f\"Warning: {pickeddepth_a_csv} not found. Using empty array for all_depths_a_cat1.\")\n",
    "                all_depths_a_cat1 = np.array([]).astype('float32')\n",
    "\n",
    "            # Load age constraints for both cores\n",
    "            consider_adjacent_core = True\n",
    "\n",
    "            data_columns = {\n",
    "                'age': 'calib502_agebp',\n",
    "                'pos_error': 'calib502_2sigma_pos', \n",
    "                'neg_error': 'calib502_2sigma_neg',\n",
    "                'min_depth': 'mindepth_cm',\n",
    "                'max_depth': 'maxdepth_cm',\n",
    "                'in_sequence': 'in_sequence',\n",
    "                'core': 'core',\n",
    "                'interpreted_bed': 'interpreted_bed'\n",
    "            }\n",
    "\n",
    "            # Load age constraints for both cores\n",
    "            age_data_a = load_core_age_constraints(CORE_A, age_base_path, consider_adjacent_core, data_columns, mute_mode=True)\n",
    "            age_data_b = load_core_age_constraints(CORE_B, age_base_path, consider_adjacent_core, data_columns, mute_mode=True)\n",
    "\n",
    "            # Load pickeddepth ages from CSV files\n",
    "            core_a_age_csv = f\"pickeddepth_ages/{CORE_A}_pickeddepth_ages_{uncertainty_method}.csv\"\n",
    "            core_b_age_csv = f\"pickeddepth_ages/{CORE_B}_pickeddepth_ages_{uncertainty_method}.csv\"\n",
    "            \n",
    "            pickeddepth_ages_a = load_pickeddepth_ages_from_csv(core_a_age_csv)\n",
    "            pickeddepth_ages_b = load_pickeddepth_ages_from_csv(core_b_age_csv)\n",
    "\n",
    "            # Set up output CSV filenames\n",
    "            output_csv_filenames = {}\n",
    "            for quality_index in target_quality_indices:\n",
    "                output_csv_filenames[quality_index] = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_fit_params.csv'\n",
    "            \n",
    "            # Execute the analysis function\n",
    "            run_multi_parameter_analysis(\n",
    "                # Core data inputs\n",
    "                log_a=log_a, \n",
    "                log_b=log_b, \n",
    "                md_a=md_a, \n",
    "                md_b=md_b,\n",
    "                all_depths_a_cat1=all_depths_a_cat1,\n",
    "                all_depths_b_cat1=all_depths_b_cat1,\n",
    "                pickeddepth_ages_a=pickeddepth_ages_a,\n",
    "                pickeddepth_ages_b=pickeddepth_ages_b,\n",
    "                age_data_a=age_data_a,\n",
    "                age_data_b=age_data_b,\n",
    "                uncertainty_method=uncertainty_method,\n",
    "                \n",
    "                # Analysis parameters\n",
    "                parameter_combinations=parameter_combinations,\n",
    "                target_quality_indices=target_quality_indices,\n",
    "                test_age_constraint_removal=test_age_constraint_removal,\n",
    "                \n",
    "                # Core identifiers\n",
    "                core_a_name=CORE_A,\n",
    "                core_b_name=CORE_B,\n",
    "                \n",
    "                # Output configuration\n",
    "                output_csv_filenames=output_csv_filenames,\n",
    "                \n",
    "                # Optional parameter\n",
    "                pca_for_dependent_dtw=pca_for_dependent_dtw,\n",
    "\n",
    "                # Maximum number of scenarios per constraint removal layer\n",
    "                max_search_per_layer= 100,\n",
    "\n",
    "                # Number of cores used in parallel processing\n",
    "                n_jobs=-1     # -1: uses all available cores\n",
    "            )\n",
    "\n",
    "            # Define input path for master CSV filenames\n",
    "            master_csv_filenames = {}\n",
    "            for quality_index in target_quality_indices:\n",
    "                master_csv_filenames[quality_index] = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_fit_params.csv'\n",
    "\n",
    "            # Define input path for synthetic CSV filenames\n",
    "            synthetic_csv_filenames = {}\n",
    "            for quality_index in target_quality_indices:\n",
    "                synthetic_csv_filenames[quality_index] = f'outputs/synthetic_PDFs_{\"_\".join(LOG_COLUMNS)}_{quality_index}.csv'\n",
    "\n",
    "            # Define output figure filenames\n",
    "            output_figure_filenames = {}\n",
    "            for quality_index in target_quality_indices:\n",
    "                output_figure_filenames[quality_index] = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_compare2null.png'\n",
    "\n",
    "            # Define animated gifs showing progressive constraint addition\n",
    "            output_gif_filenames = {}\n",
    "            for quality_index in target_quality_indices:\n",
    "                output_gif_filenames[quality_index] = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_compare2null.gif'\n",
    "                    \n",
    "            # Step 1: Calculate statistics (run once)\n",
    "            calculate_quality_comparison_t_statistics(\n",
    "                target_quality_indices=target_quality_indices,\n",
    "                master_csv_filenames=master_csv_filenames,\n",
    "                synthetic_csv_filenames=synthetic_csv_filenames,\n",
    "                CORE_A=CORE_A,\n",
    "                CORE_B=CORE_B,\n",
    "                mute_mode=False\n",
    "            )\n",
    "            \n",
    "            # Force cleanup after CSV processing\n",
    "            force_cleanup()\n",
    "\n",
    "            # Step 2: Plot results\n",
    "            plot_quality_comparison_t_statistics(\n",
    "                target_quality_indices=target_quality_indices,\n",
    "                master_csv_filenames=master_csv_filenames,\n",
    "                synthetic_csv_filenames=synthetic_csv_filenames,\n",
    "                CORE_A=CORE_A,\n",
    "                CORE_B=CORE_B,\n",
    "                mute_mode=True,\n",
    "                save_fig=True,\n",
    "                output_figure_filenames=output_figure_filenames,\n",
    "                save_gif=False, \n",
    "                max_frames = 40,\n",
    "                output_gif_filenames=output_gif_filenames,\n",
    "                plot_real_data_histogram=True,\n",
    "                plot_age_removal_step_pdf=False\n",
    "                )\n",
    "\n",
    "            plot_quality_comparison_t_statistics(\n",
    "                target_quality_indices=target_quality_indices,\n",
    "                master_csv_filenames=master_csv_filenames,\n",
    "                synthetic_csv_filenames=synthetic_csv_filenames,\n",
    "                CORE_A=CORE_A,\n",
    "                CORE_B=CORE_B,\n",
    "                mute_mode=True,\n",
    "                save_fig=False,\n",
    "                output_figure_filenames=output_figure_filenames,\n",
    "                save_gif=True, \n",
    "                max_frames = 40,\n",
    "                output_gif_filenames=output_gif_filenames,\n",
    "                plot_real_data_histogram=False,\n",
    "                plot_age_removal_step_pdf=True\n",
    "                )\n",
    "\n",
    "            # Force cleanup after processing\n",
    "            force_cleanup()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing core pair {CORE_A} vs {CORE_B}: {e}\")\n",
    "            print(\"Skipping to next core pair...\")\n",
    "            continue\n",
    "        \n",
    "        finally:\n",
    "            # Clean up memory before next iteration\n",
    "            del log_a, md_a, log_b, md_b\n",
    "            del pickeddepth_a_csv, pickeddepth_b_csv\n",
    "            del picked_data_a, picked_data_b\n",
    "            del all_depths_a_cat1, all_depths_b_cat1\n",
    "            del age_data_a, age_data_b\n",
    "            del pickeddepth_ages_a, pickeddepth_ages_b\n",
    "            del core_a_log_paths, core_b_log_paths, column_alternatives\n",
    "            del master_csv_filenames, synthetic_csv_filenames\n",
    "            del output_figure_filenames, output_gif_filenames, output_csv_filenames\n",
    "            \n",
    "            # Close any matplotlib figures to free memory\n",
    "            plt.close('all')\n",
    "            \n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "            \n",
    "            print(f\"Memory cleared after processing {CORE_A} vs {CORE_B}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"All core pair combinations have been processed!\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "11-TimeSeriesCorrelation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
