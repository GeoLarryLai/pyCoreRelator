{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25kTe2nS84Tf"
   },
   "source": [
    "# Correlating well log pairs: Complex Dynamic Time Warping with boundary constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4awuK6ffO2t"
   },
   "source": [
    "## Introduction to dynamic time warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47Kk07X_84Th"
   },
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyCoreRelator import (\n",
    "    load_log_data,\n",
    "    load_core_age_constraints,\n",
    "    load_pickeddepth_ages_from_csv,\n",
    "    run_multi_parameter_analysis,\n",
    "    calculate_quality_comparison_t_statistics,\n",
    "    plot_quality_comparison_t_statistics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Define basic parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define core pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define core names as variables for easy reference\n",
    "# CORE_A = \"M9907-22PC\"\n",
    "# CORE_A = \"M9907-25PC\"\n",
    "# CORE_A = \"M9907-12PC\"\n",
    "# CORE_A = \"M9907-31PC\"\n",
    "# CORE_A = \"M9907-11PC\"\n",
    "# CORE_A = \"RR0207-56PC\" \n",
    "CORE_A = \"M9907-01PC\"\n",
    "\n",
    "# CORE_B = \"M9907-23PC\"\n",
    "CORE_B = \"M9907-11PC\"\n",
    "# CORE_B = \"M9907-25PC\"\n",
    "# CORE_B = \"M9907-30PC\"\n",
    "# CORE_B = \"RR0207-56PC\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log data paths and column name structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log columns to extract\n",
    "# LOG_COLUMNS = ['hiresMS', 'CT', 'Lumin']  # Choose which logs to include\n",
    "# LOG_COLUMNS = ['hiresMS']  # Choose which logs to include\n",
    "LOG_COLUMNS = ['CT']  # Choose which logs to include\n",
    "# LOG_COLUMNS = ['Lumin']  # Choose which logs to include\n",
    "\n",
    "# Define depth column\n",
    "DEPTH_COLUMN = 'SB_DEPTH_cm'\n",
    "\n",
    "# For multidimensional DTW, choose DTW method:\n",
    "pca_for_dependent_dtw=False\n",
    "\n",
    "# Define directory paths\n",
    "mother_dir = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/'\n",
    "\n",
    "# Define paths for Core A\n",
    "core_a_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_MST_MLfilled.csv'\n",
    "}\n",
    "\n",
    "# Define paths for Core B\n",
    "core_b_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_MST_MLfilled.csv'\n",
    "}\n",
    "\n",
    "# Define column mapping for alternative column names\n",
    "column_alternatives = {\n",
    "    'hiresMS': ['MS'],\n",
    "    'CT': ['CT_value'],\n",
    "    'R': ['R', 'red', 'Red'],\n",
    "    'G': ['G', 'green', 'Green'],\n",
    "    'B': ['B', 'blue', 'Blue'],\n",
    "    'Lumin': ['luminance', 'Luminance'],\n",
    "    'Den_gm/cc': ['Density', 'density']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Load log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for Core A\n",
    "log_a, md_a, _, _, _ = load_log_data(\n",
    "    core_a_log_paths,\n",
    "    log_columns=LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")\n",
    "\n",
    "# Load data for Core B\n",
    "log_b, md_b, _, _, _ = load_log_data(\n",
    "    core_b_log_paths,\n",
    "    log_columns=LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load picked depth boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Define paths to the CSV files\n",
    "pickeddepth_a_csv = f'pickeddepth/{CORE_A}_pickeddepth.csv'\n",
    "pickeddepth_b_csv = f'pickeddepth/{CORE_B}_pickeddepth.csv'\n",
    "\n",
    "# Load picked depths and extract category 1 depths\n",
    "if os.path.exists(pickeddepth_a_csv):\n",
    "    picked_data_a = pd.read_csv(pickeddepth_a_csv)\n",
    "    all_depths_a_cat1 = picked_data_a[picked_data_a['category'] == 1]['picked_depths_cm'].values.astype('float32')\n",
    "    intepreted_bed_a_cat1 = picked_data_a[picked_data_a['category'] == 1]['interpreted_bed'].fillna('').values.astype('str')\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_a_csv} not found. Using empty array for all_depths_a_cat1.\")\n",
    "    all_depths_a_cat1 = np.array([]).astype('float32')\n",
    "    intepreted_bed_a_cat1 = np.array([]).astype('str').fillna('')\n",
    "\n",
    "if os.path.exists(pickeddepth_b_csv):\n",
    "    picked_data_b = pd.read_csv(pickeddepth_b_csv)\n",
    "    all_depths_b_cat1 = picked_data_b[picked_data_b['category'] == 1]['picked_depths_cm'].values.astype('float32')\n",
    "    intepreted_bed_b_cat1 = picked_data_b[picked_data_b['category'] == 1]['interpreted_bed'].fillna('').values.astype('str')\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_b_csv} not found. Using empty array for all_depths_b_cat1.\")\n",
    "    all_depths_b_cat1 = np.array([]).astype('float32')\n",
    "    intepreted_bed_b_cat1 = np.array([]).astype('str').fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load age data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load age constraints for both cores\n",
    "consider_adjacent_core = True\n",
    "\n",
    "data_columns = {\n",
    "    'age': 'calib810_agebp',\n",
    "    'pos_error': 'calib810_2sigma_pos', \n",
    "    'neg_error': 'calib810_2sigma_neg',\n",
    "    'min_depth': 'mindepth_cm',\n",
    "    'max_depth': 'maxdepth_cm',\n",
    "    'in_sequence': 'in_sequence',\n",
    "    'core': 'core',\n",
    "    'interpreted_bed': 'interpreted_bed'\n",
    "}\n",
    "\n",
    "# Configuration: Define the path to the age constraints csv file\n",
    "age_base_path = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/Age constraints/Goldfinger2012'\n",
    "\n",
    "# Load age constraints for both cores: Load the age constraints for both cores from the csv file\n",
    "age_data_a = load_core_age_constraints(CORE_A, age_base_path, consider_adjacent_core, data_columns, mute_mode=True)\n",
    "age_data_b = load_core_age_constraints(CORE_B, age_base_path, consider_adjacent_core, data_columns, mute_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load estimated ages for each picked depth boundary from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load estimated boundary age data for both cores from CSV files using the imported function\n",
    "cores = [CORE_A, CORE_B]\n",
    "pickeddepth_ages = {}\n",
    "\n",
    "# Define the uncertainty method: 'MonteCarlo', 'Linear', or 'Gaussian'\n",
    "uncertainty_method='MonteCarlo'   \n",
    "\n",
    "for core in cores:\n",
    "    core_age_csv = f\"pickeddepth_ages/{core}_pickeddepth_ages_{uncertainty_method}.csv\"\n",
    "    pickeddepth_ages[core] = load_pickeddepth_ages_from_csv(core_age_csv)\n",
    "\n",
    "# Assign to individual variables for backward compatibility\n",
    "if CORE_A in pickeddepth_ages:\n",
    "    pickeddepth_ages_a = pickeddepth_ages[CORE_A]\n",
    "if CORE_B in pickeddepth_ages:\n",
    "    pickeddepth_ages_b = pickeddepth_ages[CORE_B]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Compute quality metric distribution for all stituation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Multi-Parameter Distribution Analysis\n",
    "\n",
    "test_age_constraint_removal = True  # Set to False to disable age constraint removal testing\n",
    "\n",
    "# For multidimensional DTW, choose DTW method:\n",
    "pca_for_dependent_dtw=False\n",
    "\n",
    "# Run all parameter combinations and plot distribution curves together\n",
    "\n",
    "# Define all parameter combinations to test\n",
    "parameter_combinations = [\n",
    "    # {'age_consideration': True, 'restricted_age_correlation': True, 'shortest_path_search': False},\n",
    "    {'age_consideration': True, 'restricted_age_correlation': True, 'shortest_path_search': True},\n",
    "    # {'age_consideration': True, 'restricted_age_correlation': False, 'shortest_path_search': False},\n",
    "    # {'age_consideration': True, 'restricted_age_correlation': False, 'shortest_path_search': True},\n",
    "    # {'age_consideration': False, 'restricted_age_correlation': False, 'shortest_path_search': False},\n",
    "    {'age_consideration': False, 'restricted_age_correlation': False, 'shortest_path_search': True}\n",
    "]\n",
    "\n",
    "# Define all quality indices to process\n",
    "target_quality_indices = ['corr_coef', 'norm_dtw']\n",
    "\n",
    "output_csv_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    output_csv_filenames[quality_index] = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_fit_params.csv'\n",
    "\n",
    "# Define synthetic CSV filenames for consistent bin sizing\n",
    "synthetic_csv_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    synthetic_csv_filenames[quality_index] = f'outputs/synthetic_PDFs_{\"_\".join(LOG_COLUMNS)}_{quality_index}.csv'\n",
    "\n",
    "# Execute the analysis function\n",
    "run_multi_parameter_analysis(\n",
    "    # Core data inputs\n",
    "    log_a=log_a, \n",
    "    log_b=log_b, \n",
    "    md_a=md_a, \n",
    "    md_b=md_b,\n",
    "    all_depths_a_cat1=all_depths_a_cat1,\n",
    "    all_depths_b_cat1=all_depths_b_cat1,\n",
    "    pickeddepth_ages_a=pickeddepth_ages_a,\n",
    "    pickeddepth_ages_b=pickeddepth_ages_b,\n",
    "    age_data_a=age_data_a,\n",
    "    age_data_b=age_data_b,\n",
    "    uncertainty_method=uncertainty_method,\n",
    "    \n",
    "    # Analysis parameters\n",
    "    parameter_combinations=parameter_combinations,\n",
    "    target_quality_indices=target_quality_indices,\n",
    "    test_age_constraint_removal=test_age_constraint_removal,\n",
    "    \n",
    "    # Core identifiers\n",
    "    core_a_name=CORE_A,\n",
    "    core_b_name=CORE_B,\n",
    "    \n",
    "    # Output configuration\n",
    "    output_csv_filenames=output_csv_filenames,\n",
    "    \n",
    "    # Optional parameter\n",
    "    pca_for_dependent_dtw=pca_for_dependent_dtw,\n",
    "\n",
    "    # Maximum number of scenarios per constraint removal layer\n",
    "    max_search_per_layer= 100,\n",
    "\n",
    "    # Number of cores used in parallel processing\n",
    "    n_jobs=-1     # -1: uses all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting: compare the quality metric to the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file names outside the function\n",
    "target_quality_indices = ['corr_coef', 'norm_dtw']\n",
    "\n",
    "# Define input path for master CSV filenames\n",
    "master_csv_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    master_csv_filenames[quality_index] = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_fit_params.csv'\n",
    "\n",
    "# Define input path for synthetic CSV filenames\n",
    "synthetic_csv_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    synthetic_csv_filenames[quality_index] = f'outputs/synthetic_PDFs_{\"_\".join(LOG_COLUMNS)}_{quality_index}.csv'\n",
    "\n",
    "# Define output figure filenames\n",
    "output_figure_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    output_figure_filenames[quality_index] = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_compare2null.png'\n",
    "\n",
    "# Define animated gifs showing progressive constraint addition\n",
    "output_gif_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    output_gif_filenames[quality_index] = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_compare2null.gif'\n",
    "\n",
    "# Define mapping CSV filename: Try restricted_age_optimal first, fallback to no_age_optimal if it doesn't exist\n",
    "sequential_mappings_csv = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/mappings_restricted_age_optimal.csv'\n",
    "if not os.path.exists(sequential_mappings_csv):\n",
    "    sequential_mappings_csv = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/mappings_no_age_optimal.csv'\n",
    "    if not os.path.exists(sequential_mappings_csv):\n",
    "        sequential_mappings_csv = None\n",
    "        \n",
    "# Step 1: Calculate statistics (run once)\n",
    "calculate_quality_comparison_t_statistics(\n",
    "    target_quality_indices=target_quality_indices,\n",
    "    master_csv_filenames=master_csv_filenames,\n",
    "    synthetic_csv_filenames=synthetic_csv_filenames,\n",
    "    CORE_A=CORE_A,\n",
    "    CORE_B=CORE_B,\n",
    "    mute_mode=False\n",
    ")\n",
    "\n",
    "# Step 2: Plot results\n",
    "plot_quality_comparison_t_statistics(\n",
    "    target_quality_indices=target_quality_indices,\n",
    "    master_csv_filenames=master_csv_filenames,\n",
    "    synthetic_csv_filenames=synthetic_csv_filenames,\n",
    "    CORE_A=CORE_A,\n",
    "    CORE_B=CORE_B,\n",
    "    mute_mode=True,\n",
    "    save_fig=True,\n",
    "    output_figure_filenames=output_figure_filenames,\n",
    "    save_gif=False, \n",
    "    output_gif_filenames=output_gif_filenames,\n",
    "    max_frames=40,\n",
    "    plot_real_data_histogram=True,\n",
    "    plot_age_removal_step_pdf=False,\n",
    "    show_best_datum_match=True,\n",
    "    sequential_mappings_csv=sequential_mappings_csv\n",
    ")\n",
    "\n",
    "plot_quality_comparison_t_statistics(\n",
    "    target_quality_indices=target_quality_indices,\n",
    "    master_csv_filenames=master_csv_filenames,\n",
    "    synthetic_csv_filenames=synthetic_csv_filenames,\n",
    "    CORE_A=CORE_A,\n",
    "    CORE_B=CORE_B,\n",
    "    mute_mode=True,\n",
    "    save_fig=False,\n",
    "    output_figure_filenames=output_figure_filenames,\n",
    "    save_gif=True, \n",
    "    output_gif_filenames=output_gif_filenames,\n",
    "    max_frames=40,\n",
    "    plot_real_data_histogram=False,\n",
    "    plot_age_removal_step_pdf=True,\n",
    "    show_best_datum_match=True,\n",
    "    sequential_mappings_csv=sequential_mappings_csv\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "11-TimeSeriesCorrelation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
