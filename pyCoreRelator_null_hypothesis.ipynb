{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Required Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core A (M9907-25PC) length: 797.0 cm\n",
      "Core B (M9907-23PC) length: 783.0 cm\n",
      "Loading segment pool from available cores...\n",
      "Processing M9907-01PC...\n",
      "  Loaded: 875 points, columns: ['CT']\n",
      "Processing M9907-02TC...\n",
      "  Loaded: 619 points, columns: ['CT']\n",
      "Processing M9907-03PC...\n",
      "  Loaded: 1399 points, columns: ['CT']\n",
      "Processing M9907-05TC...\n",
      "  Loaded: 927 points, columns: ['CT']\n",
      "Processing M9907-06PC...\n",
      "Error loading /Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/_compiled_logs/M9907-06PC/ML_filled/M9907-06PC_CT_MLfilled.csv: [Errno 2] No such file or directory: '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/_compiled_logs/M9907-06PC/ML_filled/M9907-06PC_CT_MLfilled.csv'\n",
      "No log datasets were loaded\n",
      "Warning: Could not load turbidite boundaries for M9907-06PC: attempt to get argmin of an empty sequence\n",
      "  Loaded: 0 points, columns: []\n",
      "Processing M9907-07PC...\n",
      "Error loading /Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/_compiled_logs/M9907-07PC/ML_filled/M9907-07PC_CT_MLfilled.csv: [Errno 2] No such file or directory: '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/_compiled_logs/M9907-07PC/ML_filled/M9907-07PC_CT_MLfilled.csv'\n",
      "No log datasets were loaded\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     78\u001b[39m PICKED_DEPTH_PATHS = {\n\u001b[32m     79\u001b[39m     core_name: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmother_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/pyCoreRelator/pickeddepth/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcore_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_pickeddepth.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m core_name \u001b[38;5;129;01min\u001b[39;00m SEGMENT_POOL_CORES\n\u001b[32m     81\u001b[39m }\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Execute the function using the imported function from pyCoreRelator\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m _, turb_logs, depth_logs, target_dimensions = \u001b[43mload_segment_pool\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcore_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEGMENT_POOL_CORES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcore_log_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCORE_LOG_PATHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpicked_depth_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPICKED_DEPTH_PATHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLOG_COLUMNS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdepth_column\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEPTH_COLUMN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_alternatives\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCOLUMN_ALTERNATIVES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mboundary_category\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneglect_topbottom\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     93\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCore A target length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcore_a_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cm\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCore B target length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcore_b_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cm\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/pyCoreRelator/pyCoreRelator/core/null_hypothesis.py:90\u001b[39m, in \u001b[36mload_segment_pool\u001b[39m\u001b[34m(core_names, core_log_paths, picked_depth_paths, log_columns, depth_column, column_alternatives, boundary_category, neglect_topbottom)\u001b[39m\n\u001b[32m     88\u001b[39m picked_file = picked_depth_paths[core_name]\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     picked_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpicked_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Filter for specified category boundaries only\u001b[39;00m\n\u001b[32m     92\u001b[39m     category_depths = picked_df[picked_df[\u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m] == boundary_category][\u001b[33m'\u001b[39m\u001b[33mpicked_depths_cm\u001b[39m\u001b[33m'\u001b[39m].values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/work/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/work/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/work/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/work/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/work/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 2: Extract Core Lengths and Load Segment Pool\n",
    "from pyCoreRelator import load_segment_pool\n",
    "\n",
    "# Define directory paths\n",
    "mother_dir = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites'\n",
    "\n",
    "# Function to extract core length from depth data\n",
    "def get_core_length(core_name, depth_column='SB_DEPTH_cm', log_column=None):\n",
    "    \"\"\"Extract maximum depth from core data\"\"\"\n",
    "    # Try hiresMS file first (most common)\n",
    "    depth_file = f'{mother_dir}/Cascadia_core_data/OSU_dataset/_compiled_logs/{core_name}/ML_filled/{core_name}_{log_column}_MLfilled.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(depth_file)\n",
    "        return df[depth_column].max()\n",
    "    except:\n",
    "        print(f\"Warning: Could not read depth from {depth_file}\")\n",
    "        return None\n",
    "\n",
    "#####\n",
    "\n",
    "# LOG_COLUMNS = ['hiresMS', 'CT', 'Lumin']\n",
    "# LOG_COLUMNS = ['hiresMS']  # Choose one log column for segment pool\n",
    "LOG_COLUMNS = ['CT']\n",
    "# LOG_COLUMNS = ['Lumin']\n",
    "\n",
    "# Define core names and target parameters\n",
    "CORE_A = \"M9907-25PC\"\n",
    "CORE_B = \"M9907-23PC\"\n",
    "DEPTH_COLUMN = 'SB_DEPTH_cm'\n",
    "\n",
    "# Extract core lengths\n",
    "core_a_length = get_core_length(CORE_A, DEPTH_COLUMN, \"_\".join(LOG_COLUMNS))\n",
    "core_b_length = get_core_length(CORE_B, DEPTH_COLUMN, \"_\".join(LOG_COLUMNS))\n",
    "\n",
    "print(f\"Core A ({CORE_A}) length: {core_a_length} cm\")\n",
    "print(f\"Core B ({CORE_B}) length: {core_b_length} cm\")\n",
    "\n",
    "###\n",
    "# For multidimensional DTW, choose DTW method:\n",
    "pca_for_dependent_dtw=False \n",
    "\n",
    "# Define all cores for segment pool\n",
    "SEGMENT_POOL_CORES = [\n",
    "    \"M9907-01PC\", \"M9907-02TC\", \"M9907-03PC\", \"M9907-05TC\", \"M9907-06PC\", \n",
    "    \"M9907-07PC\", \"M9907-07TC\", \"M9907-08PC\", \"M9907-09PC\", \"M9907-09TC\", \n",
    "    \"M9907-10PC\", \"M9907-11PC\", \"M9907-12PC\", \"M9907-13PC\", \"M9907-14PC\", \n",
    "    \"M9907-14TC\", \"M9907-15PC\", \"M9907-16PC\", \"M9907-17PC\", \"M9907-19PC\", \n",
    "    \"M9907-20PC\", \"M9907-21PC\", \"M9907-22PC\", \"M9907-23PC\", \"M9907-25PC\", \n",
    "    \"M9907-26PC\", \"M9907-27PC\", \"M9907-28PC\", \"M9907-29PC\", \"M9907-30PC\", \n",
    "    \"M9907-31PC\", \"RR0207-01KC\", \"RR0207-02PC\", \"RR0207-55KC\", \"RR0207-56PC\", \n",
    "    \"TN0909-01JC\", \"TN0909-28JC\"\n",
    "]\n",
    "\n",
    "# Define paths and parameters for multiple log types\n",
    "CORE_LOG_PATHS = {\n",
    "    core_name: {\n",
    "        'hiresMS': f'{mother_dir}/Cascadia_core_data/OSU_dataset/_compiled_logs/{core_name}/ML_filled/{core_name}_hiresMS_MLfilled.csv',\n",
    "        'CT': f'{mother_dir}/Cascadia_core_data/OSU_dataset/_compiled_logs/{core_name}/ML_filled/{core_name}_CT_MLfilled.csv',\n",
    "        'Lumin': f'{mother_dir}/Cascadia_core_data/OSU_dataset/_compiled_logs/{core_name}/ML_filled/{core_name}_RGB_MLfilled.csv',\n",
    "        'R': f'{mother_dir}/Cascadia_core_data/OSU_dataset/_compiled_logs/{core_name}/ML_filled/{core_name}_RGB_MLfilled.csv',\n",
    "        'G': f'{mother_dir}/Cascadia_core_data/OSU_dataset/_compiled_logs/{core_name}/ML_filled/{core_name}_RGB_MLfilled.csv',\n",
    "        'B': f'{mother_dir}/Cascadia_core_data/OSU_dataset/_compiled_logs/{core_name}/ML_filled/{core_name}_RGB_MLfilled.csv',\n",
    "        'Den_gm/cc': f'{mother_dir}/Cascadia_core_data/OSU_dataset/_compiled_logs/{core_name}/ML_filled/{core_name}_MST_MLfilled.csv'\n",
    "    }\n",
    "    for core_name in SEGMENT_POOL_CORES\n",
    "}\n",
    "\n",
    "COLUMN_ALTERNATIVES = {\n",
    "    'hiresMS': ['MS'],\n",
    "    'CT': ['CT_value'],\n",
    "    'R': ['R', 'red', 'Red'],\n",
    "    'G': ['G', 'green', 'Green'],\n",
    "    'B': ['B', 'blue', 'Blue'],\n",
    "    'Lumin': ['luminance', 'Luminance'],\n",
    "    'Den_gm/cc': ['Density', 'density']\n",
    "}\n",
    "\n",
    "PICKED_DEPTH_PATHS = {\n",
    "    core_name: f'{mother_dir}/pyCoreRelator/pickeddepth/{core_name}_pickeddepth.csv'\n",
    "    for core_name in SEGMENT_POOL_CORES\n",
    "}\n",
    "\n",
    "# Execute the function using the imported function from pyCoreRelator\n",
    "_, turb_logs, depth_logs, target_dimensions = load_segment_pool(\n",
    "    core_names=SEGMENT_POOL_CORES,\n",
    "    core_log_paths=CORE_LOG_PATHS,\n",
    "    picked_depth_paths=PICKED_DEPTH_PATHS,\n",
    "    log_columns=LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    column_alternatives=COLUMN_ALTERNATIVES,\n",
    "    boundary_category=1,\n",
    "    neglect_topbottom=True\n",
    ")\n",
    "\n",
    "print(f\"Core A target length: {core_a_length} cm\")\n",
    "print(f\"Core B target length: {core_b_length} cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Plot All Turbidite Segments from Pool\n",
    "from pyCoreRelator import plot_segment_pool\n",
    "\n",
    "# Plot the segment pool using imported function\n",
    "fig, axes = plot_segment_pool(\n",
    "    segment_logs=turb_logs,\n",
    "    segment_depths=depth_logs,\n",
    "    log_column_names=LOG_COLUMNS,\n",
    "    n_cols=10,\n",
    "    figsize_per_row=3,\n",
    "    plot_segments=True,\n",
    "    save_plot=False,\n",
    "    plot_filename=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 3.5: Remove unwanted segments from Pool\n",
    "# from pyCoreRelator import modify_segment_pool\n",
    "\n",
    "# # Define the list of segments to remove\n",
    "# # for hiresMS\n",
    "# remove_list = [6, 32]\n",
    "\n",
    "# # Remove segments from the pool\n",
    "# modified_turb_logs, modified_depth_logs = modify_segment_pool(\n",
    "#     turb_logs, depth_logs, remove_list=remove_list\n",
    "# )\n",
    "\n",
    "# # Plot the modified segment pool\n",
    "# fig, axes = plot_segment_pool(\n",
    "#     segment_logs=modified_turb_logs,\n",
    "#     segment_depths=modified_depth_logs,\n",
    "#     log_column_names=LOG_COLUMNS,\n",
    "#     n_cols=10,\n",
    "#     figsize_per_row=3,\n",
    "#     plot_segments=True,\n",
    "#     save_plot=False,\n",
    "#     plot_filename=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_a_length = core_a_length *0.5\n",
    "# core_b_length = core_b_length *0.5\n",
    "core_a_length = 600\n",
    "core_b_length = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create and Plot Synthetic Core Pair\n",
    "from pyCoreRelator import create_and_plot_synthetic_core_pair\n",
    "\n",
    "# Generate and plot synthetic core pair using imported functions\n",
    "(synthetic_log_a, synthetic_md_a, inds_a, synthetic_picked_a,\n",
    " synthetic_log_b, synthetic_md_b, inds_b, synthetic_picked_b) = create_and_plot_synthetic_core_pair(\n",
    "    core_a_length, core_b_length, turb_logs, depth_logs, LOG_COLUMNS,\n",
    "    repetition=False, # True: allow reselecting turbidite segments; False: each segment can only be selected once\n",
    "    plot_results=True, \n",
    "    save_plot=False, \n",
    "    plot_filename='synthetic_core_pair.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: DTW Analysis on Synthetic Pair\n",
    "from pyCoreRelator import run_comprehensive_dtw_analysis, find_complete_core_paths\n",
    "\n",
    "# Run DTW analysis\n",
    "dtw_results, valid_dtw_pairs, segments_a, segments_b, _, _, dtw_distance_matrix_full = run_comprehensive_dtw_analysis(\n",
    "    synthetic_log_a, synthetic_log_b, synthetic_md_a, synthetic_md_b,\n",
    "    picked_depths_a=synthetic_picked_a,\n",
    "    picked_depths_b=synthetic_picked_b,\n",
    "    independent_dtw=False,\n",
    "    pca_for_dependent_dtw=pca_for_dependent_dtw,\n",
    "    top_bottom=False,\n",
    "    mute_mode=False\n",
    ")\n",
    "\n",
    "# Find complete core paths and extract r-values\n",
    "_ = find_complete_core_paths(\n",
    "    valid_dtw_pairs,\n",
    "    segments_a, \n",
    "    segments_b, \n",
    "    synthetic_log_a, \n",
    "    synthetic_log_b,\n",
    "    synthetic_picked_a, \n",
    "    synthetic_picked_b,\n",
    "    dtw_results,\n",
    "    dtw_distance_matrix_full,\n",
    "    output_csv=\"outputs/temp_synthetic_core_pair_metrics.csv\",\n",
    "    output_metric_only=True,\n",
    "    shortest_path_search=True,\n",
    "    shortest_path_level=2,\n",
    "    max_search_path=100000,\n",
    "    mute_mode=False,\n",
    "    pca_for_dependent_dtw=pca_for_dependent_dtw\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Plot R-Values Distribution from Synthetic Pair\n",
    "from pyCoreRelator import plot_correlation_distribution\n",
    "\n",
    "# Define quality index and parameters (norm_dtw, dtw_ratio, perc_diag, corr_coef, dtw_warp_eff, perc_age_overlap)\n",
    "targeted_quality_index = 'corr_coef' \n",
    "# targeted_quality_index = 'norm_dtw'\n",
    "# targeted_quality_index = 'dtw_ratio'\n",
    "# targeted_quality_index = 'dtw_warp_eff' \n",
    "# targeted_quality_index = 'perc_diag'\n",
    "\n",
    "# Define CSV filename\n",
    "csv_filename = 'outputs/temp_synthetic_core_pair_metrics.csv'\n",
    "\n",
    "# Plot correlation distribution\n",
    "_, _, fit_params = plot_correlation_distribution(\n",
    "    csv_file=csv_filename,\n",
    "    quality_index=targeted_quality_index,\n",
    "    no_bins=30,\n",
    "    save_png=False,\n",
    "    pdf_method='normal',  # 'KDE', 'skew-normal', 'normal'\n",
    "    kde_bandwidth=0.05,\n",
    "    mute_mode=False\n",
    ")\n",
    "\n",
    "# Remove temporary CSV file after loop is complete\n",
    "if os.path.exists(\"outputs/temp_synthetic_core_pair_metrics.csv\"):\n",
    "    os.remove(\"outputs/temp_synthetic_core_pair_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Re-plot Distribution Using Fit Parameters\n",
    "if 'fit_params' in locals() and fit_params is not None:\n",
    "    print(\"Re-plotting fitted curve only from 'fit_params'...\")\n",
    "    \n",
    "    # Create new figure\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    # Plot only the fitted distribution curve\n",
    "    if 'x_range' in fit_params and 'y_values' in fit_params:\n",
    "        x = fit_params['x_range']\n",
    "        y = fit_params['y_values']\n",
    "        \n",
    "        method = fit_params.get('method', 'unknown')\n",
    "        \n",
    "        if method == 'normal':\n",
    "            mean_val = fit_params['mean']\n",
    "            std_val = fit_params['std']\n",
    "            n_points = fit_params['n_points']\n",
    "            ax.plot(x, y, 'r-', linewidth=2, alpha=0.8,\n",
    "                    label=f'Normal Fit\\n(mean = {mean_val:.3f})\\n(σ = {std_val:.3f})\\nn = {n_points:,}')\n",
    "                      \n",
    "        elif method == 'skew-normal':\n",
    "            shape = fit_params['shape']\n",
    "            location = fit_params['location']\n",
    "            scale = fit_params['scale']\n",
    "            n_points = fit_params['n_points']\n",
    "            ax.plot(x, y, 'r-', linewidth=2, alpha=0.8,\n",
    "                    label=f'Skew-Normal Fit\\n(α = {shape:.3f})\\n(μ = {location:.3f})\\n(σ = {scale:.3f})\\nn = {n_points:,}')\n",
    "                      \n",
    "        elif method == 'KDE':\n",
    "            bandwidth = fit_params['bandwidth']\n",
    "            n_points = fit_params['n_points']\n",
    "            ax.plot(x, y, 'r-', linewidth=2, alpha=0.8,\n",
    "                    label=f'KDE\\n(bandwidth = {bandwidth})\\nn = {n_points:,}')\n",
    "    \n",
    "    # Add median line\n",
    "    if 'median' in fit_params:\n",
    "        median_val = fit_params['median']\n",
    "        ax.axvline(median_val, color='green', linestyle='dashed', linewidth=2,\n",
    "                  label=f'Median: {median_val:.3f}')\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel(f'{targeted_quality_index}')\n",
    "    ax.set_ylabel('Density (%)')\n",
    "    ax.set_title(f'Fitted Distribution Curve\\nSynthetic Cores {CORE_A} vs {CORE_B}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Set appropriate x-axis limits\n",
    "    if targeted_quality_index == 'corr_coef':\n",
    "        ax.set_xlim(0, 1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "                \n",
    "else:\n",
    "    print(\"No fit_params available. Please run Cell 6 first to generate the distribution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Run 100 Iterations for Synthetic Logs R-Value Findings\n",
    "from pyCoreRelator import run_comprehensive_dtw_analysis, find_complete_core_paths, plot_correlation_distribution, create_synthetic_log_with_depths\n",
    "\n",
    "# Define quality indices to iterate through (norm_dtw, dtw_ratio, perc_diag, corr_coef, dtw_warp_eff, perc_age_overlap)\n",
    "# quality_indices = ['corr_coef', 'norm_dtw', 'dtw_ratio', 'perc_diag', 'dtw_warp_eff', 'perc_age_overlap']\n",
    "quality_indices = ['corr_coef','norm_dtw']\n",
    "\n",
    "# Define number of iterations\n",
    "number_of_iterations = 200\n",
    "\n",
    "# Run iterations with progress bar\n",
    "for iteration in tqdm(range(number_of_iterations), desc=f\"Running synthetic analysis\"):\n",
    "    \n",
    "    # Generate synthetic core pair (from cell 4)\n",
    "    synthetic_log_a, synthetic_md_a, inds_a, synthetic_picked_a_tuples = create_synthetic_log_with_depths(\n",
    "        core_a_length, turb_logs, depth_logs, exclude_inds=None, \n",
    "        repetition=False # True: allow reselecting turbidite segments; False: each segment can only be selected once\n",
    "    )\n",
    "    synthetic_log_b, synthetic_md_b, inds_b, synthetic_picked_b_tuples = create_synthetic_log_with_depths(\n",
    "        core_b_length, turb_logs, depth_logs, exclude_inds=None, \n",
    "        repetition=False # True: allow reselecting turbidite segments; False: each segment can only be selected once\n",
    "    )\n",
    "    \n",
    "    # Extract depths from tuples\n",
    "    synthetic_picked_a = [depth for depth, category in synthetic_picked_a_tuples]\n",
    "    synthetic_picked_b = [depth for depth, category in synthetic_picked_b_tuples]\n",
    "    \n",
    "    # Run DTW analysis (from cell 5)\n",
    "    dtw_results, valid_dtw_pairs, segments_a, segments_b, _, _, dtw_distance_matrix_full = run_comprehensive_dtw_analysis(\n",
    "        synthetic_log_a, synthetic_log_b, synthetic_md_a, synthetic_md_b,\n",
    "        picked_depths_a=synthetic_picked_a,\n",
    "        picked_depths_b=synthetic_picked_b,\n",
    "        independent_dtw=False,\n",
    "        pca_for_dependent_dtw=pca_for_dependent_dtw,\n",
    "        top_bottom=False,\n",
    "        mute_mode=True\n",
    "    )\n",
    "    \n",
    "    # Find complete core paths\n",
    "    _ = find_complete_core_paths(\n",
    "        valid_dtw_pairs,\n",
    "        segments_a, \n",
    "        segments_b, \n",
    "        synthetic_log_a, \n",
    "        synthetic_log_b,\n",
    "        synthetic_picked_a, \n",
    "        synthetic_picked_b,\n",
    "        dtw_results,\n",
    "        dtw_distance_matrix_full,\n",
    "        output_csv=\"outputs/temp_synthetic_core_pair_metrics.csv\",\n",
    "        output_metric_only=True,\n",
    "        shortest_path_search=True,\n",
    "        shortest_path_level=2,\n",
    "        max_search_path=100000,\n",
    "        mute_mode=True,\n",
    "        pca_for_dependent_dtw=pca_for_dependent_dtw\n",
    "    )\n",
    "    \n",
    "    # Iterate through each quality index to extract fit_params\n",
    "    for targeted_quality_index in quality_indices:\n",
    "        \n",
    "        # Define output filename based on log columns and quality index        \n",
    "        output_csv_filename = f'outputs/synthetic_PDFs_{\"_\".join(LOG_COLUMNS)}_{targeted_quality_index}.csv'\n",
    "\n",
    "        # Plot correlation distribution to get fit_params only\n",
    "        _, _, fit_params = plot_correlation_distribution(\n",
    "            csv_file=\"outputs/temp_synthetic_core_pair_metrics.csv\",\n",
    "            quality_index=targeted_quality_index,\n",
    "            no_bins=30,\n",
    "            save_png=False,\n",
    "            pdf_method='normal',\n",
    "            kde_bandwidth=0.05,\n",
    "            mute_mode=True\n",
    "        )\n",
    "        \n",
    "        # Store fit_params with iteration number and incrementally save to CSV\n",
    "        if fit_params is not None:\n",
    "            fit_params_copy = fit_params.copy()\n",
    "            fit_params_copy['iteration'] = iteration\n",
    "            \n",
    "            # Incrementally save to CSV\n",
    "            df_single = pd.DataFrame([fit_params_copy])\n",
    "            if iteration == 0:\n",
    "                # Write header for first iteration\n",
    "                df_single.to_csv(output_csv_filename, mode='w', index=False, header=True)\n",
    "            else:\n",
    "                # Append subsequent iterations without header\n",
    "                df_single.to_csv(output_csv_filename, mode='a', index=False, header=False)\n",
    "            \n",
    "            del df_single, fit_params_copy\n",
    "        \n",
    "        del fit_params\n",
    "    \n",
    "    # Clear memory after each iteration\n",
    "    del synthetic_log_a, synthetic_md_a, inds_a, synthetic_picked_a_tuples\n",
    "    del synthetic_log_b, synthetic_md_b, inds_b, synthetic_picked_b_tuples\n",
    "    del synthetic_picked_a, synthetic_picked_b\n",
    "    del dtw_results, valid_dtw_pairs, segments_a, segments_b\n",
    "    \n",
    "    # Force garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "# Remove temporary CSV file after all iterations are complete\n",
    "if os.path.exists(\"outputs/temp_synthetic_core_pair_metrics.csv\"):\n",
    "    os.remove(\"outputs/temp_synthetic_core_pair_metrics.csv\")\n",
    "\n",
    "print(f\"\\nCompleted {number_of_iterations} iterations for all quality indices: {quality_indices}\")\n",
    "\n",
    "for targeted_quality_index in quality_indices:\n",
    "    output_csv_filename = f'outputs/synthetic_PDFs_{\"_\".join(LOG_COLUMNS)}_{targeted_quality_index}.csv'\n",
    "    print(f\"Distribution curves parameters for {targeted_quality_index} saved to: {output_csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Plot all distribution curves for each quality index\n",
    "\n",
    "# Define quality indices to plot\n",
    "# quality_indices = ['corr_coef', 'norm_dtw', 'dtw_ratio', 'perc_diag', 'dtw_warp_eff', 'perc_age_overlap']\n",
    "quality_indices = ['corr_coef', 'norm_dtw']\n",
    "\n",
    "for targeted_quality_index in quality_indices:\n",
    "    print(f\"\\nPlotting distribution curves for {targeted_quality_index}...\")\n",
    "\n",
    "    # Define input filename based on log columns and quality index\n",
    "    input_csv_filename = f'outputs/synthetic_PDFs_{\"_\".join(LOG_COLUMNS)}_{targeted_quality_index}.csv'\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(input_csv_filename):\n",
    "        print(f\"Error: File {input_csv_filename} does not exist. Skipping {targeted_quality_index}.\")\n",
    "        continue\n",
    "    \n",
    "    # Load fit params from CSV\n",
    "    df_fit_params = pd.read_csv(input_csv_filename)\n",
    "\n",
    "    # Convert to list of dictionaries containing only necessary columns\n",
    "    all_fit_params = []\n",
    "    mean_values = []\n",
    "    for _, row in df_fit_params.iterrows():\n",
    "        fit_params = {\n",
    "            'x_range': np.fromstring(row['x_range'].strip('[]'), sep=' ') if 'x_range' in row and pd.notna(row['x_range']) else None,\n",
    "            'y_values': np.fromstring(row['y_values'].strip('[]'), sep=' ') if 'y_values' in row and pd.notna(row['y_values']) else None\n",
    "        }\n",
    "        all_fit_params.append(fit_params)\n",
    "        \n",
    "        # Extract mean value if available\n",
    "        if 'mean' in row and pd.notna(row['mean']):\n",
    "            mean_values.append(row['mean'])\n",
    "\n",
    "    # Plot all distribution curves\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "    # Plot histogram of mean values in gray bars\n",
    "    if mean_values:\n",
    "        ax.hist(mean_values, bins=20, alpha=0.5, color='gray', density=True, label='Mean Values')\n",
    "\n",
    "    # Plot all curves as transparent red lines\n",
    "    for fit_params in all_fit_params:\n",
    "        if 'x_range' in fit_params and 'y_values' in fit_params:\n",
    "            x = fit_params['x_range']\n",
    "            y = fit_params['y_values']\n",
    "            if x is not None and y is not None:\n",
    "                ax.plot(x, y, 'r-', linewidth=.7, alpha=0.3)\n",
    "\n",
    "    # Formatting based on quality index\n",
    "    if targeted_quality_index == 'corr_coef':\n",
    "        ax.set_xlabel(\"Pearson's r\\n(Correlation Coefficient)\")\n",
    "        ax.set_xlim(0, 1.0)\n",
    "    elif targeted_quality_index == 'norm_dtw':\n",
    "        ax.set_xlabel(\"Normalized DTW Distance\")\n",
    "    elif targeted_quality_index == 'dtw_ratio':\n",
    "        ax.set_xlabel(\"DTW Ratio\")\n",
    "    elif targeted_quality_index == 'perc_diag':\n",
    "        ax.set_xlabel(\"Percentage Diagonal (%)\")\n",
    "    \n",
    "    ax.set_ylabel('Probability Density (%)')\n",
    "    ax.set_title(f'Synthetic Core {targeted_quality_index.replace(\"_\", \" \").title()}: {len(all_fit_params)} Iterations\\n[Optimal (shortest path) search; no age consideration)]')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if mean_values:\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # plt.savefig(f'outputs/synthetic_iterations_{targeted_quality_index}.png', dpi=150, bbox_inches='tight')\n",
    "    # print(f\"Saved plot for {targeted_quality_index} to outputs/synthetic_iterations_{targeted_quality_index}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Combine all binned data and recalculate distribution for each quality index\n",
    "\n",
    "# Define quality indices to iterate through (matching Cell 8)\n",
    "# quality_indices = ['corr_coef', 'norm_dtw', 'dtw_ratio', 'perc_diag', 'dtw_warp_eff', 'perc_age_overlap']\n",
    "quality_indices = ['corr_coef', 'norm_dtw']\n",
    "\n",
    "# Iterate through each quality index\n",
    "for targeted_quality_index in quality_indices:\n",
    "    print(f\"\\nProcessing combined distribution for {targeted_quality_index}...\")\n",
    "    \n",
    "    # Define input filename based on log columns and quality index \n",
    "    output_csv_filename = f'outputs/synthetic_PDFs_{\"_\".join(LOG_COLUMNS)}_{targeted_quality_index}.csv'\n",
    "    \n",
    "    # Check if the file exists\n",
    "    import os\n",
    "    if not os.path.exists(output_csv_filename):\n",
    "        print(f\"Error: File {output_csv_filename} does not exist. Skipping {targeted_quality_index}.\")\n",
    "        continue\n",
    "    \n",
    "    # Load fit params from CSV\n",
    "    df_fit_params = pd.read_csv(output_csv_filename)\n",
    "\n",
    "    # Initialize lists to collect all raw data points\n",
    "    all_raw_data = []\n",
    "\n",
    "    # Process each iteration to reconstruct raw data from binned data\n",
    "    for _, row in df_fit_params.iterrows():\n",
    "        # Extract binned data\n",
    "        bins = np.fromstring(row['bins'].strip('[]'), sep=' ') if 'bins' in row and pd.notna(row['bins']) else None\n",
    "        hist_percentages = np.fromstring(row['hist'].strip('[]'), sep=' ') if 'hist' in row and pd.notna(row['hist']) else None\n",
    "        n_points = row['n_points'] if 'n_points' in row and pd.notna(row['n_points']) else None\n",
    "        \n",
    "        if bins is not None and hist_percentages is not None and n_points is not None:\n",
    "            # IMPORTANT FIX: The hist values in CSV are not actually percentages but \n",
    "            # raw histogram counts from the old buggy normalization code.\n",
    "            # We need to properly normalize them to reconstruct the correct data.\n",
    "            \n",
    "            # First, normalize the histogram values so they represent proper proportions\n",
    "            hist_sum = np.sum(hist_percentages)\n",
    "            if hist_sum > 0:\n",
    "                # Normalize to get proper proportions, then scale by n_points to get counts\n",
    "                raw_counts = (hist_percentages / hist_sum) * n_points\n",
    "            else:\n",
    "                raw_counts = np.zeros_like(hist_percentages)\n",
    "            \n",
    "            # Reconstruct data points by sampling from each bin\n",
    "            bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "            bin_width = bins[1] - bins[0]\n",
    "            \n",
    "            for i, count in enumerate(raw_counts):\n",
    "                if count > 0:\n",
    "                    # Generate random points within each bin\n",
    "                    n_samples = int(round(count))\n",
    "                    if n_samples > 0:\n",
    "                        # Sample uniformly within the bin\n",
    "                        bin_samples = np.random.uniform(\n",
    "                            bins[i], bins[i+1], n_samples\n",
    "                        )\n",
    "                        all_raw_data.extend(bin_samples)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    combined_data = np.array(all_raw_data)\n",
    "\n",
    "    print(f\"Combined {len(combined_data)} data points from {len(df_fit_params)} iterations\")\n",
    "    \n",
    "    # Verify histogram normalization\n",
    "    total_percentage = np.sum(hist_percentages)\n",
    "    print(f\"Histogram total percentage: {total_percentage:.2f}% (should be 100%)\")\n",
    "    \n",
    "    # Verify PDF curve integration (approximate using trapezoidal rule)\n",
    "    dx = x_fitted[1] - x_fitted[0]\n",
    "    pdf_area = np.trapz(y_fitted, dx=dx)\n",
    "    print(f\"PDF curve area: {pdf_area:.2f}% (should be ~100%)\")\n",
    "\n",
    "    # Calculate combined statistics\n",
    "    combined_mean = np.mean(combined_data)\n",
    "    combined_std = np.std(combined_data)\n",
    "    combined_median = np.median(combined_data)\n",
    "\n",
    "    # Create new histogram from combined data as PERCENTAGES (not density)\n",
    "    n_bins = 30  # You can adjust this\n",
    "    hist_combined, bins_combined = np.histogram(combined_data, bins=n_bins, density=False)\n",
    "    # Convert counts to percentages\n",
    "    hist_percentages = (hist_combined / len(combined_data)) * 100\n",
    "    bin_width = bins_combined[1] - bins_combined[0]\n",
    "\n",
    "    # Fit normal distribution to combined data\n",
    "    from scipy import stats\n",
    "    fitted_mean, fitted_std = stats.norm.fit(combined_data)\n",
    "\n",
    "    # Generate fitted curve and scale to percentage (must match histogram scaling)\n",
    "    x_fitted = np.linspace(combined_data.min(), combined_data.max(), 1000)\n",
    "    # PDF must be scaled by: bin_width * 100 (to convert density to percentage per bin)\n",
    "    y_fitted = stats.norm.pdf(x_fitted, fitted_mean, fitted_std) * bin_width * 100\n",
    "\n",
    "    # Create the plot (same style as Cell 9)\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "    # Plot combined histogram in gray bars as PERCENTAGES\n",
    "    ax.hist(combined_data, bins=n_bins, alpha=0.5, color='gray', density=False, \n",
    "            weights=np.ones(len(combined_data)) * 100 / len(combined_data), label='Combined Data')\n",
    "\n",
    "    # Plot fitted normal curve as red line (scaled to percentage)\n",
    "    ax.plot(x_fitted, y_fitted, 'r-', linewidth=2, alpha=0.8,\n",
    "            label=f'Normal Fit (μ={fitted_mean:.3f}, σ={fitted_std:.3f})')\n",
    "\n",
    "    # Formatting based on quality index\n",
    "    if targeted_quality_index == 'corr_coef':\n",
    "        ax.set_xlabel(\"Pearson's r\\n(Correlation Coefficient)\")\n",
    "        ax.set_xlim(0, 1.0)\n",
    "    elif targeted_quality_index == 'norm_dtw':\n",
    "        ax.set_xlabel(\"Normalized DTW Distance\")\n",
    "    elif targeted_quality_index == 'dtw_ratio':\n",
    "        ax.set_xlabel(\"DTW Ratio\")\n",
    "    elif targeted_quality_index == 'perc_diag':\n",
    "        ax.set_xlabel(\"Percentage Diagonal (%)\")\n",
    "    \n",
    "    ax.set_ylabel('Percentage (%)')\n",
    "    ax.set_title(f'Combined {targeted_quality_index.replace(\"_\", \" \").title()} Distribution from All {len(df_fit_params)} Iterations\\n[Synthetic Core Analysis - Null Hypothesis]')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_filename = f'outputs/combined_synthetic_distribution_{\"_\".join(LOG_COLUMNS)}_{targeted_quality_index}.png'\n",
    "\n",
    "    plt.savefig(output_filename, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Print comprehensive summary\n",
    "    print(f\"\\nCombined Distribution Summary for {targeted_quality_index}:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total data points: {len(combined_data):,}\")\n",
    "    print(f\"Number of iterations combined: {len(df_fit_params)}\")\n",
    "    print(f\"Combined Mean: {combined_mean:.4f}\")\n",
    "    print(f\"Combined Median: {combined_median:.4f}\")\n",
    "    print(f\"Combined Std Dev: {combined_std:.4f}\")\n",
    "    print(f\"Data Range: {combined_data.min():.4f} to {combined_data.max():.4f}\")\n",
    "    print(f\"\\nFitted Normal Distribution:\")\n",
    "    print(f\"Fitted Mean (μ): {fitted_mean:.4f}\")\n",
    "    print(f\"Fitted Std Dev (σ): {fitted_std:.4f}\")\n",
    "\n",
    "    # Calculate some percentiles for reference\n",
    "    percentiles = [5, 25, 50, 75, 95]\n",
    "    pct_values = np.percentile(combined_data, percentiles)\n",
    "    print(f\"\\nPercentiles:\")\n",
    "    for pct, val in zip(percentiles, pct_values):\n",
    "        print(f\"{pct}th percentile: {val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
