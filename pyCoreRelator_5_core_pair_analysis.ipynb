{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25kTe2nS84Tf"
   },
   "source": [
    "# Correlating well log pairs: Complex Dynamic Time Warping with boundary constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4awuK6ffO2t"
   },
   "source": [
    "## Introduction to dynamic time warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47Kk07X_84Th"
   },
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from IPython.display import Image as IPImage, display\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyCoreRelator import (\n",
    "    run_comprehensive_dtw_analysis,\n",
    "    find_complete_core_paths,\n",
    "    diagnose_chain_breaks,\n",
    "    calculate_interpolated_ages,\n",
    "    load_pickeddepth_ages_from_csv,\n",
    "    visualize_combined_segments,\n",
    "    visualize_dtw_results_from_csv,\n",
    "    load_log_data,\n",
    "    plot_core_data,\n",
    "    plot_dtw_matrix_with_paths,\n",
    "    plot_correlation_distribution,\n",
    "    find_best_mappings,\n",
    "    load_core_age_constraints\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Test with Cascadia hi-res MS logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define core pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORE_A = \"M9907-25PC\"\n",
    "# CORE_A = \"M9907-23PC\"\n",
    "CORE_A = \"M9907-11PC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_B = \"M9907-23PC\"\n",
    "# CORE_B = \"M9907-11PC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data structures and core images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum data points to extract (set to None to use all data)\n",
    "MAX_DATA_POINTS = None  # Set to a number like 1000 to limit data points, or None for all data\n",
    "# MAX_DATA_POINTS = 1000  # Set to a number like 1000 to limit data points, or None for all data\n",
    "\n",
    "# Define log columns to extract\n",
    "LOG_COLUMNS = ['hiresMS', 'CT', 'Lumin']  # Choose which logs to include\n",
    "# LOG_COLUMNS = ['hiresMS']  # Choose which logs to include\n",
    "# LOG_COLUMNS = ['hiresMS', 'CT']  # Choose which logs to include\n",
    "# LOG_COLUMNS = ['CT']  # Choose which logs to include\n",
    "# LOG_COLUMNS = ['Lumin']  # Choose which logs to include\n",
    "DEPTH_COLUMN = 'SB_DEPTH_cm'\n",
    "\n",
    "# Define directory paths\n",
    "mother_dir = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/'\n",
    "\n",
    "# Define paths for Core A\n",
    "core_a_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_MST_MLfilled.csv'\n",
    "}\n",
    "\n",
    "core_a_rgb_img_path = f\"{mother_dir}_compiled_logs/{CORE_A}/{CORE_A}_RGB.tiff\"\n",
    "core_a_ct_img_path = f\"{mother_dir}_compiled_logs/{CORE_A}/{CORE_A}_CT.tiff\"\n",
    "\n",
    "# Define paths for Core B\n",
    "core_b_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_MST_MLfilled.csv'\n",
    "}\n",
    "core_b_rgb_img_path = f\"{mother_dir}_compiled_logs/{CORE_B}/{CORE_B}_RGB.tiff\"\n",
    "core_b_ct_img_path = f\"{mother_dir}_compiled_logs/{CORE_B}/{CORE_B}_CT.tiff\"\n",
    "\n",
    "# Define column mapping for alternative column names\n",
    "column_alternatives = {\n",
    "    'hiresMS': ['MS'],\n",
    "    'CT': ['CT_value'],\n",
    "    'R': ['R', 'red', 'Red'],\n",
    "    'G': ['G', 'green', 'Green'],\n",
    "    'B': ['B', 'blue', 'Blue'],\n",
    "    'Lumin': ['luminance', 'Luminance'],\n",
    "    'Den_gm/cc': ['Density', 'density']\n",
    "}\n",
    "\n",
    "# Load data for Core A\n",
    "log_a, md_a, available_columns_a, rgb_img_a, ct_img_a = load_log_data(\n",
    "    core_a_log_paths,\n",
    "    {'rgb': core_a_rgb_img_path, 'ct': core_a_ct_img_path},\n",
    "    LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")\n",
    "\n",
    "print(\"\\n=== DEBUG: Core A Loading ===\")\n",
    "print(f\"LOG_COLUMNS requested: {LOG_COLUMNS}\")\n",
    "print(f\"Available columns loaded: {available_columns_a}\")\n",
    "print(f\"Shape of log_a: {log_a.shape}\")\n",
    "print(f\"Type of log_a: {type(log_a)}\")\n",
    "if hasattr(log_a, 'ndim'):\n",
    "    print(f\"log_a dimensions: {log_a.ndim}\")\n",
    "    if log_a.ndim > 1:\n",
    "        print(f\"log_a has {log_a.shape[1]} columns\\n\")\n",
    "    else:\n",
    "        print(\"log_a is 1D (single column)\\n\")\n",
    "\n",
    "# Load data for Core B\n",
    "log_b, md_b, available_columns_b, rgb_img_b, ct_img_b = load_log_data(\n",
    "    core_b_log_paths,\n",
    "    {'rgb': core_b_rgb_img_path, 'ct': core_b_ct_img_path},\n",
    "    LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")\n",
    "\n",
    "print(\"\\n=== DEBUG: Core B Loading ===\")\n",
    "print(f\"LOG_COLUMNS requested: {LOG_COLUMNS}\")\n",
    "print(f\"Available columns loaded: {available_columns_b}\")\n",
    "print(f\"Shape of log_b: {log_b.shape}\")\n",
    "print(f\"Type of log_b: {type(log_b)}\")\n",
    "if hasattr(log_b, 'ndim'):\n",
    "    print(f\"log_b dimensions: {log_b.ndim}\")\n",
    "    if log_b.ndim > 1:\n",
    "        print(f\"log_b has {log_b.shape[1]} columns\\n\")\n",
    "    else:\n",
    "        print(\"log_b is 1D (single column)\\n\")\n",
    "\n",
    "# Extract first N data points if MAX_DATA_POINTS is specified\n",
    "if MAX_DATA_POINTS is not None:\n",
    "    print(f\"\\n=== Extracting first {MAX_DATA_POINTS} data points ===\")\n",
    "    \n",
    "    # Extract for Core A\n",
    "    if len(log_a) > MAX_DATA_POINTS:\n",
    "        log_a = log_a[:MAX_DATA_POINTS]\n",
    "        md_a = md_a[:MAX_DATA_POINTS]\n",
    "        print(f\"Core A: Extracted first {MAX_DATA_POINTS} points from {len(log_a)} total points\")\n",
    "        \n",
    "        # Normalize the extracted data to 0-1 range\n",
    "        if log_a.ndim > 1:\n",
    "            # Multi-column data\n",
    "            for col in range(log_a.shape[1]):\n",
    "                col_min = log_a[:, col].min()\n",
    "                col_max = log_a[:, col].max()\n",
    "                if col_max > col_min:  # Avoid division by zero\n",
    "                    log_a[:, col] = (log_a[:, col] - col_min) / (col_max - col_min)\n",
    "        else:\n",
    "            # Single column data\n",
    "            col_min = log_a.min()\n",
    "            col_max = log_a.max()\n",
    "            if col_max > col_min:  # Avoid division by zero\n",
    "                log_a = (log_a - col_min) / (col_max - col_min)\n",
    "        print(f\"Core A: Normalized extracted data to 0-1 range\")\n",
    "    else:\n",
    "        print(f\"Core A: Using all {len(log_a)} available points (less than {MAX_DATA_POINTS})\")\n",
    "\n",
    "    # Extract for Core B\n",
    "    if len(log_b) > MAX_DATA_POINTS:\n",
    "        log_b = log_b[:MAX_DATA_POINTS]\n",
    "        md_b = md_b[:MAX_DATA_POINTS]\n",
    "        print(f\"Core B: Extracted first {MAX_DATA_POINTS} points from {len(log_b)} total points\")\n",
    "        \n",
    "        # Normalize the extracted data to 0-1 range\n",
    "        if log_b.ndim > 1:\n",
    "            # Multi-column data\n",
    "            for col in range(log_b.shape[1]):\n",
    "                col_min = log_b[:, col].min()\n",
    "                col_max = log_b[:, col].max()\n",
    "                if col_max > col_min:  # Avoid division by zero\n",
    "                    log_b[:, col] = (log_b[:, col] - col_min) / (col_max - col_min)\n",
    "        else:\n",
    "            # Single column data\n",
    "            col_min = log_b.min()\n",
    "            col_max = log_b.max()\n",
    "            if col_max > col_min:  # Avoid division by zero\n",
    "                log_b = (log_b - col_min) / (col_max - col_min)\n",
    "        print(f\"Core B: Normalized extracted data to 0-1 range\")\n",
    "    else:\n",
    "        print(f\"Core B: Using all {len(log_b)} available points (less than {MAX_DATA_POINTS})\")\n",
    "\n",
    "    print(f\"Final shapes - Core A: {log_a.shape}, Core B: {log_b.shape}\")\n",
    "else:\n",
    "    print(f\"\\n=== Using all available data points ===\")\n",
    "    print(f\"Core A: {log_a.shape}, Core B: {log_b.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract and show each category of picked depths ###\n",
    "%matplotlib inline\n",
    "\n",
    "# Define paths to the CSV files of picked depths\n",
    "pickeddepth_a_csv = f'pickeddepth/{CORE_A}_pickeddepth.csv'\n",
    "pickeddepth_b_csv = f'pickeddepth/{CORE_B}_pickeddepth.csv'\n",
    "\n",
    "# Load picked depths and categories directly\n",
    "if os.path.exists(pickeddepth_a_csv):\n",
    "    picked_data_a = pd.read_csv(pickeddepth_a_csv)\n",
    "    picked_depths_a = picked_data_a['picked_depths_cm'].values.tolist()\n",
    "    picked_categories_a = picked_data_a['category'].values.tolist()\n",
    "    print(f\"Loaded {len(picked_depths_a)} picked depths for {CORE_A}\")\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_a_csv} not found. Using empty arrays for Core A.\")\n",
    "    picked_depths_a = []\n",
    "    picked_categories_a = []\n",
    "\n",
    "if os.path.exists(pickeddepth_b_csv):\n",
    "    picked_data_b = pd.read_csv(pickeddepth_b_csv)\n",
    "    picked_depths_b = picked_data_b['picked_depths_cm'].values.tolist()\n",
    "    picked_categories_b = picked_data_b['category'].values.tolist()\n",
    "    print(f\"Loaded {len(picked_depths_b)} picked depths for {CORE_B}\")\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_b_csv} not found. Using empty arrays for Core B.\")\n",
    "    picked_depths_b = []\n",
    "    picked_categories_b = []\n",
    "\n",
    "# Create uncertainty arrays (assuming uncertainty size is 1 cm)\n",
    "picked_uncertainty_a = [1] * len(picked_depths_a)\n",
    "picked_uncertainty_b = [1] * len(picked_depths_b)\n",
    "\n",
    "# Now plot the cores with enhanced plot_core_data function\n",
    "is_multilog = log_a.ndim > 1 and log_a.shape[1] > 1\n",
    "fig_a, ax_a = plot_core_data(\n",
    "    # Input data\n",
    "    md_a,                                           # depth array\n",
    "    log_a,                                          # log data array\n",
    "    f\"{CORE_A}\",                                    # core name\n",
    "    # Image data\n",
    "    core_img_1=rgb_img_a,                           # RGB image array\n",
    "    core_img_2=ct_img_a,                            # CT scan image array\n",
    "    # Data configuration\n",
    "    available_columns=available_columns_a,          # available data columns\n",
    "    is_multilog=is_multilog,                        # multi-column log flag\n",
    "    # Picked depth data\n",
    "    picked_depths=picked_depths_a,                  # picked depth values\n",
    "    picked_categories=picked_categories_a,          # picked categories\n",
    "    picked_uncertainties=picked_uncertainty_a,      # uncertainty values\n",
    "    # Visualization settings\n",
    "    figsize=(20, 4),                                # figure size (width, height)\n",
    "    # show_category=[1],                              # categories to display\n",
    "    show_bed_number=False                            # show bed numbers flag\n",
    ")\n",
    "\n",
    "# Do the same for Core B\n",
    "is_multilog = log_b.ndim > 1 and log_b.shape[1] > 1\n",
    "fig_b, ax_b = plot_core_data(\n",
    "    # Input data\n",
    "    md_b,                                           # depth array\n",
    "    log_b,                                          # log data array\n",
    "    f\"{CORE_B}\",                                    # core name\n",
    "    # Image data\n",
    "    core_img_1=rgb_img_b,                           # RGB image array\n",
    "    core_img_2=ct_img_b,                            # CT scan image array\n",
    "    # Data configuration\n",
    "    available_columns=available_columns_b,          # available data columns\n",
    "    is_multilog=is_multilog,                        # multi-column log flag\n",
    "    # Picked depth data\n",
    "    picked_depths=picked_depths_b,                  # picked depth values\n",
    "    picked_categories=picked_categories_b,          # picked categories\n",
    "    picked_uncertainties=picked_uncertainty_b,      # uncertainty values\n",
    "    # Visualization settings\n",
    "    figsize=(20, 4),                                # figure size (width, height)\n",
    "    # show_category=[1],                              # categories to display\n",
    "    show_bed_number=False                            # show bed numbers flag\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Usage Examples and Executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Define paths to the CSV files of picked depths\n",
    "pickeddepth_a_csv = f'pickeddepth/{CORE_A}_pickeddepth.csv'\n",
    "pickeddepth_b_csv = f'pickeddepth/{CORE_B}_pickeddepth.csv'\n",
    "\n",
    "# Load picked depths and categories directly\n",
    "if os.path.exists(pickeddepth_a_csv):\n",
    "    picked_data_a = pd.read_csv(pickeddepth_a_csv)\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_a_csv} not found.\")\n",
    "    picked_data_a = pd.DataFrame()\n",
    "\n",
    "if os.path.exists(pickeddepth_b_csv):\n",
    "    picked_data_b = pd.read_csv(pickeddepth_b_csv)\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_b_csv} not found.\")\n",
    "    picked_data_b = pd.DataFrame()\n",
    "\n",
    "# Extract category 1 picked depths and interpreted bed names for both cores\n",
    "cat1_data_a = picked_data_a[picked_data_a['category'] == 1]\n",
    "print(f\"Loaded {len(cat1_data_a)} picked depths from {CORE_A}\")\n",
    "\n",
    "cat1_data_b = picked_data_b[picked_data_b['category'] == 1]\n",
    "print(f\"Loaded {len(cat1_data_b)} picked depths from {CORE_B}\")\n",
    "\n",
    "picked_depths_a_cat1 = cat1_data_a['picked_depths_cm'].values.tolist()\n",
    "picked_depths_b_cat1 = cat1_data_b['picked_depths_cm'].values.tolist()\n",
    "picked_categories_a_cat1 = [1] * len(picked_depths_a_cat1)\n",
    "picked_categories_b_cat1 = [1] * len(picked_depths_b_cat1)\n",
    "picked_uncertainty_a_cat1 = [1] * len(picked_depths_a_cat1)\n",
    "picked_uncertainty_b_cat1 = [1] * len(picked_depths_b_cat1)\n",
    "\n",
    "interpreted_bed_a = cat1_data_a['interpreted_bed'].fillna('').values.tolist()\n",
    "interpreted_bed_b = cat1_data_b['interpreted_bed'].fillna('').values.tolist()\n",
    "\n",
    "# Plot both cores with interpreted bed names (category 1 only)\n",
    "is_multilog = log_a.ndim > 1 and log_a.shape[1] > 1\n",
    "fig_a_beds, ax_a_beds = plot_core_data(\n",
    "    # Input data\n",
    "    md_a,                                           # depth array\n",
    "    log_a,                                          # log data array\n",
    "    f\"{CORE_A}\",                                    # core name\n",
    "    # Image data\n",
    "    core_img_1=rgb_img_a,                           # RGB image array\n",
    "    core_img_2=ct_img_a,                            # CT scan image array\n",
    "    # Data configuration\n",
    "    available_columns=available_columns_a,          # available data columns\n",
    "    is_multilog=is_multilog,                        # multi-column log flag\n",
    "    # Picked depth data\n",
    "    picked_depths=picked_depths_a_cat1,             # picked depth values (category 1 only)\n",
    "    picked_categories=picked_categories_a_cat1,     # picked categories (category 1 only)\n",
    "    picked_uncertainties=picked_uncertainty_a_cat1, # uncertainty values (category 1 only)\n",
    "    # Visualization settings\n",
    "    figsize=(20, 4),                                # figure size (width, height)\n",
    "    show_interpreted_bed_name=interpreted_bed_a     # show interpreted bed names\n",
    ")\n",
    "\n",
    "is_multilog = log_b.ndim > 1 and log_b.shape[1] > 1\n",
    "fig_b_beds, ax_b_beds = plot_core_data(\n",
    "    # Input data\n",
    "    md_b,                                           # depth array\n",
    "    log_b,                                          # log data array\n",
    "    f\"{CORE_B}\",                                    # core name\n",
    "    # Image data\n",
    "    core_img_1=rgb_img_b,                           # RGB image array\n",
    "    core_img_2=ct_img_b,                            # CT scan image array\n",
    "    # Data configuration\n",
    "    available_columns=available_columns_b,          # available data columns\n",
    "    is_multilog=is_multilog,                        # multi-column log flag\n",
    "    # Picked depth data\n",
    "    picked_depths=picked_depths_b_cat1,             # picked depth values (category 1 only)\n",
    "    picked_categories=picked_categories_b_cat1,     # picked categories (category 1 only)\n",
    "    picked_uncertainties=picked_uncertainty_b_cat1, # uncertainty values (category 1 only)\n",
    "    # Visualization settings\n",
    "    figsize=(20, 4),                                # figure size (width, height)\n",
    "    show_interpreted_bed_name=interpreted_bed_b     # show interpreted bed names\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load age constraints for both cores\n",
    "consider_adjacent_core = False\n",
    "\n",
    "data_columns = {\n",
    "    'age': 'calib810_agebp',\n",
    "    'pos_error': 'calib810_2sigma_pos', \n",
    "    'neg_error': 'calib810_2sigma_neg',\n",
    "    'min_depth': 'mindepth_cm',\n",
    "    'max_depth': 'maxdepth_cm',\n",
    "    'in_sequence': 'in_sequence',\n",
    "    'core': 'core',\n",
    "    'interpreted_bed': 'interpreted_bed'\n",
    "}\n",
    "\n",
    "# Configuration\n",
    "age_base_path = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/Age constraints/Goldfinger2012'\n",
    "\n",
    "# Load age constraints for both cores\n",
    "age_data_a = load_core_age_constraints(CORE_A, age_base_path, consider_adjacent_core, data_columns)\n",
    "age_data_b = load_core_age_constraints(CORE_B, age_base_path, consider_adjacent_core, data_columns)\n",
    "\n",
    "uncertainty_method='MonteCarlo'   # 'MonteCarlo', 'Linear', or 'Gaussian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate interpolated ages for Core A using the function\n",
    "pickeddepth_ages_a = calculate_interpolated_ages(\n",
    "    # Input data\n",
    "    picked_depths=picked_depths_a_cat1,                             # depths to interpolate ages for\n",
    "    age_constraints_depths=age_data_a['depths'],                         # age constraint depths\n",
    "    age_constraints_ages=age_data_a['ages'],                             # age constraint ages\n",
    "    age_constraints_pos_errors=age_data_a['pos_errors'],                 # positive errors\n",
    "    age_constraints_neg_errors=age_data_a['neg_errors'],                 # negative errors\n",
    "    age_constraints_in_sequence_flags=age_data_a['in_sequence_flags'],   # in-sequence flags\n",
    "    age_constraint_source_core=age_data_a['core'],                       # source core for each constraint\n",
    "    # Core boundaries\n",
    "    top_bottom=True,                                                     # include top and bottom depths/ages\n",
    "    top_depth=0.0,                                                       # top of core depth\n",
    "    bottom_depth=md_a[-1],                                               # max depth of core a\n",
    "    top_age=0,                                                           # default age at top of core\n",
    "    top_age_pos_error=75,                                                # default positive uncertainty of top age\n",
    "    top_age_neg_error=75,                                                # default negative uncertainty of top age\n",
    "    # Uncertainty calculation\n",
    "    uncertainty_method=uncertainty_method,                               # uncertainty calculation method: 'MonteCarlo', 'Linear', or 'Gaussian'\n",
    "    n_monte_carlo=10000,                                                 # number of Monte Carlo iterations\n",
    "    # Visualization and output\n",
    "    show_plot=True,                                                      # display plot\n",
    "    core_name=CORE_A,                                                    # core name for plot title\n",
    "    export_csv=True,                                                     # export results to CSV\n",
    "    csv_filename=f'pickeddepth_ages/{CORE_A}_pickeddepth_ages_{uncertainty_method}.csv',                         # CSV filename for results\n",
    "    mute_mode=False\n",
    ")\n",
    "\n",
    "# Print the age constraint data for Core A\n",
    "print(\"\\nAge Constraints for Core A:\")\n",
    "if len(age_data_a['depths']) > 0:\n",
    "    for i in range(len(age_data_a['depths'])):\n",
    "        depth_val = age_data_a['depths'].iloc[i] if isinstance(age_data_a['depths'], pd.Series) else age_data_a['depths'][i]\n",
    "        age_val = age_data_a['ages'][i]\n",
    "        pos_err_val = age_data_a['pos_errors'][i]\n",
    "        neg_err_val = age_data_a['neg_errors'][i]\n",
    "        in_seq = age_data_a['in_sequence_flags'][i]\n",
    "        \n",
    "        # Add source core and interpreted bed info if they exist\n",
    "        source_core_info = f\", Source Core: {age_data_a['core'][i]}\" if i < len(age_data_a['core']) else \"\"\n",
    "        bed_info = f\", Interpreted Bed: {age_data_a['interpreted_bed'][i]}\" if i < len(age_data_a['interpreted_bed']) else \"\"\n",
    "        \n",
    "        print(f\"Depth: {depth_val:.2f} cm, Age: {age_val:.1f} years BP (+{pos_err_val:.1f} ; -{neg_err_val:.1f}), In Sequence: {in_seq}{source_core_info}{bed_info}\")\n",
    "else:\n",
    "    print(f\"No age constraints available in {CORE_A}\")\n",
    "\n",
    "# Print the interpolated ages\n",
    "print(f\"\\nEstimated Ages for picked depths in {CORE_A}:\")\n",
    "for i, depth in enumerate(pickeddepth_ages_a['depths']):\n",
    "    print(f\"Depth: {depth:.2f} cm, Age: {pickeddepth_ages_a['ages'][i]:.1f} years BP (+{pickeddepth_ages_a['pos_uncertainties'][i]:.1f} ; -{pickeddepth_ages_a['neg_uncertainties'][i]:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate interpolated ages for Core B using the function\n",
    "pickeddepth_ages_b = calculate_interpolated_ages(\n",
    "    # Input data\n",
    "    picked_depths=picked_depths_b_cat1,                             # depths to interpolate ages for\n",
    "    age_constraints_depths=age_data_b['depths'],                         # age constraint depths\n",
    "    age_constraints_ages=age_data_b['ages'],                             # age constraint ages\n",
    "    age_constraints_pos_errors=age_data_b['pos_errors'],                 # positive errors\n",
    "    age_constraints_neg_errors=age_data_b['neg_errors'],                 # negative errors\n",
    "    age_constraints_in_sequence_flags=age_data_b['in_sequence_flags'],   # in-sequence flags\n",
    "    age_constraint_source_core=age_data_b['core'],                       # source core for each constraint\n",
    "    # Core boundaries\n",
    "    top_bottom=True,                                                     # include top and bottom depths/ages\n",
    "    top_depth=0.0,                                                       # top of core depth\n",
    "    bottom_depth=md_b[-1],                                               # max depth of core b\n",
    "    top_age=0,                                                           # default age at top of core\n",
    "    top_age_pos_error=75,                                                # default positive uncertainty of top age\n",
    "    top_age_neg_error=75,                                                # default negative uncertainty of top age\n",
    "    # Uncertainty calculation\n",
    "    uncertainty_method=uncertainty_method,                               # uncertainty calculation method: 'MonteCarlo', 'Linear', or 'Gaussian'\n",
    "    n_monte_carlo=10000,                                                 # number of Monte Carlo sampling iterations\n",
    "    # Visualization and output\n",
    "    show_plot=True,                                                      # display plot\n",
    "    core_name=CORE_B,                                                    # core name for plot title\n",
    "    export_csv=True,                                                     # export results to CSV\n",
    "    csv_filename=f'pickeddepth_ages/{CORE_B}_pickeddepth_ages_{uncertainty_method}.csv',                         # CSV filename for results\n",
    "    mute_mode=False\n",
    ")\n",
    "\n",
    "# Print the age constraint data for Core B\n",
    "print(\"\\nAge Constraints for Core B:\")\n",
    "if len(age_data_b['depths']) > 0:\n",
    "    for i in range(len(age_data_b['depths'])):\n",
    "        depth_val = age_data_b['depths'].iloc[i] if isinstance(age_data_b['depths'], pd.Series) else age_data_b['depths'][i]\n",
    "        age_val = age_data_b['ages'][i]\n",
    "        pos_err_val = age_data_b['pos_errors'][i]\n",
    "        neg_err_val = age_data_b['neg_errors'][i]\n",
    "        in_seq = age_data_b['in_sequence_flags'][i]\n",
    "        \n",
    "        # Add source core and interpreted bed info if they exist\n",
    "        source_core_info = f\", Source Core: {age_data_b['core'][i]}\" if i < len(age_data_b['core']) else \"\"\n",
    "        bed_info = f\", Interpreted Bed: {age_data_b['interpreted_bed'][i]}\" if i < len(age_data_b['interpreted_bed']) else \"\"\n",
    "        \n",
    "        print(f\"Depth: {depth_val:.2f} cm, Age: {age_val:.1f} years BP (+{pos_err_val:.1f} ; -{neg_err_val:.1f}), In Sequence: {in_seq}{source_core_info}{bed_info}\")\n",
    "else:\n",
    "    print(f\"No age constraints available in {CORE_B}\")\n",
    "\n",
    "print(f\"\\nEstimated Ages for picked depths in {CORE_B}:\")\n",
    "for i, depth in enumerate(pickeddepth_ages_b['depths']):\n",
    "    print(f\"Depth: {depth:.2f} cm, Age: {pickeddepth_ages_b['ages'][i]:.1f} years BP (+{pickeddepth_ages_b['pos_uncertainties'][i]:.1f} ; -{pickeddepth_ages_b['neg_uncertainties'][i]:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load estimated boundary age data for both cores from CSV files using the imported function\n",
    "cores = [CORE_A, CORE_B]\n",
    "pickeddepth_ages = {}\n",
    "\n",
    "for core in cores:\n",
    "    core_age_csv = f\"pickeddepth_ages/{core}_pickeddepth_ages_{uncertainty_method}.csv\"\n",
    "    pickeddepth_ages[core] = load_pickeddepth_ages_from_csv(core_age_csv)\n",
    "\n",
    "# Assign to individual variables for backward compatibility\n",
    "if CORE_A in pickeddepth_ages:\n",
    "    pickeddepth_ages_a = pickeddepth_ages[CORE_A]\n",
    "if CORE_B in pickeddepth_ages:\n",
    "    pickeddepth_ages_b = pickeddepth_ages[CORE_B]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find out all segment pairs among boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file names for age consideration or not\n",
    "\n",
    "# Check if the last age in either core is NaN to determine age consideration\n",
    "last_age_a = pickeddepth_ages_a['ages'][-1] if len(pickeddepth_ages_a['ages']) > 0 else float('nan')\n",
    "last_age_b = pickeddepth_ages_b['ages'][-1] if len(pickeddepth_ages_b['ages']) > 0 else float('nan')\n",
    "\n",
    "age_consideration = not (pd.isna(last_age_a) or pd.isna(last_age_b))\n",
    "# age_consideration = False\n",
    "\n",
    "restricted_age_correlation=True\n",
    "\n",
    "shortest_path_search=True\n",
    "\n",
    "if age_consideration:\n",
    "    if restricted_age_correlation:\n",
    "        YES_NO_AGE = 'restricted_age'\n",
    "    else:\n",
    "        YES_NO_AGE = 'loose_age'\n",
    "else:\n",
    "    YES_NO_AGE = 'no_age'\n",
    "\n",
    "if shortest_path_search:\n",
    "    SEARCH_METHOD = 'optimal'\n",
    "else:\n",
    "    SEARCH_METHOD = 'random'\n",
    "\n",
    "# Define whether to use independent DTW or not for multidimensional DTW analysis\n",
    "# If False (default): it performs dependent multidimensional DTW analysis. \n",
    "# If True: it performs independent multidimensional DTW analysis.\n",
    "independent_dtw=False \n",
    "\n",
    "# For multidimensional DTW, choose DTW method:\n",
    "# If False (default): Conventional multidimensional DTW approach\n",
    "# If True: PCA-based dependent DTW approach (perform 1D DTW on the PC1 axis)\n",
    "pca_for_dependent_dtw=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "    \n",
    "# Run comprehensive DTW analysis\n",
    "dtw_results, valid_dtw_pairs, segments_a, segments_b, depth_boundaries_a, depth_boundaries_b, dtw_distance_matrix_full = run_comprehensive_dtw_analysis(\n",
    "    # Input data\n",
    "    log_a,                                                      # Core A log data\n",
    "    log_b,                                                      # Core B log data\n",
    "    md_a,                                                       # Core A measured depth\n",
    "    md_b,                                                       # Core B measured depth\n",
    "    picked_depths_a=picked_depths_a_cat1,                         # Selected depths for core A\n",
    "    picked_depths_b=picked_depths_b_cat1,                         # Selected depths for core B\n",
    "    core_a_name=CORE_A,                                        # Name identifier for core A\n",
    "    core_b_name=CORE_B,                                        # Name identifier for core B\n",
    "    # Analysis parameters\n",
    "    top_bottom=True,                                            # Include top and bottom boundaries\n",
    "    top_depth=0.0,                                              # Starting depth for analysis\n",
    "    independent_dtw=independent_dtw,                            # Use independent DTW if True\n",
    "    exclude_deadend=True,                                       # Exclude dead-end segments\n",
    "    pca_for_dependent_dtw=pca_for_dependent_dtw,                # Use PCA for dependent DTW\n",
    "    # Age constraints\n",
    "    age_consideration=age_consideration,                        # Include age constraints\n",
    "    ages_a=pickeddepth_ages_a,                                  # Age data for core A depths\n",
    "    ages_b=pickeddepth_ages_b,                                  # Age data for core B depths\n",
    "    restricted_age_correlation=restricted_age_correlation,      # Use strict age correlation\n",
    "    all_constraint_ages_a=age_data_a['in_sequence_ages'],      # All age constraints for core A\n",
    "    all_constraint_ages_b=age_data_b['in_sequence_ages'],      # All age constraints for core B\n",
    "    all_constraint_depths_a=age_data_a['in_sequence_depths'],  # All depth constraints for core A\n",
    "    all_constraint_depths_b=age_data_b['in_sequence_depths'],  # All depth constraints for core B\n",
    "    all_constraint_pos_errors_a=age_data_a['in_sequence_pos_errors'], # Positive age errors for core A\n",
    "    all_constraint_pos_errors_b=age_data_b['in_sequence_pos_errors'], # Positive age errors for core B\n",
    "    all_constraint_neg_errors_a=age_data_a['in_sequence_neg_errors'], # Negative age errors for core A\n",
    "    all_constraint_neg_errors_b=age_data_b['in_sequence_neg_errors'], # Negative age errors for core B\n",
    "    age_constraint_a_source_cores=age_data_a['core'],          # Source cores for age constraints A\n",
    "    age_constraint_b_source_cores=age_data_b['core'],          # Source cores for age constraints B\n",
    "    # Visualization\n",
    "    visualize_pairs=True,                                       # Create pair visualizations\n",
    "    visualize_segment_labels=False,                             # Show segment labels in plots\n",
    "    create_dtw_matrix=True,                                     # Generate DTW distance matrix\n",
    "    dtwmatrix_output_filename=f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/SegmentPair_DTW_matrix_{YES_NO_AGE}_{SEARCH_METHOD}.png', # Matrix plot filename\n",
    "    creategif=True,                                             # Create animated GIF\n",
    "    gif_output_filename=f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/SegmentPair_DTW_animation_{YES_NO_AGE}_{SEARCH_METHOD}.gif', # GIF filename\n",
    "    max_frames=50,                                              # Maximum frames in animation\n",
    "    color_interval_size=5,                                      # Color coding interval size\n",
    "    keep_frames=False,                                           # Save individual frames\n",
    "    # Debug and processing\n",
    "    debug=False                                                 # Enable debug output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Estimation of the Number of Possible Solutions (Total Complete Paths)\n",
    "\n",
    "### 1. Segment Generation Formula\n",
    "\n",
    "Given $n$ units for each core, the **number of valid segments per core** follows:\n",
    "\n",
    "$$S = 2n + 1$$\n",
    "\n",
    "**Examples from solution_order.csv**:\n",
    "- 6 units → 13 segments ($S = 2(6) + 1$) ✓\n",
    "- 8 units → 17 segments ($S = 2(8) + 1$) ✓\n",
    "- 18 units → 37 segments ($S = 2(18) + 1$) ✓\n",
    "- 30 units → 61 segments ($S = 2(30) + 1$) ✓\n",
    "- 31 units → 63 segments ($S = 2(31) + 1$) ✓\n",
    "\n",
    "The relationship holds consistently across all 97 core pairs, with $S \\approx 2n + 1$ for most cases.\n",
    "\n",
    "### 2. Valid Segment Pairs\n",
    "\n",
    "Not all segment pairs $(S_A \\times S_B)$ are valid. DTW analysis filters pairs based on correlation quality:\n",
    "\n",
    "**Theoretical maximum**: $P_{\\max} = S_A \\times S_B$  \n",
    "**Actual valid pairs**: $P_{\\mathrm{valid}} \\ll P_{\\max}$ (determined by DTW filtering)\n",
    "\n",
    "**Representative examples**:\n",
    "- $17 \\times 13 = 221$ max → **158 valid pairs** (71% retention)\n",
    "- $37 \\times 53 = 1961$ max → **1448 valid pairs** (74% retention)\n",
    "- $61 \\times 63 = 3843$ max → **2851 valid pairs** (74% retention)\n",
    "- $41 \\times 39 = 1599$ max → **1179 valid pairs** (74% retention)\n",
    "\n",
    "**Typical retention rate**: ~70-75% of theoretical maximum pairs remain valid after DTW filtering.\n",
    "\n",
    "**Linear relationship from 97 core pairs**:\n",
    "$$P_{\\mathrm{valid}} \\approx 0.745 \\times (S_A \\times S_B)$$\n",
    "\n",
    "### 3. Solution Count Formula\n",
    "\n",
    "The relationship between valid pairs and total solutions follows a **quadratic-in-log-space** pattern:\n",
    "\n",
    "$$\\ln(C) = 4.395 \\times (\\ln P_{\\mathrm{valid}})^2 - 43.179 \\times \\ln P_{\\mathrm{valid}} + 116.872$$\n",
    "\n",
    "Equivalently:\n",
    "$$C = e^{4.395 (\\ln P_{\\mathrm{valid}})^2 - 43.179 \\ln P_{\\mathrm{valid}} + 116.872}$$\n",
    "\n",
    "This empirical formula was fitted to 97 core pair analyses with $R^2 > 0.999$.\n",
    "\n",
    "### 4. Complete Estimation Chain\n",
    "\n",
    "Combining all relationships:\n",
    "\n",
    "1. **Segments from units**: $S_A = 2n_A + 1$, $S_B = 2n_B + 1$\n",
    "2. **Valid pairs from segments**: $P_{\\mathrm{valid}} \\approx 0.745 \\times S_A \\times S_B$\n",
    "3. **Solutions from valid pairs**: $\\ln(C) = 4.395 (\\ln P_{\\mathrm{valid}})^2 - 43.179 \\ln P_{\\mathrm{valid}} + 116.872$\n",
    "\n",
    "### 5. Practical Examples\n",
    "\n",
    "Based on actual data from 97 core pair analyses:\n",
    "\n",
    "| Units per core | Valid segments | Valid pairs ($P_{\\mathrm{valid}}$) | Est. solutions ($C$) |\n",
    "|----------------|----------------|-------------|---------------------|\n",
    "| 6-8 | 13-17 | 150-700 | $10^{4}$ - $10^{8}$ |\n",
    "| 11-14 | 23-29 | 250-1,100 | $10^{5}$ - $10^{13}$ |\n",
    "| 18-22 | 37-45 | 1,100-1,900 | $10^{13}$ - $10^{18}$ |\n",
    "| 24-28 | 49-57 | 1,400-2,700 | $10^{15}$ - $10^{22}$ |\n",
    "| 30-31 | 61-63 | 2,400-2,900 | $10^{20}$ - $10^{24}$ |\n",
    "\n",
    "**Key finding**: The quadratic-in-log-space relationship means solution count grows rapidly with problem size. Small increases in the number of units per core lead to dramatic increases in computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_result = diagnose_chain_breaks(\n",
    "    # Input data\n",
    "    valid_dtw_pairs,                                        # Valid DTW segment pairs from analysis\n",
    "    segments_a,                                             # Segment definitions for core A\n",
    "    segments_b,                                             # Segment definitions for core Bss\n",
    "    depth_boundaries_a,                                     # Depth boundaries for core A segments\n",
    "    depth_boundaries_b                                      # Depth boundaries for core B segments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Search complete DTW paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_path_search_result = find_complete_core_paths(\n",
    "    # Input data\n",
    "    valid_dtw_pairs,                                                                # Valid DTW segment pairs from analysis\n",
    "    segments_a,                                                                     # Segment definitions for core A\n",
    "    segments_b,                                                                     # Segment definitions for core B\n",
    "    log_a,                                                                          # Log data for core A\n",
    "    log_b,                                                                          # Log data for core B\n",
    "    depth_boundaries_a,                                                             # Depth boundaries for core A segments\n",
    "    depth_boundaries_b,                                                             # Depth boundaries for core B segments\n",
    "    dtw_results,                                                                    # DTW analysis results\n",
    "    dtw_distance_matrix_full,                                                       # Full DTW distance matrix\n",
    "    # Output settings\n",
    "    output_csv=f\"outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/mappings_{YES_NO_AGE}_{SEARCH_METHOD}.csv\",     # Output CSV filename for mappings\n",
    "    # Search parameters\n",
    "    start_from_top_only=True,                                                       # Start path search from top segments only\n",
    "    shortest_path_search=shortest_path_search,                                      # Use shortest path search algorithm\n",
    "    shortest_path_level=2,                                                          # Path level preference (higher = more segments)\n",
    "    max_search_path=100000,                                                         # Maximum paths per segment pair to avoid memory issues\n",
    "    # Processing settings\n",
    "    batch_size=1000,                                                                # Processing batch size\n",
    "    n_jobs=-1,                                                                      # Number of CPU cores (-1 uses all available)\n",
    "    debug=False,                                                                    # Enable debug output,\n",
    "    pca_for_dependent_dtw=pca_for_dependent_dtw\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "correlation_gif_name=f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/CombinedDTW_correlation_mappings_{YES_NO_AGE}_{SEARCH_METHOD}.gif'\n",
    "matrix_gif_name=f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/CombinedDTW_matrix_mappings_{YES_NO_AGE}_{SEARCH_METHOD}.gif'\n",
    "\n",
    "# 1. First, read all available mappings from a CSV (assuming it was created by find_all_sequential_mappings)\n",
    "sequential_mappings_csv = f\"outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/mappings_{YES_NO_AGE}_{SEARCH_METHOD}.csv\"\n",
    "\n",
    "# 3. Visualize a representative subset of the mappings\n",
    "visualize_dtw_results_from_csv(\n",
    "    # Input data\n",
    "    sequential_mappings_csv,                                                        # CSV file with sequential mappings\n",
    "    log_a,                                                                          # Log data for core A\n",
    "    log_b,                                                                          # Log data for core B\n",
    "    md_a,                                                                           # Measured depth data for core A\n",
    "    md_b,                                                                           # Measured depth data for core B\n",
    "    dtw_results,                                                                    # DTW analysis results\n",
    "    valid_dtw_pairs,                                                                # Valid DTW segment pairs\n",
    "    segments_a,                                                                     # Segment definitions for core A\n",
    "    segments_b,                                                                     # Segment definitions for core B\n",
    "    depth_boundaries_a,                                                             # Depth boundaries for core A segments\n",
    "    depth_boundaries_b,                                                             # Depth boundaries for core B segments\n",
    "    dtw_distance_matrix_full,                                                       # Full DTW distance matrix\n",
    "    # Core identifiers\n",
    "    core_a_name=CORE_A,                                                             # Name identifier for core A\n",
    "    core_b_name=CORE_B,                                                             # Name identifier for core B\n",
    "    # Visualization settings\n",
    "    color_interval_size=10,                                                         # Color interval size for visualization\n",
    "    debug=False,                                                                    # Enable debug output\n",
    "    visualize_pairs=False,                                                          # Show DTW pairs in visualization\n",
    "    visualize_segment_labels=False,                                                 # Show segment labels in visualization\n",
    "    mark_depths=False,                                                               # Mark depth points in visualization\n",
    "    # GIF output settings\n",
    "    creategif=True,                                                                 # Create animated GIF output\n",
    "    correlation_gif_output_filename=correlation_gif_name,                          # Output filename for correlation GIF\n",
    "    matrix_gif_output_filename=matrix_gif_name,                                    # Output filename for matrix GIF\n",
    "    max_frames=50,                                                                  # Maximum number of frames in GIF\n",
    "    keep_frames=False,                                                               # Keep individual frames after GIF creation\n",
    "    # Age constraints\n",
    "    mark_ages=age_consideration,                                                    # Mark age constraints in visualization\n",
    "    ages_a=pickeddepth_ages_a,                                                      # Age data for core A\n",
    "    ages_b=pickeddepth_ages_b,                                                      # Age data for core B\n",
    "    all_constraint_depths_a=age_data_a['in_sequence_depths'],                       # Depth constraints for core A\n",
    "    all_constraint_depths_b=age_data_b['in_sequence_depths'],                       # Depth constraints for core B\n",
    "    all_constraint_ages_a=age_data_a['in_sequence_ages'],                           # Age constraints for core A\n",
    "    all_constraint_ages_b=age_data_b['in_sequence_ages'],                           # Age constraints for core B\n",
    "    all_constraint_pos_errors_a=age_data_a['in_sequence_pos_errors'],               # Positive age errors for core A\n",
    "    all_constraint_pos_errors_b=age_data_b['in_sequence_pos_errors'],               # Positive age errors for core B\n",
    "    all_constraint_neg_errors_a=age_data_a['in_sequence_neg_errors'],               # Negative age errors for core A\n",
    "    all_constraint_neg_errors_b=age_data_b['in_sequence_neg_errors'],               # Negative age errors for core B\n",
    "    age_constraint_a_source_cores=age_data_a['core'],                               # Source cores for age constraints A\n",
    "    age_constraint_b_source_cores=age_data_b['core'],                                # Source cores for age constraints B\n",
    "    # Interpreted bed correlation\n",
    "    interpreted_bed_a=interpreted_bed_a,         # Interpreted bed name for Core A\n",
    "    interpreted_bed_b=interpreted_bed_b          # Interpreted bed name for Core B\n",
    ")\n",
    "\n",
    "# Display the GIFs\n",
    "print(\"DTW Correlation Mappings GIF:\")\n",
    "display(IPImage(correlation_gif_name))\n",
    "\n",
    "print(\"DTW Matrix Mappings GIF:\")\n",
    "display(IPImage(matrix_gif_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved DTW results\n",
    "sequential_mappings_csv = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/mappings_{YES_NO_AGE}_{SEARCH_METHOD}.csv'\n",
    "output_matrix_png_filename = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/CombinedDTW_matrix_mappings_colored_{YES_NO_AGE}_{SEARCH_METHOD}.png'\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "_ = plot_dtw_matrix_with_paths(\n",
    "    # Input data\n",
    "    dtw_distance_matrix_full,                                                       # Full DTW distance matrix\n",
    "    sequential_mappings_csv=sequential_mappings_csv,                                # CSV file with sequential mappings\n",
    "    # Core identifiers\n",
    "    core_a_name=CORE_A,                                                             # Name identifier for core A\n",
    "    core_b_name=CORE_B,                                                             # Name identifier for core B\n",
    "    md_a=md_a,                                                                      # Metadata for core A\n",
    "    md_b=md_b,                                                                      # Metadata for core B\n",
    "    # Visualization settings\n",
    "    mode='all_paths_colored',                                                       # Visualization mode\n",
    "    color_metric='norm_dtw',                                                    # Metric used for coloring paths\n",
    "                                                                                    # Available options: 'corr_coef', 'norm_dtw', 'dtw_ratio', 'perc_diag', 'dtw_warp_eff', 'perc_age_overlap', None (uses mapping_id)\n",
    "    output_filename=output_matrix_png_filename,                                     # Output filename for the plot\n",
    "    # Age constraint data\n",
    "    age_constraint_a_depths=age_data_a['in_sequence_depths'] if age_consideration else None,  # Depth constraints for core A\n",
    "    age_constraint_a_ages=age_data_a['in_sequence_ages'] if age_consideration else None,      # Age constraints for core A\n",
    "    age_constraint_a_source_cores=age_data_a['core'] if age_consideration else None,          # Source cores for age constraints A\n",
    "    age_constraint_b_depths=age_data_b['in_sequence_depths'] if age_consideration else None,  # Depth constraints for core B\n",
    "    age_constraint_b_ages=age_data_b['in_sequence_ages'] if age_consideration else None,      # Age constraints for core B\n",
    "    age_constraint_b_source_cores=age_data_b['core'] if age_consideration else None,          # Source cores for age constraints B\n",
    "    # Performance settings\n",
    "    n_jobs=-1                                                                       # Number of parallel jobs (-1 means use all processors)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find the best mapping ####\n",
    "# Load the DTW results\n",
    "sequential_mappings_csv = f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/mappings_{YES_NO_AGE}_{SEARCH_METHOD}.csv'\n",
    "\n",
    "# Use custom metric weights\n",
    "custom_weights = {\n",
    "    'corr_coef': 1.0,\n",
    "    'perc_diag': 0.0,\n",
    "    'norm_dtw': 1.0,\n",
    "    'dtw_ratio': 0.0,\n",
    "    'perc_age_overlap': 0.0,\n",
    "    'dtw_warp_eff': 0.0\n",
    "}\n",
    "\n",
    "# # To just find the best-scored mappings\n",
    "# top_mapping_ids, top_mapping_pairs, top_mappings_df = find_best_mappings(\n",
    "#     csv_file_path=sequential_mappings_csv,\n",
    "#     top_n=10,\n",
    "#     filter_shortest_dtw=True,\n",
    "#     metric_weight=custom_weights\n",
    "# )\n",
    "\n",
    "### To find the best-scored mappings that comply the intepreted bed correlation\n",
    "top_mapping_ids, top_mapping_pairs, top_mappings_df = find_best_mappings(\n",
    "    csv_file_path=sequential_mappings_csv,\n",
    "    metric_weight=custom_weights,\n",
    "    picked_depths_a_cat1=picked_depths_a_cat1,\n",
    "    picked_depths_b_cat1=picked_depths_b_cat1,\n",
    "    interpreted_bed_a=interpreted_bed_a,\n",
    "    interpreted_bed_b=interpreted_bed_b,\n",
    "    valid_dtw_pairs=valid_dtw_pairs,\n",
    "    segments_a=segments_a,\n",
    "    segments_b=segments_b\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "visualize_pairs=False\n",
    "\n",
    "if visualize_pairs:\n",
    "    visualize_type='pairs'\n",
    "    visualize_segment_labels=False\n",
    "    mark_depths=True\n",
    "else:\n",
    "    visualize_type='fullpath'\n",
    "    visualize_segment_labels=False\n",
    "    mark_depths=False\n",
    "\n",
    "# Visualize the combined segments\n",
    "_, _, _, _ = visualize_combined_segments(\n",
    "    # Input data\n",
    "    log_a=log_a,                                # Core A log data\n",
    "    log_b=log_b,                                # Core B log data\n",
    "    md_a=md_a,                                  # Core A measured depths\n",
    "    md_b=md_b,                                  # Core B measured depths\n",
    "    dtw_results=dtw_results,                    # DTW alignment results\n",
    "    valid_dtw_pairs=valid_dtw_pairs,            # Valid DTW pairs\n",
    "    segments_a=segments_a,                      # Core A segments\n",
    "    segments_b=segments_b,                      # Core B segments\n",
    "    depth_boundaries_a=depth_boundaries_a,      # Core A depth boundaries\n",
    "    depth_boundaries_b=depth_boundaries_b,      # Core B depth boundaries\n",
    "    dtw_distance_matrix_full=dtw_distance_matrix_full,       # Full DTW distance matrix\n",
    "    segment_pairs_to_combine=top_mapping_pairs[0],           # Valid pairs to combine\n",
    "    # Visualization options\n",
    "    color_interval_size=10,                     # Size of color intervals\n",
    "    visualize_pairs=visualize_pairs,            # Whether to visualize pairs (True/False)\n",
    "    visualize_segment_labels=visualize_segment_labels, # Whether to show segment labels (True/False)\n",
    "    mark_depths=mark_depths,                    # Whether to mark depths (True/False)\n",
    "    # Output paths\n",
    "    correlation_save_path=f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/CombinedDTW_correlation_{YES_NO_AGE}_{SEARCH_METHOD}_{top_mapping_ids[0]}_{visualize_type}.png',\n",
    "    # correlation_save_path=f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/CombinedDTW_correlation_{YES_NO_AGE}_{SEARCH_METHOD}_{top_mapping_ids[0]}_{visualize_type}.svg',\n",
    "    matrix_save_path=f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/CombinedDTW_matrix_{YES_NO_AGE}_{SEARCH_METHOD}_{top_mapping_ids[0]}_{visualize_type}.png',\n",
    "    # Age constraint parameters\n",
    "    mark_ages=age_consideration,                # Whether to mark ages (True/False)\n",
    "    ages_a=pickeddepth_ages_a if age_consideration else None, # Core A ages\n",
    "    ages_b=pickeddepth_ages_b if age_consideration else None, # Core B ages\n",
    "    all_constraint_ages_a=age_data_a['in_sequence_ages'] if age_consideration else None, # Core A constraint ages\n",
    "    all_constraint_ages_b=age_data_b['in_sequence_ages'] if age_consideration else None, # Core B constraint ages\n",
    "    all_constraint_depths_a=age_data_a['in_sequence_depths'] if age_consideration else None, # Core A constraint depths\n",
    "    all_constraint_depths_b=age_data_b['in_sequence_depths'] if age_consideration else None, # Core B constraint depths\n",
    "    all_constraint_pos_errors_a=age_data_a['in_sequence_pos_errors'] if age_consideration else None, # Core A positive errors\n",
    "    all_constraint_pos_errors_b=age_data_b['in_sequence_pos_errors'] if age_consideration else None, # Core B positive errors\n",
    "    all_constraint_neg_errors_a=age_data_a['in_sequence_neg_errors'] if age_consideration else None, # Core A negative errors\n",
    "    all_constraint_neg_errors_b=age_data_b['in_sequence_neg_errors'] if age_consideration else None, # Core B negative errors\n",
    "    age_constraint_a_source_cores=age_data_a['core'] if age_consideration else None, # Core A source cores\n",
    "    age_constraint_b_source_cores=age_data_b['core'] if age_consideration else None, # Core B source cores\n",
    "    # Core identifiers\n",
    "    core_a_name=CORE_A,                         # Name of Core A\n",
    "    core_b_name=CORE_B,                         # Name of Core B\n",
    "    # Interpreted bed correlation\n",
    "    interpreted_bed_a=interpreted_bed_a,         # Interpreted bed name for Core A\n",
    "    interpreted_bed_b=interpreted_bed_b          # Interpreted bed name for Core B\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available quality indices: 'corr_coef', 'norm_dtw', 'dtw_ratio', 'perc_diag', 'dtw_warp_eff', 'perc_age_overlap'\n",
    "# targeted_quality_index = 'corr_coef' \n",
    "targeted_quality_index = ['corr_coef', 'norm_dtw']  # Can be a single string or list of strings\n",
    "\n",
    "# Handle both single index and multiple indices\n",
    "if isinstance(targeted_quality_index, str):\n",
    "    targeted_quality_index = [targeted_quality_index]\n",
    "\n",
    "# Loop over all targeted quality indices\n",
    "for quality_idx in targeted_quality_index:\n",
    "    plot_correlation_distribution(\n",
    "        # Input parameters\n",
    "        csv_file=f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/mappings_{YES_NO_AGE}_{SEARCH_METHOD}.csv',  # Path to mappings CSV file\n",
    "        target_mapping_id=top_mapping_ids[0],                                             # ID of mapping to analyze\n",
    "        quality_index=quality_idx,                                                        # Quality metric to plot\n",
    "        # Core names\n",
    "        core_a_name=CORE_A,                                                               # Core A name\n",
    "        core_b_name=CORE_B,                                                               # Core B name\n",
    "        # Histogram parameters\n",
    "        bin_width=None,                                                                   # Bin width (auto if None)\n",
    "        # Output parameters\n",
    "        save_png=True,                                                                    # Whether to save plot as PNG\n",
    "        png_filename=f'outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{\"r-values\" if quality_idx == \"corr_coef\" else quality_idx}_distribution_{YES_NO_AGE}_{SEARCH_METHOD}.png',  # Output filename\n",
    "        # Distribution fitting parameters\n",
    "        pdf_method='normal',                                                              # PDF fitting method: 'KDE', 'skew-normal', or 'normal'\n",
    "        kde_bandwidth=0.05,                                                               # Bandwidth for KDE method\n",
    "        mute_mode=False                                                                   # Whether to suppress print statements\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "11-TimeSeriesCorrelation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
