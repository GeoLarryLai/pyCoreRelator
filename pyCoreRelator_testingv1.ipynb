{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25kTe2nS84Tf"
   },
   "source": [
    "# Correlating well log pairs: Complex Dynamic Time Warping with boundary constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4awuK6ffO2t"
   },
   "source": [
    "## Introduction to dynamic time warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "47Kk07X_84Th"
   },
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "import csv\n",
    "import glob\n",
    "from IPython.display import Image as IPImage, display\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyCoreRelator import (\n",
    "    run_comprehensive_dtw_analysis,\n",
    "    find_complete_core_paths,\n",
    "    diagnose_chain_breaks,\n",
    "    calculate_interpolated_ages,\n",
    "    visualize_combined_segments,\n",
    "    visualize_dtw_results_from_csv,\n",
    "    load_log_data,\n",
    "    plot_core_data,\n",
    "    plot_dtw_matrix_with_paths,\n",
    "    plot_correlation_distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Test with Cascadia hi-res MS logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define core pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define core names as variables for easy reference\n",
    "# CORE_A = \"M9907-22PC\"\n",
    "# CORE_B = \"M9907-23PC\"\n",
    "CORE_A = \"M9907-23PC\"\n",
    "CORE_B = \"M9907-25PC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data structures and core images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Define paths to the CSV files\n",
    "pickeddepth_a_csv = f'pickeddepth/{CORE_A}_pickeddepth.csv'\n",
    "pickeddepth_b_csv = f'pickeddepth/{CORE_B}_pickeddepth.csv'\n",
    "\n",
    "# Load picked depths and categories from CSV files\n",
    "if os.path.exists(pickeddepth_b_csv):\n",
    "    picked_data_b = pd.read_csv(pickeddepth_b_csv)\n",
    "    # Combine depths and categories into tuples\n",
    "    picked_b = list(zip(picked_data_b['picked_depths_cm'].values.tolist(), \n",
    "                        picked_data_b['category'].values.tolist()))\n",
    "    print(f\"Loaded {len(picked_b)} picked depths for {CORE_B}\")\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_b_csv} not found. Using empty list for picked_b.\")\n",
    "    picked_b = []\n",
    "\n",
    "if os.path.exists(pickeddepth_a_csv):\n",
    "    picked_data_a = pd.read_csv(pickeddepth_a_csv)\n",
    "    # Combine depths and categories into tuples\n",
    "    picked_a = list(zip(picked_data_a['picked_depths_cm'].values.tolist(), \n",
    "                        picked_data_a['category'].values.tolist()))\n",
    "    print(f\"Loaded {len(picked_a)} picked depths for {CORE_A}\")\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_a_csv} not found. Using empty list for picked_a.\")\n",
    "    picked_a = []\n",
    "\n",
    "# Create uncertainty arrays (assuming uncertainty size is 2 cm)\n",
    "picked_uncertainty_b = [1] * len(picked_b)\n",
    "picked_uncertainty_a = [1] * len(picked_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log columns to extract\n",
    "# LOG_COLUMNS = ['hiresMS', 'CT', 'R', 'G', 'B']  # Choose which logs to include\n",
    "# LOG_COLUMNS = ['hiresMS', 'CT', 'Lumin']  # Choose which logs to include\n",
    "LOG_COLUMNS = ['hiresMS']  # Choose which logs to include\n",
    "DEPTH_COLUMN = 'SB_DEPTH_cm'\n",
    "\n",
    "# Define directory paths\n",
    "mother_dir = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/'\n",
    "\n",
    "# Define paths for Core A\n",
    "core_a_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv'\n",
    "}\n",
    "\n",
    "core_a_rgb_img_path = f\"{mother_dir}_compiled_logs/{CORE_A}/{CORE_A}_RGB.tiff\"\n",
    "core_a_ct_img_path = f\"{mother_dir}_compiled_logs/{CORE_A}/{CORE_A}_CT.tiff\"\n",
    "\n",
    "# Define paths for Core B\n",
    "core_b_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv'\n",
    "}\n",
    "core_b_rgb_img_path = f\"{mother_dir}_compiled_logs/{CORE_B}/{CORE_B}_RGB.tiff\"\n",
    "core_b_ct_img_path = f\"{mother_dir}_compiled_logs/{CORE_B}/{CORE_B}_CT.tiff\"\n",
    "\n",
    "# Define column mapping for alternative column names\n",
    "column_alternatives = {\n",
    "    'hiresMS': ['MS'],\n",
    "    'CT': ['CT_value'],\n",
    "    'R': ['R', 'red', 'Red'],\n",
    "    'G': ['G', 'green', 'Green'],\n",
    "    'B': ['B', 'blue', 'Blue'],\n",
    "    'Lumin': ['luminance', 'Luminance'],\n",
    "    'Den_gm/cc': ['Density', 'density']\n",
    "}\n",
    "\n",
    "# Load data for Core A\n",
    "log_a, md_a, available_columns_a, rgb_img_a, ct_img_a = load_log_data(\n",
    "    core_a_log_paths,\n",
    "    {'rgb': core_a_rgb_img_path, 'ct': core_a_ct_img_path},\n",
    "    LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")\n",
    "\n",
    "print(\"\\n=== DEBUG: Core A Loading ===\")\n",
    "print(f\"LOG_COLUMNS requested: {LOG_COLUMNS}\")\n",
    "print(f\"Available columns loaded: {available_columns_a}\")\n",
    "print(f\"Shape of log_a: {log_a.shape}\")\n",
    "print(f\"Type of log_a: {type(log_a)}\")\n",
    "if hasattr(log_a, 'ndim'):\n",
    "    print(f\"log_a dimensions: {log_a.ndim}\")\n",
    "    if log_a.ndim > 1:\n",
    "        print(f\"log_a has {log_a.shape[1]} columns\\n\")\n",
    "    else:\n",
    "        print(\"log_a is 1D (single column)\\n\")\n",
    "\n",
    "# Load data for Core B\n",
    "log_b, md_b, available_columns_b, rgb_img_b, ct_img_b = load_log_data(\n",
    "    core_b_log_paths,\n",
    "    {'rgb': core_b_rgb_img_path, 'ct': core_b_ct_img_path},\n",
    "    LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")\n",
    "\n",
    "print(\"\\n=== DEBUG: Core B Loading ===\")\n",
    "print(f\"LOG_COLUMNS requested: {LOG_COLUMNS}\")\n",
    "print(f\"Available columns loaded: {available_columns_b}\")\n",
    "print(f\"Shape of log_b: {log_b.shape}\")\n",
    "print(f\"Type of log_b: {type(log_b)}\")\n",
    "if hasattr(log_b, 'ndim'):\n",
    "    print(f\"log_b dimensions: {log_b.ndim}\")\n",
    "    if log_b.ndim > 1:\n",
    "        print(f\"log_b has {log_b.shape[1]} columns\\n\")\n",
    "    else:\n",
    "        print(\"log_b is 1D (single column)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract depths and categories from the loaded tuples\n",
    "picked_depths_a = [depth for depth, category in picked_a] if picked_a else []\n",
    "picked_categories_a = [category for depth, category in picked_a] if picked_a else []\n",
    "\n",
    "picked_depths_b = [depth for depth, category in picked_b] if picked_b else []\n",
    "picked_categories_b = [category for depth, category in picked_b] if picked_b else []\n",
    "\n",
    "# Now plot the cores with enhanced plot_core_data function\n",
    "is_multilog = log_a.ndim > 1 and log_a.shape[1] > 1\n",
    "fig_a, ax_a = plot_core_data(\n",
    "    md_a, log_a, \n",
    "    f\"{CORE_A}\",\n",
    "    rgb_img=rgb_img_a, \n",
    "    ct_img=ct_img_a,\n",
    "    figsize=(20, 4),\n",
    "    available_columns=available_columns_a,\n",
    "    is_multilog=is_multilog,\n",
    "    picked_depths=picked_depths_a,\n",
    "    picked_categories=picked_categories_a,\n",
    "    picked_uncertainties=picked_uncertainty_a,\n",
    "    show_category= [1],  # Show all categories, or [1] for specific ones\n",
    "    show_bed_number=True\n",
    ")\n",
    "\n",
    "# Do the same for Core B\n",
    "is_multilog = log_b.ndim > 1 and log_b.shape[1] > 1\n",
    "fig_b, ax_b = plot_core_data(\n",
    "    md_b, log_b, \n",
    "    f\"{CORE_B}\",\n",
    "    rgb_img=rgb_img_b, \n",
    "    ct_img=ct_img_b,\n",
    "    figsize=(20, 4),\n",
    "    available_columns=available_columns_b,\n",
    "    is_multilog=is_multilog,\n",
    "    picked_depths=picked_depths_b,\n",
    "    picked_categories=picked_categories_b,\n",
    "    picked_uncertainties=picked_uncertainty_b,\n",
    "    show_category=[1],\n",
    "    show_bed_number=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Usage Examples and Executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a subset of picked depths of category 1 for both cores\n",
    "all_depths_a_cat1 = np.array([depth for depth, category in picked_a if category == 1]).astype('float32')\n",
    "all_depths_b_cat1 = np.array([depth for depth, category in picked_b if category == 1]).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_age_constraints(core_name, consider_adjacent_core=False):\n",
    "    \"\"\"\n",
    "    Load age constraints for a specific core, optionally including data from adjacent cores.\n",
    "    \n",
    "    Args:\n",
    "        core_name: Name of the core to load data for\n",
    "        consider_adjacent_core: If True, also load data from cores with similar names\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing all age constraint data\n",
    "    \"\"\"\n",
    "    base_path = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/Age constraints/Goldfinger2012'\n",
    "    csv_files = []\n",
    "    \n",
    "    # Add primary core CSV\n",
    "    primary_csv = f'{base_path}/{core_name}_age.csv'\n",
    "    csv_files.append(primary_csv)\n",
    "    \n",
    "    # Add adjacent core CSVs if specified\n",
    "    if consider_adjacent_core:\n",
    "        # Get base part of core name (without last two characters)\n",
    "        core_base = core_name[:-2]\n",
    "        # Look for similar core names in the directory\n",
    "        if os.path.exists(base_path):\n",
    "            for file in os.listdir(base_path):\n",
    "                if file.endswith('_age.csv') and file.startswith(f'{core_base}'):\n",
    "                    potential_core = file.split('_age.csv')[0]\n",
    "                    if potential_core != core_name:  # Skip the primary core\n",
    "                        csv_files.append(f'{base_path}/{file}')\n",
    "    \n",
    "    # Initialize result containers\n",
    "    all_data = pd.DataFrame()\n",
    "    result = {\n",
    "        'depths': [],\n",
    "        'ages': [],\n",
    "        'pos_errors': [],\n",
    "        'neg_errors': [],\n",
    "        'in_sequence_flags': [],\n",
    "        'in_sequence_depths': [],\n",
    "        'in_sequence_ages': [],\n",
    "        'in_sequence_pos_errors': [],\n",
    "        'in_sequence_neg_errors': [],\n",
    "        'out_sequence_depths': [],\n",
    "        'out_sequence_ages': [],\n",
    "        'out_sequence_pos_errors': [],\n",
    "        'out_sequence_neg_errors': [],\n",
    "        'core': [],\n",
    "        'interpreted_bed': []\n",
    "    }\n",
    "    \n",
    "    # Define required columns\n",
    "    required_columns = ['calib502_agebp', 'calib502_2sigma_pos', 'calib502_2sigma_neg', \n",
    "                      'mindepth_cm', 'maxdepth_cm', 'in_sequence', 'core', 'interpreted_bed']\n",
    "    \n",
    "    # Process each CSV file\n",
    "    loaded_files = 0\n",
    "    for csv_file in csv_files:\n",
    "        if os.path.exists(csv_file):\n",
    "            data = pd.read_csv(csv_file)\n",
    "            # Filter rows with all required columns available\n",
    "            for col in required_columns:\n",
    "                data = data.dropna(subset=[col])\n",
    "            \n",
    "            all_data = pd.concat([all_data, data])\n",
    "            loaded_files += 1\n",
    "    \n",
    "    if loaded_files > 0:\n",
    "        print(f\"Loaded {len(all_data)} age constraints for {core_name}\")\n",
    "        \n",
    "        # Sort by age if multiple cores were combined\n",
    "        if consider_adjacent_core:\n",
    "            all_data = all_data.sort_values(by='mindepth_cm')\n",
    "        \n",
    "        # Extract all age constraints\n",
    "        result['depths'] = (all_data['mindepth_cm'] + all_data['maxdepth_cm']) / 2\n",
    "        result['ages'] = all_data['calib502_agebp'].tolist()\n",
    "        result['pos_errors'] = all_data['calib502_2sigma_pos'].tolist()\n",
    "        result['neg_errors'] = all_data['calib502_2sigma_neg'].tolist()\n",
    "        result['in_sequence_flags'] = all_data['in_sequence'].tolist()\n",
    "        result['core'] = all_data['core'].tolist()\n",
    "        result['interpreted_bed'] = all_data['interpreted_bed'].tolist()\n",
    "        \n",
    "        # Separate in-sequence and out-of-sequence constraints\n",
    "        for i in range(len(result['in_sequence_flags'])):\n",
    "            if result['in_sequence_flags'][i] == 1:\n",
    "                result['in_sequence_depths'].append(result['depths'].iloc[i] if isinstance(result['depths'], pd.Series) else result['depths'][i])\n",
    "                result['in_sequence_ages'].append(result['ages'][i])\n",
    "                result['in_sequence_pos_errors'].append(result['pos_errors'][i])\n",
    "                result['in_sequence_neg_errors'].append(result['neg_errors'][i])\n",
    "            else:\n",
    "                result['out_sequence_depths'].append(result['depths'].iloc[i] if isinstance(result['depths'], pd.Series) else result['depths'][i])\n",
    "                result['out_sequence_ages'].append(result['ages'][i])\n",
    "                result['out_sequence_pos_errors'].append(result['pos_errors'][i])\n",
    "                result['out_sequence_neg_errors'].append(result['neg_errors'][i])\n",
    "    else:\n",
    "        print(f\"Warning: No age constraint files found for {core_name}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define core names\n",
    "CORE_A = \"M9907-23PC\"\n",
    "CORE_B = \"M9907-25PC\"\n",
    "\n",
    "# Load age constraints for both cores\n",
    "consider_adjacent_core = True\n",
    "age_data_a = load_age_constraints(CORE_A, consider_adjacent_core)\n",
    "age_data_b = load_age_constraints(CORE_B, consider_adjacent_core)\n",
    "\n",
    "# Extract variables for backward compatibility\n",
    "age_constraint_a_depths = age_data_a['depths']\n",
    "age_constraint_a_ages = age_data_a['ages']\n",
    "age_constraint_a_pos_errors = age_data_a['pos_errors']\n",
    "age_constraint_a_neg_errors = age_data_a['neg_errors']\n",
    "age_constraint_a_in_sequence_flags = age_data_a['in_sequence_flags']\n",
    "age_constraint_a_in_sequence_depths = age_data_a['in_sequence_depths']\n",
    "age_constraint_a_in_sequence_ages = age_data_a['in_sequence_ages']\n",
    "age_constraint_a_in_sequence_pos_errors = age_data_a['in_sequence_pos_errors']\n",
    "age_constraint_a_in_sequence_neg_errors = age_data_a['in_sequence_neg_errors']\n",
    "age_constraint_a_out_sequence_depths = age_data_a['out_sequence_depths']\n",
    "age_constraint_a_out_sequence_ages = age_data_a['out_sequence_ages']\n",
    "age_constraint_a_out_sequence_pos_errors = age_data_a['out_sequence_pos_errors']\n",
    "age_constraint_a_out_sequence_neg_errors = age_data_a['out_sequence_neg_errors']\n",
    "age_constraint_a_source_cores = age_data_a['core']\n",
    "age_constraint_a_interpreted_beds = age_data_a['interpreted_bed']\n",
    "\n",
    "age_constraint_b_depths = age_data_b['depths']\n",
    "age_constraint_b_ages = age_data_b['ages']\n",
    "age_constraint_b_pos_errors = age_data_b['pos_errors']\n",
    "age_constraint_b_neg_errors = age_data_b['neg_errors']\n",
    "age_constraint_b_in_sequence_flags = age_data_b['in_sequence_flags']\n",
    "age_constraint_b_in_sequence_depths = age_data_b['in_sequence_depths']\n",
    "age_constraint_b_in_sequence_ages = age_data_b['in_sequence_ages']\n",
    "age_constraint_b_in_sequence_pos_errors = age_data_b['in_sequence_pos_errors']\n",
    "age_constraint_b_in_sequence_neg_errors = age_data_b['in_sequence_neg_errors']\n",
    "age_constraint_b_out_sequence_depths = age_data_b['out_sequence_depths']\n",
    "age_constraint_b_out_sequence_ages = age_data_b['out_sequence_ages']\n",
    "age_constraint_b_out_sequence_pos_errors = age_data_b['out_sequence_pos_errors']\n",
    "age_constraint_b_out_sequence_neg_errors = age_data_b['out_sequence_neg_errors']\n",
    "age_constraint_b_source_cores = age_data_b['core']\n",
    "age_constraint_b_interpreted_beds = age_data_b['interpreted_bed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate interpolated ages for Core A using the function\n",
    "pickeddepth_ages_a = calculate_interpolated_ages(\n",
    "    picked_depths=all_depths_a_cat1,\n",
    "    age_constraints_depths=age_constraint_a_depths,\n",
    "    age_constraints_ages=age_constraint_a_ages,\n",
    "    age_constraints_pos_errors=age_constraint_a_pos_errors,\n",
    "    age_constraints_neg_errors=age_constraint_a_neg_errors,\n",
    "    age_constraints_in_sequence_flags=age_constraint_a_in_sequence_flags, # optional. If not provided, all age constraints are treated as in-sequence.\n",
    "    age_constraint_source_core=age_constraint_a_source_cores,\n",
    "    top_bottom=True, #whether to include top and bottom depths/ages in the results\n",
    "    top_age=0,  # Default age at top of core\n",
    "    top_age_pos_error=75,  # Default uncertainty of the top age\n",
    "    top_age_neg_error=75,  # Default uncertainty of the top age\n",
    "    top_depth=0.0,  # Assuming top of core is 0 cm depth\n",
    "    bottom_depth=md_a[-1],  # Max depth of core a\n",
    "    uncertainty_method='MonteCarlo', #'MonteCarlo', 'Linear', or 'Gaussian'. Default is 'MonteCarlo'. Linear is the most conservative.\n",
    "    n_monte_carlo=10000, #number of Monte Carlo sampling iterations. Default is 10000.\n",
    "    show_plot=True,\n",
    "    core_name=CORE_A,\n",
    "    export_csv=True\n",
    ")\n",
    "\n",
    "# Print the age constraint data for Core A\n",
    "print(\"\\nAge Constraints for Core A:\")\n",
    "if len(age_constraint_a_depths) > 0:\n",
    "    for i in range(len(age_constraint_a_depths)):\n",
    "        depth_val = age_constraint_a_depths.iloc[i] if isinstance(age_constraint_a_depths, pd.Series) else age_constraint_a_depths[i]\n",
    "        age_val = age_constraint_a_ages[i]\n",
    "        pos_err_val = age_constraint_a_pos_errors[i]\n",
    "        neg_err_val = age_constraint_a_neg_errors[i]\n",
    "        in_seq = age_constraint_a_in_sequence_flags[i]\n",
    "        \n",
    "        # Add source core and interpreted bed info if they exist\n",
    "        source_core_info = f\", Source Core: {age_constraint_a_source_cores[i]}\" if 'age_constraint_a_source_cores' in locals() and i < len(age_constraint_a_source_cores) else \"\"\n",
    "        bed_info = f\", Interpreted Bed: {age_constraint_a_interpreted_beds[i]}\" if 'age_constraint_a_interpreted_beds' in locals() and i < len(age_constraint_a_interpreted_beds) else \"\"\n",
    "        \n",
    "        print(f\"Depth: {depth_val:.2f} cm, Age: {age_val:.1f} years BP (+{pos_err_val:.1f} ; -{neg_err_val:.1f}), In Sequence: {in_seq}{source_core_info}{bed_info}\")\n",
    "else:\n",
    "    print(f\"No age constraints available in {CORE_A}\")\n",
    "\n",
    "# Print the interpolated ages\n",
    "print(f\"\\nEstimated Ages for picked depths in {CORE_A}:\")\n",
    "for i, depth in enumerate(pickeddepth_ages_a['depths']):\n",
    "    print(f\"Depth: {depth:.2f} cm, Age: {pickeddepth_ages_a['ages'][i]:.1f} years BP (+{pickeddepth_ages_a['pos_uncertainties'][i]:.1f} ; -{pickeddepth_ages_a['neg_uncertainties'][i]:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate interpolated ages for Core B using the function\n",
    "pickeddepth_ages_b = calculate_interpolated_ages(\n",
    "    picked_depths=all_depths_b_cat1,\n",
    "    age_constraints_depths=age_constraint_b_depths,\n",
    "    age_constraints_ages=age_constraint_b_ages,\n",
    "    age_constraints_pos_errors=age_constraint_b_pos_errors,\n",
    "    age_constraints_neg_errors=age_constraint_b_neg_errors,\n",
    "    age_constraints_in_sequence_flags=age_constraint_b_in_sequence_flags, # optional. If not provided, all age constraints are treated as in-sequence.\n",
    "    age_constraint_source_core=age_constraint_b_source_cores,\n",
    "    uncertainty_method='MonteCarlo', #'MonteCarlo', 'Linear', or 'Gaussian'. Default is 'MonteCarlo'. Linear is the most conservative.\n",
    "    n_monte_carlo=10000, #number of Monte Carlo sampling iterations. Default is 10000.\n",
    "    top_bottom=True, #whether to include top and bottom depths/ages in the results\n",
    "    top_age=0,  # Default age at top of core\n",
    "    top_age_pos_error=75,  # Default uncertainty of the top age\n",
    "    top_age_neg_error=75,  # Default uncertainty of the top age\n",
    "    top_depth=0.0,  # Assuming top of core is 0 cm depth\n",
    "    bottom_depth=md_b[-1],  # Max depth of core b\n",
    "    show_plot=True,\n",
    "    core_name=CORE_B,\n",
    "    export_csv=True\n",
    ")\n",
    "\n",
    "# Print the age constraint data for Core B\n",
    "print(\"\\nAge Constraints for Core B:\")\n",
    "if len(age_constraint_b_depths) > 0:\n",
    "    for i in range(len(age_constraint_b_depths)):\n",
    "        depth_val = age_constraint_b_depths.iloc[i] if isinstance(age_constraint_b_depths, pd.Series) else age_constraint_b_depths[i]\n",
    "        age_val = age_constraint_b_ages[i]\n",
    "        pos_err_val = age_constraint_b_pos_errors[i]\n",
    "        neg_err_val = age_constraint_b_neg_errors[i]\n",
    "        in_seq = age_constraint_b_in_sequence_flags[i]\n",
    "        \n",
    "        # Add source core and interpreted bed info if they exist\n",
    "        source_core_info = f\", Source Core: {age_constraint_b_source_cores[i]}\" if 'age_constraint_b_source_cores' in locals() and i < len(age_constraint_b_source_cores) else \"\"\n",
    "        bed_info = f\", Interpreted Bed: {age_constraint_b_interpreted_beds[i]}\" if 'age_constraint_b_interpreted_beds' in locals() and i < len(age_constraint_b_interpreted_beds) else \"\"\n",
    "        \n",
    "        print(f\"Depth: {depth_val:.2f} cm, Age: {age_val:.1f} years BP (+{pos_err_val:.1f} ; -{neg_err_val:.1f}), In Sequence: {in_seq}{source_core_info}{bed_info}\")\n",
    "else:\n",
    "    print(f\"No age constraints available in {CORE_B}\")\n",
    "\n",
    "print(f\"\\nEstimated Ages for picked depths in {CORE_B}:\")\n",
    "for i, depth in enumerate(pickeddepth_ages_b['depths']):\n",
    "    print(f\"Depth: {depth:.2f} cm, Age: {pickeddepth_ages_b['ages'][i]:.1f} years BP (+{pickeddepth_ages_b['pos_uncertainties'][i]:.1f} ; -{pickeddepth_ages_b['neg_uncertainties'][i]:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the age data from CSV files\n",
    "\n",
    "\n",
    "# Load age data for Core A\n",
    "core_a_age_csv = f\"{CORE_A}_pickeddepth_age.csv\"\n",
    "if os.path.exists(core_a_age_csv):\n",
    "    df_ages_a = pd.read_csv(core_a_age_csv)\n",
    "    pickeddepth_ages_a = {\n",
    "        'depths': df_ages_a['picked_depths_cm'].values.astype('float32').tolist(),\n",
    "        'ages': df_ages_a['est_age'].values.astype('float32').tolist(),\n",
    "        'pos_uncertainties': df_ages_a['est_age_poserr'].values.astype('float32').tolist(),\n",
    "        'neg_uncertainties': df_ages_a['est_age_negerr'].values.astype('float32').tolist()\n",
    "    }\n",
    "    print(f\"Loaded age data for {CORE_A} from CSV file\")\n",
    "else:\n",
    "    print(f\"Warning: Could not find age data CSV for {CORE_A}\")\n",
    "\n",
    "# Load age data for Core B\n",
    "core_b_age_csv = f\"{CORE_B}_pickeddepth_age.csv\"\n",
    "if os.path.exists(core_b_age_csv):\n",
    "    df_ages_b = pd.read_csv(core_b_age_csv)\n",
    "    pickeddepth_ages_b = {\n",
    "        'depths': df_ages_b['picked_depths_cm'].values.astype('float32').tolist(),\n",
    "        'ages': df_ages_b['est_age'].values.astype('float32').tolist(),\n",
    "        'pos_uncertainties': df_ages_b['est_age_poserr'].values.astype('float32').tolist(),\n",
    "        'neg_uncertainties': df_ages_b['est_age_negerr'].values.astype('float32').tolist()\n",
    "    }\n",
    "    print(f\"Loaded age data for {CORE_B} from CSV file\")\n",
    "else:\n",
    "    print(f\"Warning: Could not find age data CSV for {CORE_B}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find out all segment pairs among boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file names for age consideration or not\n",
    "age_consideration=True\n",
    "restricted_age_correlation=True\n",
    "\n",
    "if age_consideration:\n",
    "    if restricted_age_correlation:\n",
    "        YES_NO_AGE = 'restricted_age'\n",
    "    else:\n",
    "        YES_NO_AGE = 'loose_age'\n",
    "else:\n",
    "    if restricted_age_correlation:\n",
    "        YES_NO_AGE = 'no_age_restricted'\n",
    "    else:\n",
    "        YES_NO_AGE = 'no_age_full'\n",
    "\n",
    "# Define whether to use independent DTW or not\n",
    "independent_dtw=False # If False (default), it performs dependent DTW. If True, it performs independent DTW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Example usage:\n",
    "# Set picked_depths_a and picked_depths_b to None to use auto-segmentation\n",
    "\n",
    "# Define the folder path\n",
    "frames_folder = \"outputs/SegmentPair_DTW_frames\"\n",
    "\n",
    "# Check if the folder exists\n",
    "if os.path.exists(frames_folder):\n",
    "    # Get all PNG files in the folderf d\n",
    "    png_files = glob.glob(os.path.join(frames_folder, \"*.png\"))\n",
    "    \n",
    "    # Delete each PNG file\n",
    "    for png_file in png_files:\n",
    "        try:\n",
    "            os.remove(png_file)\n",
    "            # print(f\"Deleted: {png_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {png_file}: {e}\")\n",
    "    \n",
    "    print(f\"Cleaned up {len(png_files)} PNG files from {frames_folder}\")\n",
    "else:\n",
    "    print(f\"Folder '{frames_folder}' does not exist. Creating it...\")\n",
    "    os.makedirs(frames_folder, exist_ok=True)\n",
    "    \n",
    "# Run comprehensive DTW analysis\n",
    "dtw_results, valid_dtw_pairs, segments_a, segments_b, depth_boundaries_a, depth_boundaries_b, dtw_distance_matrix_full = run_comprehensive_dtw_analysis(\n",
    "    log_a, log_b, md_a, md_b, \n",
    "    picked_depths_a=all_depths_a_cat1, \n",
    "    picked_depths_b=all_depths_b_cat1,\n",
    "    top_bottom=True,\n",
    "    top_depth=0.0,\n",
    "    independent_dtw=independent_dtw,\n",
    "    exclude_deadend=True,\n",
    "    visualize_pairs=True,\n",
    "    visualize_segment_labels=False,\n",
    "    create_dtw_matrix=True, \n",
    "    dtwmatrix_output_filename=f'SegmentPair_DTW_matrix_{CORE_A}_{CORE_B}_{YES_NO_AGE}.png',\n",
    "    creategif=True,\n",
    "    gif_output_filename=f'SegmentPair_DTW_animation_{CORE_A}_{CORE_B}_{YES_NO_AGE}.gif',\n",
    "    max_frames=50,\n",
    "    color_interval_size=5,\n",
    "    keep_frames=True,\n",
    "    debug=False,\n",
    "    age_consideration=age_consideration,\n",
    "    ages_a=pickeddepth_ages_a,\n",
    "    ages_b=pickeddepth_ages_b,\n",
    "    restricted_age_correlation=restricted_age_correlation,\n",
    "    all_constraint_ages_a=age_constraint_a_in_sequence_ages,\n",
    "    all_constraint_ages_b=age_constraint_b_in_sequence_ages,\n",
    "    all_constraint_depths_a=age_constraint_a_in_sequence_depths,\n",
    "    all_constraint_depths_b=age_constraint_b_in_sequence_depths,\n",
    "    all_constraint_pos_errors_a=age_constraint_a_in_sequence_pos_errors,\n",
    "    all_constraint_pos_errors_b=age_constraint_b_in_sequence_pos_errors,\n",
    "    all_constraint_neg_errors_a=age_constraint_a_in_sequence_neg_errors,\n",
    "    all_constraint_neg_errors_b=age_constraint_b_in_sequence_neg_errors,\n",
    "    # Age constraint visualization parameters\n",
    "    age_constraint_a_source_cores=age_constraint_a_source_cores,\n",
    "    age_constraint_b_source_cores=age_constraint_b_source_cores,\n",
    "    core_a_name=CORE_A,\n",
    "    core_b_name=CORE_B\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_result = diagnose_chain_breaks(\n",
    "    valid_dtw_pairs, \n",
    "    segments_a, \n",
    "    segments_b, \n",
    "    depth_boundaries_a, \n",
    "    depth_boundaries_b\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_path_search_result = find_complete_core_paths(\n",
    "    valid_dtw_pairs, segments_a, segments_b,\n",
    "    log_a, log_b,\n",
    "    depth_boundaries_a, depth_boundaries_b,\n",
    "    dtw_results,\n",
    "    output_csv=f\"sequential_mappings_{CORE_A}_{CORE_B}_{YES_NO_AGE}.csv\",\n",
    "    start_from_top_only=True,\n",
    "    batch_size=1000,\n",
    "    n_jobs=-1,                 #Number of CPU cores to use for parallel processing. -1 means to use all available cores.\n",
    "    debug=False,\n",
    "    batch_grouping=False,      #A faster way to quickly assess complete paths for large datasets yet risk of missing several complete paths\n",
    "    n_groups=4,                #The larger the number, the faster the process but the higher risk of missing some complete paths,\n",
    "    shortest_path_search=True, #Minimizes the number of segments in the path - preferring less numbers of pinch outs\n",
    "    shortest_path_level=2,     #The higher the number, the more segments in the path - preferring more numbers of pinch outs\n",
    "    max_search_path=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "correlation_save_path=f'CombinedDTW_correlation_mappings_{CORE_A}_{CORE_B}_{YES_NO_AGE}.gif'\n",
    "matrix_save_path=f'CombinedDTW_matrix_mappings_{CORE_A}_{CORE_B}_{YES_NO_AGE}.gif'\n",
    "\n",
    "# 1. First, read all available mappings from a CSV (assuming it was created by find_all_sequential_mappings)\n",
    "sequential_mappings_csv = f\"outputs/sequential_mappings_{CORE_A}_{CORE_B}_{YES_NO_AGE}.csv\"\n",
    "\n",
    "# 3. Visualize a representative subset of the mappings\n",
    "visualize_dtw_results_from_csv(\n",
    "    sequential_mappings_csv,\n",
    "    log_a, log_b, md_a, md_b, \n",
    "    dtw_results, valid_dtw_pairs, \n",
    "    segments_a, segments_b, \n",
    "    depth_boundaries_a, depth_boundaries_b,\n",
    "    dtw_distance_matrix_full,\n",
    "    color_interval_size=5,\n",
    "    debug=False,\n",
    "    # GIF output\n",
    "    creategif=True,\n",
    "    correlation_gif_output_filename=correlation_save_path,\n",
    "    matrix_gif_output_filename=matrix_save_path,\n",
    "    max_frames=50,  # Limits visualization to 100 frames\n",
    "    keep_frames=True,\n",
    "    # Mark depths and ages\n",
    "    visualize_pairs=False,\n",
    "    visualize_segment_labels=False,\n",
    "    mark_depths=True,\n",
    "    mark_ages=True,\n",
    "    ages_a=pickeddepth_ages_a,  # Age data for core A\n",
    "    ages_b=pickeddepth_ages_b,  # Age data for core B\n",
    "    all_constraint_depths_a=age_constraint_a_in_sequence_depths,\n",
    "    all_constraint_depths_b=age_constraint_b_in_sequence_depths,\n",
    "    all_constraint_ages_a=age_constraint_a_in_sequence_ages,\n",
    "    all_constraint_ages_b=age_constraint_b_in_sequence_ages,\n",
    "    all_constraint_pos_errors_a=age_constraint_a_in_sequence_pos_errors,\n",
    "    all_constraint_pos_errors_b=age_constraint_b_in_sequence_pos_errors,\n",
    "    all_constraint_neg_errors_a=age_constraint_a_in_sequence_neg_errors,\n",
    "    all_constraint_neg_errors_b=age_constraint_b_in_sequence_neg_errors,\n",
    "    # ADD these new parameters:\n",
    "    age_constraint_a_source_cores=age_constraint_a_source_cores,\n",
    "    age_constraint_b_source_cores=age_constraint_b_source_cores,\n",
    "    core_a_name=CORE_A,\n",
    "    core_b_name=CORE_B\n",
    ")\n",
    "\n",
    "# Display the GIFs\n",
    "print(\"DTW Correlation Mappings GIF:\")\n",
    "display(IPImage(f\"outputs/{correlation_save_path}\"))\n",
    "\n",
    "print(\"DTW Matrix Mappings GIF:\")\n",
    "display(IPImage(f\"outputs/{matrix_save_path}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved DTW results\n",
    "sequential_mappings_csv = f'outputs/sequential_mappings_{CORE_A}_{CORE_B}_{YES_NO_AGE}.csv'\n",
    "output_matrix_png_filename = f'CombinedDTW_matrix_mappings_colored_{CORE_A}_{CORE_B}_{YES_NO_AGE}.png'\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "_ = plot_dtw_matrix_with_paths(\n",
    "    dtw_distance_matrix_full,\n",
    "    mode='all_paths_colored',\n",
    "    color_metric='perc_age_overlap', # Default is None, which means the mapping_id is used for coloring.\n",
    "    # Available quality indices: 'corr_coef', 'norm_dtw', 'dtw_ratio', 'perc_diag', 'variance_deviation', 'match_min', 'match_mean', 'perc_age_overlap'\n",
    "    sequential_mappings_csv=sequential_mappings_csv,\n",
    "    output_filename=output_matrix_png_filename,\n",
    "    n_jobs=-1,\n",
    "    # ADD these new parameters:\n",
    "    age_constraint_a_depths=age_constraint_a_in_sequence_depths,\n",
    "    age_constraint_a_ages=age_constraint_a_in_sequence_ages,\n",
    "    age_constraint_a_source_cores=age_constraint_a_source_cores,\n",
    "    age_constraint_b_depths=age_constraint_b_in_sequence_depths,\n",
    "    age_constraint_b_ages=age_constraint_b_in_sequence_ages,\n",
    "    age_constraint_b_source_cores=age_constraint_b_source_cores,\n",
    "    md_a=md_a,\n",
    "    md_b=md_b,\n",
    "    core_a_name=CORE_A,\n",
    "    core_b_name=CORE_B\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib widget\n",
    "\n",
    "# Load the DTW results\n",
    "sequential_mappings_csv = f'outputs/sequential_mappings_{CORE_A}_{CORE_B}_{YES_NO_AGE}.csv'\n",
    "dtw_results_df = pd.read_csv(sequential_mappings_csv)\n",
    "\n",
    "# Remove any infinite or NaN values\n",
    "dtw_results_df = dtw_results_df.replace([np.inf, -np.inf], np.nan).dropna(subset=['corr_coef', 'perc_diag', 'perc_age_overlap'])\n",
    "\n",
    "# Extract the three metrics we want to visualize\n",
    "corr_coef = dtw_results_df['corr_coef']\n",
    "perc_diag = dtw_results_df['perc_diag']\n",
    "perc_age_overlap = dtw_results_df['perc_age_overlap']\n",
    "\n",
    "# Calculate the 95th percentile for perc_diag\n",
    "perc_diag_threshold = np.percentile(perc_diag, 99.9)\n",
    "\n",
    "# Create a figure for 3D plotting\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Identify points with high percent diagonality\n",
    "high_diag_indices = perc_diag >= perc_diag_threshold\n",
    "high_diag_mappings = dtw_results_df.loc[high_diag_indices, 'mapping_id'].tolist()\n",
    "\n",
    "# Create the scatter plot\n",
    "# First plot all points in gray\n",
    "ax.scatter(corr_coef[~high_diag_indices], perc_age_overlap[~high_diag_indices], perc_diag[~high_diag_indices], \n",
    "           color='gray', s=10, alpha=0.3)\n",
    "\n",
    "# Then plot the high diagonality points in red\n",
    "ax.scatter(corr_coef[high_diag_indices], perc_age_overlap[high_diag_indices], perc_diag[high_diag_indices], \n",
    "           color='red', s=30, alpha=0.9)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Correlation Coefficient')\n",
    "ax.set_ylabel('Percent Age Overlap')\n",
    "ax.set_zlabel('Percent Diagonality')\n",
    "ax.set_title('3D Visualization of DTW Quality Indices')\n",
    "\n",
    "# Use the full range of the data for axis limits\n",
    "x_min, x_max = corr_coef.min(), corr_coef.max()\n",
    "y_min, y_max = perc_age_overlap.min(), perc_age_overlap.max()\n",
    "z_min, z_max = perc_diag.min(), perc_diag.max()\n",
    "\n",
    "# Add a tiny margin (0.5% of range) for aesthetic purposes\n",
    "x_margin = 0.005 * (x_max - x_min)\n",
    "y_margin = 0.005 * (y_max - y_min)\n",
    "z_margin = 0.005 * (z_max - z_min)\n",
    "\n",
    "# Set axis limits to cover the full data range\n",
    "ax.set_xlim(x_min - x_margin, x_max + x_margin)\n",
    "ax.set_ylim(y_min - y_margin, y_max + y_margin)\n",
    "ax.set_zlim(z_min - z_margin, z_max + z_margin)\n",
    "\n",
    "# Set the viewing angle to elevation=20°, azimuth=-30°\n",
    "ax.view_init(elev=25, azim=-15)\n",
    "\n",
    "# Add a legend\n",
    "ax.scatter([], [], [], c='red', s=50, label=f'High Diagonality (≥ {perc_diag_threshold:.1f}%)')\n",
    "ax.scatter([], [], [], c='gray', s=10, alpha=0.3, label='Other Points')\n",
    "ax.legend()\n",
    "\n",
    "# Get total mappings from complete_path_search_result\n",
    "total_mappings = complete_path_search_result['total_complete_paths_theoretical']\n",
    "\n",
    "# Add statistics as text on the plot\n",
    "stats_text = (\n",
    "    f\"Analyzed mappings: {len(dtw_results_df)}/{total_mappings}\\n\"\n",
    "    f\"High diagonality mappings: {len(high_diag_mappings)} ({len(high_diag_mappings)/len(dtw_results_df)*100:.1f}%)\"\n",
    ")\n",
    "plt.figtext(0.27, 0.825, stats_text, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print statistics and list of high diagonality mappings\n",
    "print(f\"=== High Diagonality Mappings (≥ {perc_diag_threshold:.1f}%) ===\")\n",
    "print(f\"Found {len(high_diag_mappings)} mappings out of {len(dtw_results_df)} total ({len(high_diag_mappings)/len(dtw_results_df)*100:.1f}%)\")\n",
    "print(\"\\nMapping IDs with high diagonality:\")\n",
    "\n",
    "# Get details for high diagonality mappings\n",
    "high_diag_df = dtw_results_df[high_diag_indices].sort_values(by='perc_diag', ascending=False)\n",
    "\n",
    "# Print detailed information about each high diagonality mapping\n",
    "for idx, row in high_diag_df.iterrows():\n",
    "    print(f\"Mapping ID {int(row['mapping_id'])}: perc_diag={row['perc_diag']:.1f}%, corr_coef={row['corr_coef']:.3f}, perc_age_overlap={row['perc_age_overlap']:.1f}%\")\n",
    "# Calculate a combined score for overall ranking\n",
    "print(\"\\n=== Top 5 Overall Best Mappings (Shortest DTW Path Length) ===\")\n",
    "\n",
    "# Filter for mappings with the shortest DTW path length\n",
    "if 'length' in dtw_results_df.columns:\n",
    "    # Use the length column directly from the CSV file\n",
    "    dtw_results_df['dtw_path_length'] = dtw_results_df['length']\n",
    "    \n",
    "    # Find the shortest DTW path length\n",
    "    min_length = dtw_results_df['dtw_path_length'].min()\n",
    "    shortest_mappings = dtw_results_df[dtw_results_df['dtw_path_length'] == min_length]\n",
    "else:\n",
    "    print(\"Warning: No 'length' column found in the dataframe. Using all mappings.\")\n",
    "    shortest_mappings = dtw_results_df\n",
    "    min_length = \"N/A\"\n",
    "\n",
    "print(f\"Considering only the shortest DTW path length mappings\")\n",
    "print(f\"Number of mappings considered: {len(shortest_mappings)} out of {len(dtw_results_df)}\")\n",
    "print(f\"DTW path length: {min_length}\")\n",
    "\n",
    "# Create a copy for scoring calculations\n",
    "df_for_ranking = shortest_mappings.copy()\n",
    "\n",
    "# Define the metrics to use for scoring\n",
    "metrics = {\n",
    "    'perc_diag': {'higher_is_better': True, 'weight': 1.0},\n",
    "    'norm_dtw': {'higher_is_better': False, 'weight': 1.0},\n",
    "    'dtw_ratio': {'higher_is_better': False, 'weight': 0.0},\n",
    "    'corr_coef': {'higher_is_better': True, 'weight': 3.0},\n",
    "    'wrapping_deviation': {'higher_is_better': False, 'weight': 0.0},\n",
    "    'mean_matching_function': {'higher_is_better': False, 'weight': 0.0},\n",
    "    'perc_age_overlap': {'higher_is_better': True, 'weight': 1.0}\n",
    "}\n",
    "\n",
    "# Initialize the combined score column\n",
    "df_for_ranking['combined_score'] = 0.0\n",
    "total_weight = 0.0\n",
    "\n",
    "# Calculate scores for each metric and add to combined score\n",
    "for metric, props in metrics.items():\n",
    "    if metric in df_for_ranking.columns:\n",
    "        # Make sure we have valid data to work with\n",
    "        valid_data = df_for_ranking[~df_for_ranking[metric].isna()]\n",
    "        \n",
    "        if len(valid_data) > 0:\n",
    "            min_val = valid_data[metric].min()\n",
    "            max_val = valid_data[metric].max()\n",
    "            \n",
    "            # Only normalize if there's a range\n",
    "            if max_val > min_val:\n",
    "                if props['higher_is_better']:\n",
    "                    # For metrics where higher values are better (like perc_diag, correlation)\n",
    "                    df_for_ranking[f'{metric}_score'] = (df_for_ranking[metric] - min_val) / (max_val - min_val)\n",
    "                else:\n",
    "                    # For metrics where lower values are better (like norm_dtw, dtw_ratio)\n",
    "                    df_for_ranking[f'{metric}_score'] = 1 - ((df_for_ranking[metric] - min_val) / (max_val - min_val))\n",
    "            else:\n",
    "                # If all values are the same, assign a score of 1\n",
    "                df_for_ranking[f'{metric}_score'] = 1.0\n",
    "                \n",
    "            # Add to weighted sum\n",
    "            weight = props['weight']\n",
    "            df_for_ranking['combined_score'] += df_for_ranking[f'{metric}_score'].fillna(0) * weight\n",
    "            total_weight += weight\n",
    "\n",
    "# Normalize the combined score by the total weight\n",
    "if total_weight > 0:\n",
    "    df_for_ranking['combined_score'] = df_for_ranking['combined_score'] / total_weight\n",
    "else:\n",
    "    print(\"Warning: No valid metrics found for scoring.\")\n",
    "    df_for_ranking['combined_score'] = 0.0\n",
    "\n",
    "# Verify we don't have NaN values in the combined score\n",
    "if df_for_ranking['combined_score'].isna().any():\n",
    "    print(\"Warning: NaN values detected in combined scores. Replacing with zeros.\")\n",
    "    df_for_ranking['combined_score'] = df_for_ranking['combined_score'].fillna(0)\n",
    "\n",
    "# Get top 5 mappings by combined score\n",
    "top_overall = df_for_ranking.sort_values(by='combined_score', ascending=False).head(5)\n",
    "\n",
    "print(\"\\nTop 5 mappings considering all metrics combined (with higher weights for perc_diag, norm_dtw, and dtw_ratio):\")\n",
    "for idx, row in top_overall.iterrows():\n",
    "    if 'mapping_id' in row:\n",
    "        print(f\"Mapping ID {int(row['mapping_id'])}: Combined Score={row['combined_score']:.3f}\")\n",
    "    if 'dtw_path_length' in row:\n",
    "        print(f\"  dtw_path_length={row['dtw_path_length']:.1f}\")\n",
    "    if 'corr_coef' in row:\n",
    "        print(f\"  correlation coefficient r={row['corr_coef']:.3f}\")\n",
    "    if 'perc_diag' in row:\n",
    "        print(f\"  perc_diag={row['perc_diag']:.1f}%\")\n",
    "    if 'norm_dtw' in row:\n",
    "        print(f\"  norm_dtw={row['norm_dtw']:.3f}\")\n",
    "    if 'dtw_ratio' in row:\n",
    "        print(f\"  dtw_ratio={row['dtw_ratio']:.3f}\")\n",
    "\n",
    "\n",
    "    # Print post_wrap_corr\n",
    "\n",
    "    # Print additional metrics if available\n",
    "    if 'perc_age_overlap' in row:\n",
    "        print(f\"  perc_age_overlap={row['perc_age_overlap']:.1f}%\")\n",
    "    if 'wrapping_deviation' in row:\n",
    "        print(f\"  wrapping_deviation={row['wrapping_deviation']:.3f}\")\n",
    "    if 'post_wrap_corr' in row:\n",
    "        print(f\"  post_wrap_corr={row['post_wrap_corr']:.3f}\")\n",
    "    if 'mean_matching_function' in row:\n",
    "        print(f\"  mean_matching_function={row['mean_matching_function']:.3f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Save the top 1 mapping ID from the best overall combined score\n",
    "if not top_overall.empty:\n",
    "    best_mapping_id = int(top_overall.iloc[0]['mapping_id'])\n",
    "    print(f\"\\nSaving best mapping ID: {best_mapping_id}\")\n",
    "else:\n",
    "    best_mapping_id = None\n",
    "    print(\"\\nWarning: No valid mappings found to save as best mapping ID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a list of segment pairs to combine (1-based index). For example: [(1,2), (4,5), ...]\n",
    "\n",
    "# Load the sequential mappings from the CSV file\n",
    "\n",
    "\n",
    "def parse_compact_path(compact_path_str):\n",
    "    \"\"\"Parse compact path format \"2,3;4,5;6,7\" back to list of tuples\"\"\"\n",
    "    if not compact_path_str or compact_path_str == \"\":\n",
    "        return []\n",
    "    return [tuple(map(int, pair.split(','))) for pair in compact_path_str.split(';')]\n",
    "\n",
    "csv_file = f'outputs/sequential_mappings_{CORE_A}_{CORE_B}_{YES_NO_AGE}.csv'\n",
    "target_mapping_id = best_mapping_id\n",
    "target_data_row = None\n",
    "\n",
    "with open(csv_file, newline='') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        if int(row['mapping_id']) == target_mapping_id:\n",
    "            # UPDATED: Parse compact format instead of ast.literal_eval\n",
    "            target_data_row = parse_compact_path(row['path'])\n",
    "            break\n",
    "\n",
    "if target_data_row is None:\n",
    "    raise KeyError(f\"Mapping ID {target_mapping_id} not found in {csv_file}\")\n",
    "\n",
    "# convert 1-based to 0-based indices for python\n",
    "valid_pairs_to_combine = [(a-1, b-1) for a, b in target_data_row]\n",
    "\n",
    "print(\"Using mapping ID\", target_mapping_id)\n",
    "print(target_data_row)\n",
    "\n",
    "# convert 1-based to 0-based indices for python\n",
    "valid_pairs_to_combine = [(a-1, b-1) for a, b in target_data_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "visualize_pairs=False\n",
    "\n",
    "if visualize_pairs:\n",
    "    visualize_type='pairs'\n",
    "    visualize_segment_labels=True\n",
    "    mark_depths=True\n",
    "else:\n",
    "    visualize_type='fullpath'\n",
    "    visualize_segment_labels=False\n",
    "    mark_depths=False\n",
    "\n",
    "# Visualize the combined segments\n",
    "combined_wp, combined_quality, _, _ = visualize_combined_segments(\n",
    "    log_a, log_b, md_a, md_b, \n",
    "    dtw_results, valid_dtw_pairs, \n",
    "    segments_a, segments_b, \n",
    "    depth_boundaries_a, depth_boundaries_b,\n",
    "    dtw_distance_matrix_full,\n",
    "    valid_pairs_to_combine,\n",
    "    color_interval_size=5,\n",
    "    visualize_pairs=visualize_pairs,\n",
    "    visualize_segment_labels=visualize_segment_labels,\n",
    "    correlation_save_path=f'CombinedDTW_correlation_{CORE_A}_{CORE_B}_{YES_NO_AGE}_{target_mapping_id}_{visualize_type}.png',\n",
    "    matrix_save_path=f'CombinedDTW_matrix_{CORE_A}_{CORE_B}_{YES_NO_AGE}_{target_mapping_id}_{visualize_type}.png',\n",
    "    mark_depths=mark_depths,\n",
    "    mark_ages=True,\n",
    "    ages_a=pickeddepth_ages_a,\n",
    "    ages_b=pickeddepth_ages_b,\n",
    "    all_constraint_ages_a=age_constraint_a_in_sequence_ages,\n",
    "    all_constraint_ages_b=age_constraint_b_in_sequence_ages,\n",
    "    all_constraint_depths_a=age_constraint_a_in_sequence_depths,\n",
    "    all_constraint_depths_b=age_constraint_b_in_sequence_depths,\n",
    "    all_constraint_pos_errors_a=age_constraint_a_in_sequence_pos_errors,\n",
    "    all_constraint_pos_errors_b=age_constraint_b_in_sequence_pos_errors,\n",
    "    all_constraint_neg_errors_a=age_constraint_a_in_sequence_neg_errors,\n",
    "    all_constraint_neg_errors_b=age_constraint_b_in_sequence_neg_errors,\n",
    "    age_constraint_a_source_cores=age_constraint_a_source_cores,\n",
    "    age_constraint_b_source_cores=age_constraint_b_source_cores,\n",
    "    core_a_name=CORE_A,\n",
    "    core_b_name=CORE_B\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for the file naming\n",
    "CORE_A = \"M9907-23PC\"\n",
    "CORE_B = \"M9907-25PC\"\n",
    "\n",
    "# YES_NO_AGE = \"loose_age\"\n",
    "\n",
    "if age_consideration:\n",
    "    if restricted_age_correlation:\n",
    "        YES_NO_AGE = \"restricted_age\"\n",
    "    else:\n",
    "        YES_NO_AGE = \"loose_age\"\n",
    "\n",
    "targeted_quality_index = 'corr_coef' \n",
    "# Available quality indices: 'corr_coef', 'norm_dtw', 'dtw_ratio', 'perc_diag', 'variance_deviation', 'match_min', 'match_mean', 'perc_age_overlap'\n",
    "\n",
    "# Example usage:\n",
    "plot_correlation_distribution(f'outputs/sequential_mappings_{CORE_A}_{CORE_B}_{YES_NO_AGE}.csv', \n",
    "                              target_mapping_id, \n",
    "                              targeted_quality_index,\n",
    "                              no_bins=None,\n",
    "                              save_png=True,\n",
    "                              png_filename=f'{\"r-values\" if targeted_quality_index == \"corr_coef\" else targeted_quality_index}_distribution_{CORE_A}_{CORE_B}_{YES_NO_AGE}.png',\n",
    "                              pdf_method='skew-normal', #'KDE', 'skew-normal', 'normal'\n",
    "                              kde_bandwidth=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "11-TimeSeriesCorrelation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
