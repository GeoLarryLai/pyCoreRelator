{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25kTe2nS84Tf"
   },
   "source": [
    "# Correlating well log pairs: Complex Dynamic Time Warping with boundary constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4awuK6ffO2t"
   },
   "source": [
    "## Introduction to dynamic time warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "47Kk07X_84Th"
   },
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import glob\n",
    "from IPython.display import Image as IPImage, display\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyCoreRelator import (\n",
    "    run_comprehensive_dtw_analysis,\n",
    "    find_complete_core_paths,\n",
    "    diagnose_chain_breaks,\n",
    "    calculate_interpolated_ages,\n",
    "    visualize_combined_segments,\n",
    "    visualize_dtw_results_from_csv,\n",
    "    load_log_data,\n",
    "    plot_core_data,\n",
    "    plot_dtw_matrix_with_paths,\n",
    "    plot_correlation_distribution,\n",
    "    find_best_mappings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Test with Cascadia hi-res MS logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define core pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define core names as variables for easy reference\n",
    "# CORE_A = \"M9907-22PC\"\n",
    "# CORE_B = \"M9907-23PC\"\n",
    "CORE_A = \"M9907-23PC\"\n",
    "# CORE_B = \"M9907-25PC\"\n",
    "# CORE_B = \"M9907-12PC\"\n",
    "CORE_B = \"RR0207-56PC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data structures and core images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log columns to extract\n",
    "# LOG_COLUMNS = ['hiresMS', 'CT', 'R', 'G', 'B']  # Choose which logs to include\n",
    "# LOG_COLUMNS = ['hiresMS', 'CT', 'Lumin']  # Choose which logs to include\n",
    "LOG_COLUMNS = ['hiresMS']  # Choose which logs to include\n",
    "DEPTH_COLUMN = 'SB_DEPTH_cm'\n",
    "\n",
    "# Define directory paths\n",
    "mother_dir = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/'\n",
    "\n",
    "# Define paths for Core A\n",
    "core_a_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_RGB_MLfilled.csv',\n",
    "    'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_A}/ML_filled/{CORE_A}_MST_MLfilled.csv'\n",
    "}\n",
    "\n",
    "core_a_rgb_img_path = f\"{mother_dir}_compiled_logs/{CORE_A}/{CORE_A}_RGB.tiff\"\n",
    "core_a_ct_img_path = f\"{mother_dir}_compiled_logs/{CORE_A}/{CORE_A}_CT.tiff\"\n",
    "\n",
    "# Define paths for Core B\n",
    "core_b_log_paths = {\n",
    "    'hiresMS': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_CT_MLfilled.csv',\n",
    "    'Lumin': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'R': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'G': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'B': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_RGB_MLfilled.csv',\n",
    "    'Den_gm/cc': f'{mother_dir}_compiled_logs/{CORE_B}/ML_filled/{CORE_B}_MST_MLfilled.csv'\n",
    "}\n",
    "core_b_rgb_img_path = f\"{mother_dir}_compiled_logs/{CORE_B}/{CORE_B}_RGB.tiff\"\n",
    "core_b_ct_img_path = f\"{mother_dir}_compiled_logs/{CORE_B}/{CORE_B}_CT.tiff\"\n",
    "\n",
    "# Define column mapping for alternative column names\n",
    "column_alternatives = {\n",
    "    'hiresMS': ['MS'],\n",
    "    'CT': ['CT_value'],\n",
    "    'R': ['R', 'red', 'Red'],\n",
    "    'G': ['G', 'green', 'Green'],\n",
    "    'B': ['B', 'blue', 'Blue'],\n",
    "    'Lumin': ['luminance', 'Luminance'],\n",
    "    'Den_gm/cc': ['Density', 'density']\n",
    "}\n",
    "\n",
    "# Load data for Core A\n",
    "log_a, md_a, available_columns_a, rgb_img_a, ct_img_a = load_log_data(\n",
    "    core_a_log_paths,\n",
    "    {'rgb': core_a_rgb_img_path, 'ct': core_a_ct_img_path},\n",
    "    LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")\n",
    "\n",
    "print(\"\\n=== DEBUG: Core A Loading ===\")\n",
    "print(f\"LOG_COLUMNS requested: {LOG_COLUMNS}\")\n",
    "print(f\"Available columns loaded: {available_columns_a}\")\n",
    "print(f\"Shape of log_a: {log_a.shape}\")\n",
    "print(f\"Type of log_a: {type(log_a)}\")\n",
    "if hasattr(log_a, 'ndim'):\n",
    "    print(f\"log_a dimensions: {log_a.ndim}\")\n",
    "    if log_a.ndim > 1:\n",
    "        print(f\"log_a has {log_a.shape[1]} columns\\n\")\n",
    "    else:\n",
    "        print(\"log_a is 1D (single column)\\n\")\n",
    "\n",
    "# Load data for Core B\n",
    "log_b, md_b, available_columns_b, rgb_img_b, ct_img_b = load_log_data(\n",
    "    core_b_log_paths,\n",
    "    {'rgb': core_b_rgb_img_path, 'ct': core_b_ct_img_path},\n",
    "    LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")\n",
    "\n",
    "print(\"\\n=== DEBUG: Core B Loading ===\")\n",
    "print(f\"LOG_COLUMNS requested: {LOG_COLUMNS}\")\n",
    "print(f\"Available columns loaded: {available_columns_b}\")\n",
    "print(f\"Shape of log_b: {log_b.shape}\")\n",
    "print(f\"Type of log_b: {type(log_b)}\")\n",
    "if hasattr(log_b, 'ndim'):\n",
    "    print(f\"log_b dimensions: {log_b.ndim}\")\n",
    "    if log_b.ndim > 1:\n",
    "        print(f\"log_b has {log_b.shape[1]} columns\\n\")\n",
    "    else:\n",
    "        print(\"log_b is 1D (single column)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Define paths to the CSV files\n",
    "pickeddepth_a_csv = f'pickeddepth/{CORE_A}_pickeddepth.csv'\n",
    "pickeddepth_b_csv = f'pickeddepth/{CORE_B}_pickeddepth.csv'\n",
    "\n",
    "# Load picked depths and categories from CSV files\n",
    "if os.path.exists(pickeddepth_b_csv):\n",
    "    picked_data_b = pd.read_csv(pickeddepth_b_csv)\n",
    "    # Combine depths and categories into tuples\n",
    "    picked_b = list(zip(picked_data_b['picked_depths_cm'].values.tolist(), \n",
    "                        picked_data_b['category'].values.tolist()))\n",
    "    print(f\"Loaded {len(picked_b)} picked depths for {CORE_B}\")\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_b_csv} not found. Using empty list for picked_b.\")\n",
    "    picked_b = []\n",
    "\n",
    "if os.path.exists(pickeddepth_a_csv):\n",
    "    picked_data_a = pd.read_csv(pickeddepth_a_csv)\n",
    "    # Combine depths and categories into tuples\n",
    "    picked_a = list(zip(picked_data_a['picked_depths_cm'].values.tolist(), \n",
    "                        picked_data_a['category'].values.tolist()))\n",
    "    print(f\"Loaded {len(picked_a)} picked depths for {CORE_A}\")\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_a_csv} not found. Using empty list for picked_a.\")\n",
    "    picked_a = []\n",
    "\n",
    "# Create uncertainty arrays (assuming uncertainty size is 2 cm)\n",
    "picked_uncertainty_b = [1] * len(picked_b)\n",
    "picked_uncertainty_a = [1] * len(picked_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract depths and categories from the loaded tuples\n",
    "picked_depths_a = [depth for depth, category in picked_a] if picked_a else []\n",
    "picked_categories_a = [category for depth, category in picked_a] if picked_a else []\n",
    "\n",
    "picked_depths_b = [depth for depth, category in picked_b] if picked_b else []\n",
    "picked_categories_b = [category for depth, category in picked_b] if picked_b else []\n",
    "\n",
    "# Now plot the cores with enhanced plot_core_data function\n",
    "is_multilog = log_a.ndim > 1 and log_a.shape[1] > 1\n",
    "fig_a, ax_a = plot_core_data(\n",
    "    # Input data\n",
    "    md_a,                                           # depth array\n",
    "    log_a,                                          # log data array\n",
    "    f\"{CORE_A}\",                                    # core name\n",
    "    # Image data\n",
    "    rgb_img=rgb_img_a,                              # RGB image array\n",
    "    ct_img=ct_img_a,                                # CT scan image array\n",
    "    # Data configuration\n",
    "    available_columns=available_columns_a,          # available data columns\n",
    "    is_multilog=is_multilog,                        # multi-column log flag\n",
    "    # Picked depth data\n",
    "    picked_depths=picked_depths_a,                  # picked depth values\n",
    "    picked_categories=picked_categories_a,          # picked categories\n",
    "    picked_uncertainties=picked_uncertainty_a,      # uncertainty values\n",
    "    # Visualization settings\n",
    "    figsize=(20, 4),                                # figure size (width, height)\n",
    "    show_category=[1],                              # categories to display\n",
    "    show_bed_number=True                            # show bed numbers flag\n",
    ")\n",
    "\n",
    "# Do the same for Core B\n",
    "is_multilog = log_b.ndim > 1 and log_b.shape[1] > 1\n",
    "fig_b, ax_b = plot_core_data(\n",
    "    # Input data\n",
    "    md_b,                                           # depth array\n",
    "    log_b,                                          # log data array\n",
    "    f\"{CORE_B}\",                                    # core name\n",
    "    # Image data\n",
    "    rgb_img=rgb_img_b,                              # RGB image array\n",
    "    ct_img=ct_img_b,                                # CT scan image array\n",
    "    # Data configuration\n",
    "    available_columns=available_columns_b,          # available data columns\n",
    "    is_multilog=is_multilog,                        # multi-column log flag\n",
    "    # Picked depth data\n",
    "    picked_depths=picked_depths_b,                  # picked depth values\n",
    "    picked_categories=picked_categories_b,          # picked categories\n",
    "    picked_uncertainties=picked_uncertainty_b,      # uncertainty values\n",
    "    # Visualization settings\n",
    "    figsize=(20, 4),                                # figure size (width, height)\n",
    "    show_category=[1],                              # categories to display\n",
    "    show_bed_number=True                            # show bed numbers flag\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIONAL: Extract top 200 cm of log data, images, picked depths and rename cores\n",
    "# # Comment out this cell to use full core data with original names\n",
    "\n",
    "# # Define depth cutoff\n",
    "# DEPTH_CUTOFF = 250.0  # cm\n",
    "\n",
    "# # First, need to reset the original core names for CSV file loading\n",
    "# # (since the previous cell may have already renamed them)\n",
    "# ORIGINAL_CORE_A = \"M9907-23PC\"\n",
    "# ORIGINAL_CORE_B = \"M9907-25PC\"\n",
    "\n",
    "# # Reload the original log data before clipping\n",
    "# mother_dir = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/'\n",
    "# # LOG_COLUMNS = ['hiresMS']  \n",
    "# LOG_COLUMNS = ['hiresMS', 'CT', 'Lumin'] \n",
    "# DEPTH_COLUMN = 'SB_DEPTH_cm'\n",
    "\n",
    "# # Define paths for original cores\n",
    "# core_a_log_paths = {\n",
    "#     'hiresMS': f'{mother_dir}_compiled_logs/{ORIGINAL_CORE_A}/ML_filled/{ORIGINAL_CORE_A}_hiresMS_MLfilled.csv',\n",
    "#     'CT': f'{mother_dir}_compiled_logs/{ORIGINAL_CORE_A}/ML_filled/{ORIGINAL_CORE_A}_CT_MLfilled.csv',\n",
    "#     'Lumin': f'{mother_dir}_compiled_logs/{ORIGINAL_CORE_A}/ML_filled/{ORIGINAL_CORE_A}_RGB_MLfilled.csv',\n",
    "#     'R': f'{mother_dir}_compiled_logs/{ORIGINAL_CORE_A}/ML_filled/{ORIGINAL_CORE_A}_RGB_MLfilled.csv',\n",
    "#     'G': f'{mother_dir}_compiled_logs/{ORIGINAL_CORE_A}/ML_filled/{ORIGINAL_CORE_A}_RGB_MLfilled.csv',\n",
    "#     'B': f'{mother_dir}_compiled_logs/{ORIGINAL_CORE_A}/ML_filled/{ORIGINAL_CORE_A}_RGB_MLfilled.csv',\n",
    "#     'Den_gm/cc': f'{mother_dir}_compiled_logs/{ORIGINAL_CORE_A}/ML_filled/{ORIGINAL_CORE_A}_MST_MLfilled.csv'\n",
    "# }\n",
    "\n",
    "# core_b_log_paths = {\n",
    "#     'hiresMS': f'{mother_dir}_compiled_logs/{ORIGINAL_CORE_B}/ML_filled/{ORIGINAL_CORE_B}_hiresMS_MLfilled.csv',\n",
    "#     'CT': f'{mother_dir}_compiled_logs/{ORIGINAL_CORE_B}/ML_filled/{ORIGINAL_CORE_B}_CT_MLfilled.csv',\n",
    "#     'Lumin': f'{mother_dir}_compiled_logs/{ORIGINAL_CORE_B}/ML_filled/{ORIGINAL_CORE_B}_RGB_MLfilled.csv',\n",
    "#     'R': f'{mother_dir}_compiled_logs/{ORIGINAL_CORE_B}/ML_filled/{ORIGINAL_CORE_B}_RGB_MLfilled.csv',\n",
    "#     'G': f'{mother_dir}_compiled_logs/{ORIGINAL_CORE_B}/ML_filled/{ORIGINAL_CORE_B}_RGB_MLfilled.csv',\n",
    "#     'B': f'{mother_dir}_compiled_logs/{ORIGINAL_CORE_B}/ML_filled/{ORIGINAL_CORE_B}_RGB_MLfilled.csv',\n",
    "#     'Den_gm/cc': f'{mother_dir}_compiled_logs/{ORIGINAL_CORE_B}/ML_filled/{ORIGINAL_CORE_B}_MST_MLfilled.csv'\n",
    "# }\n",
    "\n",
    "# core_a_rgb_img_path = f\"{mother_dir}_compiled_logs/{ORIGINAL_CORE_A}/{ORIGINAL_CORE_A}_RGB.tiff\"\n",
    "# core_a_ct_img_path = f\"{mother_dir}_compiled_logs/{ORIGINAL_CORE_A}/{ORIGINAL_CORE_A}_CT.tiff\"\n",
    "# core_b_rgb_img_path = f\"{mother_dir}_compiled_logs/{ORIGINAL_CORE_B}/{ORIGINAL_CORE_B}_RGB.tiff\"\n",
    "# core_b_ct_img_path = f\"{mother_dir}_compiled_logs/{ORIGINAL_CORE_B}/{ORIGINAL_CORE_B}_CT.tiff\"\n",
    "\n",
    "# column_alternatives = {\n",
    "#     'hiresMS': ['MS'],\n",
    "#     'CT': ['CT_value'],\n",
    "#     'R': ['R', 'red', 'Red'],\n",
    "#     'G': ['G', 'green', 'Green'],\n",
    "#     'B': ['B', 'blue', 'Blue'],\n",
    "#     'Lumin': ['luminance', 'Luminance'],\n",
    "#     'Den_gm/cc': ['Density', 'density']\n",
    "# }\n",
    "\n",
    "# # Reload original full data (with normalize=False to get raw data)\n",
    "# log_a_orig, md_a_orig, available_columns_a, rgb_img_a_orig, ct_img_a_orig = load_log_data(\n",
    "#     core_a_log_paths,\n",
    "#     {'rgb': core_a_rgb_img_path, 'ct': core_a_ct_img_path},\n",
    "#     LOG_COLUMNS,\n",
    "#     depth_column=DEPTH_COLUMN,\n",
    "#     normalize=False,\n",
    "#     column_alternatives=column_alternatives\n",
    "# )\n",
    "\n",
    "# log_b_orig, md_b_orig, available_columns_b, rgb_img_b_orig, ct_img_b_orig = load_log_data(\n",
    "#     core_b_log_paths,\n",
    "#     {'rgb': core_b_rgb_img_path, 'ct': core_b_ct_img_path},\n",
    "#     LOG_COLUMNS,\n",
    "#     depth_column=DEPTH_COLUMN,\n",
    "#     normalize=False,\n",
    "#     column_alternatives=column_alternatives\n",
    "# )\n",
    "\n",
    "# # Extract top 200 cm for Core A\n",
    "# cutoff_mask_a = md_a_orig <= DEPTH_CUTOFF\n",
    "# log_a_raw = log_a_orig[cutoff_mask_a]\n",
    "# md_a = md_a_orig[cutoff_mask_a]\n",
    "\n",
    "# # Extract top 200 cm for Core B\n",
    "# cutoff_mask_b = md_b_orig <= DEPTH_CUTOFF\n",
    "# log_b_raw = log_b_orig[cutoff_mask_b]\n",
    "# md_b = md_b_orig[cutoff_mask_b]\n",
    "\n",
    "# # Renormalize the clipped log signals\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Initialize scaler\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # For multi-column data\n",
    "# if log_a_raw.ndim > 1:\n",
    "#     log_a = scaler.fit_transform(log_a_raw)\n",
    "# else:\n",
    "#     log_a = scaler.fit_transform(log_a_raw.reshape(-1, 1)).flatten()\n",
    "\n",
    "# if log_b_raw.ndim > 1:\n",
    "#     log_b = scaler.fit_transform(log_b_raw)\n",
    "# else:\n",
    "#     log_b = scaler.fit_transform(log_b_raw.reshape(-1, 1)).flatten()\n",
    "\n",
    "# # Clip images proportionally\n",
    "# def clip_image(img, max_depth_orig, depth_cutoff):\n",
    "#     if img is not None:\n",
    "#         clip_ratio = min(1.0, depth_cutoff / max_depth_orig)\n",
    "#         if img.ndim == 3:  # Color image\n",
    "#             clip_height = int(img.shape[0] * clip_ratio)\n",
    "#             return img[:clip_height, :, :]\n",
    "#         elif img.ndim == 2:  # Grayscale image\n",
    "#             clip_height = int(img.shape[0] * clip_ratio)\n",
    "#             return img[:clip_height, :]\n",
    "#     return img\n",
    "\n",
    "# # Clip images for both cores\n",
    "# rgb_img_a = clip_image(rgb_img_a_orig, md_a_orig.max(), DEPTH_CUTOFF)\n",
    "# ct_img_a = clip_image(ct_img_a_orig, md_a_orig.max(), DEPTH_CUTOFF)\n",
    "# rgb_img_b = clip_image(rgb_img_b_orig, md_b_orig.max(), DEPTH_CUTOFF)\n",
    "# ct_img_b = clip_image(ct_img_b_orig, md_b_orig.max(), DEPTH_CUTOFF)\n",
    "\n",
    "# # Reload picked depths using original core names\n",
    "# pickeddepth_a_csv = f'pickeddepth/{ORIGINAL_CORE_A}_pickeddepth.csv'\n",
    "# pickeddepth_b_csv = f'pickeddepth/{ORIGINAL_CORE_B}_pickeddepth.csv'\n",
    "\n",
    "# # Load and filter picked depths for Core A\n",
    "# if os.path.exists(pickeddepth_a_csv):\n",
    "#     picked_data_a = pd.read_csv(pickeddepth_a_csv)\n",
    "#     # Filter to depths within cutoff\n",
    "#     picked_data_a_filtered = picked_data_a[picked_data_a['picked_depths_cm'] <= DEPTH_CUTOFF]\n",
    "#     picked_a = list(zip(picked_data_a_filtered['picked_depths_cm'].values.tolist(), \n",
    "#                         picked_data_a_filtered['category'].values.tolist()))\n",
    "#     print(f\"Loaded {len(picked_a)} picked depths (≤{DEPTH_CUTOFF} cm) for {ORIGINAL_CORE_A}\")\n",
    "# else:\n",
    "#     print(f\"Warning: {pickeddepth_a_csv} not found. Using empty list for picked_a.\")\n",
    "#     picked_a = []\n",
    "\n",
    "# # Load and filter picked depths for Core B\n",
    "# if os.path.exists(pickeddepth_b_csv):\n",
    "#     picked_data_b = pd.read_csv(pickeddepth_b_csv)\n",
    "#     # Filter to depths within cutoff\n",
    "#     picked_data_b_filtered = picked_data_b[picked_data_b['picked_depths_cm'] <= DEPTH_CUTOFF]\n",
    "#     picked_b = list(zip(picked_data_b_filtered['picked_depths_cm'].values.tolist(), \n",
    "#                         picked_data_b_filtered['category'].values.tolist()))\n",
    "#     print(f\"Loaded {len(picked_b)} picked depths (≤{DEPTH_CUTOFF} cm) for {ORIGINAL_CORE_B}\")\n",
    "# else:\n",
    "#     print(f\"Warning: {pickeddepth_b_csv} not found. Using empty list for picked_b.\")\n",
    "#     picked_b = []\n",
    "\n",
    "# # Update uncertainty arrays\n",
    "# picked_uncertainty_a = [1] * len(picked_a)\n",
    "# picked_uncertainty_b = [1] * len(picked_b)\n",
    "\n",
    "# # Extract filtered depths and categories\n",
    "# picked_depths_a = [depth for depth, category in picked_a] if picked_a else []\n",
    "# picked_categories_a = [category for depth, category in picked_a] if picked_a else []\n",
    "# picked_depths_b = [depth for depth, category in picked_b] if picked_b else []\n",
    "# picked_categories_b = [category for depth, category in picked_b] if picked_b else []\n",
    "\n",
    "# # Rename cores\n",
    "# CORE_A = \"Core_A\"\n",
    "# CORE_B = \"Core_B\"\n",
    "\n",
    "# print(f\"\\n=== TOP 200 CM EXTRACTION COMPLETE ===\")\n",
    "# print(f\"Core A: Extracted {len(md_a)} data points (0 to {md_a.max():.1f} cm)\")\n",
    "# print(f\"Core B: Extracted {len(md_b)} data points (0 to {md_b.max():.1f} cm)\")\n",
    "# print(f\"Cores renamed to: {CORE_A} and {CORE_B}\")\n",
    "# print(f\"Log A shape after clipping and renormalization: {log_a.shape}\")\n",
    "# print(f\"Log B shape after clipping and renormalization: {log_b.shape}\")\n",
    "# print(f\"Log A range after renormalization: [{log_a.min():.3f}, {log_a.max():.3f}]\")\n",
    "# print(f\"Log B range after renormalization: [{log_b.min():.3f}, {log_b.max():.3f}]\")\n",
    "# print(f\"Picked depths A: {len(picked_depths_a)} depths\")\n",
    "# print(f\"Picked depths B: {len(picked_depths_b)} depths\")\n",
    "# if rgb_img_a is not None:\n",
    "#     print(f\"RGB image A shape after clipping: {rgb_img_a.shape}\")\n",
    "# if rgb_img_b is not None:\n",
    "#     print(f\"RGB image B shape after clipping: {rgb_img_b.shape}\")\n",
    "# if ct_img_a is not None:\n",
    "#     print(f\"CT image A shape after clipping: {ct_img_a.shape}\")\n",
    "# if ct_img_b is not None:\n",
    "#     print(f\"CT image B shape after clipping: {ct_img_b.shape}\")\n",
    "# print(\"=== Comment out this cell to use full core data ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Usage Examples and Executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a subset of picked depths of category 1 for both cores\n",
    "all_depths_a_cat1 = np.array([depth for depth, category in picked_a if category == 1]).astype('float32')\n",
    "all_depths_b_cat1 = np.array([depth for depth, category in picked_b if category == 1]).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_age_constraints(core_name, consider_adjacent_core=False):\n",
    "    \"\"\"\n",
    "    Load age constraints for a specific core, optionally including data from adjacent cores.\n",
    "    \n",
    "    Args:\n",
    "        core_name: Name of the core to load data for\n",
    "        consider_adjacent_core: If True, also load data from cores with similar names\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing all age constraint data\n",
    "    \"\"\"\n",
    "    base_path = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/Age constraints/Goldfinger2012'\n",
    "    csv_files = []\n",
    "    \n",
    "    # Add primary core CSV\n",
    "    primary_csv = f'{base_path}/{core_name}_age.csv'\n",
    "    csv_files.append(primary_csv)\n",
    "    \n",
    "    # Add adjacent core CSVs if specified\n",
    "    if consider_adjacent_core:\n",
    "        # Get base part of core name (without last two characters)\n",
    "        core_base = core_name[:-2]\n",
    "        # Look for similar core names in the directory\n",
    "        if os.path.exists(base_path):\n",
    "            for file in os.listdir(base_path):\n",
    "                if file.endswith('_age.csv') and file.startswith(f'{core_base}'):\n",
    "                    potential_core = file.split('_age.csv')[0]\n",
    "                    if potential_core != core_name:  # Skip the primary core\n",
    "                        csv_files.append(f'{base_path}/{file}')\n",
    "    \n",
    "    # Initialize result containers\n",
    "    all_data = pd.DataFrame()\n",
    "    result = {\n",
    "        'depths': [],\n",
    "        'ages': [],\n",
    "        'pos_errors': [],\n",
    "        'neg_errors': [],\n",
    "        'in_sequence_flags': [],\n",
    "        'in_sequence_depths': [],\n",
    "        'in_sequence_ages': [],\n",
    "        'in_sequence_pos_errors': [],\n",
    "        'in_sequence_neg_errors': [],\n",
    "        'out_sequence_depths': [],\n",
    "        'out_sequence_ages': [],\n",
    "        'out_sequence_pos_errors': [],\n",
    "        'out_sequence_neg_errors': [],\n",
    "        'core': [],\n",
    "        'interpreted_bed': []\n",
    "    }\n",
    "    \n",
    "    # Define required columns\n",
    "    required_columns = ['calib502_agebp', 'calib502_2sigma_pos', 'calib502_2sigma_neg', \n",
    "                      'mindepth_cm', 'maxdepth_cm', 'in_sequence', 'core', 'interpreted_bed']\n",
    "    \n",
    "    # Process each CSV file\n",
    "    loaded_files = 0\n",
    "    for csv_file in csv_files:\n",
    "        if os.path.exists(csv_file):\n",
    "            data = pd.read_csv(csv_file)\n",
    "            # Filter rows with all required columns available\n",
    "            for col in required_columns:\n",
    "                data = data.dropna(subset=[col])\n",
    "            \n",
    "            all_data = pd.concat([all_data, data])\n",
    "            loaded_files += 1\n",
    "    \n",
    "    if loaded_files > 0:\n",
    "        print(f\"Loaded {len(all_data)} age constraints for {core_name}\")\n",
    "        \n",
    "        # Sort by age if multiple cores were combined\n",
    "        if consider_adjacent_core:\n",
    "            all_data = all_data.sort_values(by='mindepth_cm')\n",
    "        \n",
    "        # Extract all age constraints\n",
    "        result['depths'] = (all_data['mindepth_cm'] + all_data['maxdepth_cm']) / 2\n",
    "        result['ages'] = all_data['calib502_agebp'].tolist()\n",
    "        result['pos_errors'] = all_data['calib502_2sigma_pos'].tolist()\n",
    "        result['neg_errors'] = all_data['calib502_2sigma_neg'].tolist()\n",
    "        result['in_sequence_flags'] = all_data['in_sequence'].tolist()\n",
    "        result['core'] = all_data['core'].tolist()\n",
    "        result['interpreted_bed'] = all_data['interpreted_bed'].tolist()\n",
    "        \n",
    "        # Separate in-sequence and out-of-sequence constraints\n",
    "        for i in range(len(result['in_sequence_flags'])):\n",
    "            if result['in_sequence_flags'][i] == 1:\n",
    "                result['in_sequence_depths'].append(result['depths'].iloc[i] if isinstance(result['depths'], pd.Series) else result['depths'][i])\n",
    "                result['in_sequence_ages'].append(result['ages'][i])\n",
    "                result['in_sequence_pos_errors'].append(result['pos_errors'][i])\n",
    "                result['in_sequence_neg_errors'].append(result['neg_errors'][i])\n",
    "            else:\n",
    "                result['out_sequence_depths'].append(result['depths'].iloc[i] if isinstance(result['depths'], pd.Series) else result['depths'][i])\n",
    "                result['out_sequence_ages'].append(result['ages'][i])\n",
    "                result['out_sequence_pos_errors'].append(result['pos_errors'][i])\n",
    "                result['out_sequence_neg_errors'].append(result['neg_errors'][i])\n",
    "    else:\n",
    "        print(f\"Warning: No age constraint files found for {core_name}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load age constraints for both cores\n",
    "consider_adjacent_core = True\n",
    "age_data_a = load_age_constraints(CORE_A, consider_adjacent_core)\n",
    "age_data_b = load_age_constraints(CORE_B, consider_adjacent_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate interpolated ages for Core A using the function\n",
    "pickeddepth_ages_a = calculate_interpolated_ages(\n",
    "    # Input data\n",
    "    picked_depths=all_depths_a_cat1,                                     # depths to interpolate ages for\n",
    "    age_constraints_depths=age_data_a['depths'],                         # age constraint depths\n",
    "    age_constraints_ages=age_data_a['ages'],                             # age constraint ages\n",
    "    age_constraints_pos_errors=age_data_a['pos_errors'],                 # positive errors\n",
    "    age_constraints_neg_errors=age_data_a['neg_errors'],                 # negative errors\n",
    "    age_constraints_in_sequence_flags=age_data_a['in_sequence_flags'],   # in-sequence flags\n",
    "    age_constraint_source_core=age_data_a['core'],                       # source core for each constraint\n",
    "    # Core boundaries\n",
    "    top_bottom=True,                                                     # include top and bottom depths/ages\n",
    "    top_depth=0.0,                                                       # top of core depth\n",
    "    bottom_depth=md_a[-1],                                               # max depth of core a\n",
    "    top_age=0,                                                           # default age at top of core\n",
    "    top_age_pos_error=75,                                                # default positive uncertainty of top age\n",
    "    top_age_neg_error=75,                                                # default negative uncertainty of top age\n",
    "    # Uncertainty calculation\n",
    "    uncertainty_method='MonteCarlo',                                     # uncertainty calculation method\n",
    "    n_monte_carlo=10000,                                                 # number of Monte Carlo iterations\n",
    "    # Visualization and output\n",
    "    show_plot=True,                                                      # display plot\n",
    "    core_name=CORE_A,                                                    # core name for plot title\n",
    "    export_csv=True                                                      # export results to CSV\n",
    ")\n",
    "\n",
    "# Print the age constraint data for Core A\n",
    "print(\"\\nAge Constraints for Core A:\")\n",
    "if len(age_data_a['depths']) > 0:\n",
    "    for i in range(len(age_data_a['depths'])):\n",
    "        depth_val = age_data_a['depths'].iloc[i] if isinstance(age_data_a['depths'], pd.Series) else age_data_a['depths'][i]\n",
    "        age_val = age_data_a['ages'][i]\n",
    "        pos_err_val = age_data_a['pos_errors'][i]\n",
    "        neg_err_val = age_data_a['neg_errors'][i]\n",
    "        in_seq = age_data_a['in_sequence_flags'][i]\n",
    "        \n",
    "        # Add source core and interpreted bed info if they exist\n",
    "        source_core_info = f\", Source Core: {age_data_a['core'][i]}\" if i < len(age_data_a['core']) else \"\"\n",
    "        bed_info = f\", Interpreted Bed: {age_data_a['interpreted_bed'][i]}\" if i < len(age_data_a['interpreted_bed']) else \"\"\n",
    "        \n",
    "        print(f\"Depth: {depth_val:.2f} cm, Age: {age_val:.1f} years BP (+{pos_err_val:.1f} ; -{neg_err_val:.1f}), In Sequence: {in_seq}{source_core_info}{bed_info}\")\n",
    "else:\n",
    "    print(f\"No age constraints available in {CORE_A}\")\n",
    "\n",
    "# Print the interpolated ages\n",
    "print(f\"\\nEstimated Ages for picked depths in {CORE_A}:\")\n",
    "for i, depth in enumerate(pickeddepth_ages_a['depths']):\n",
    "    print(f\"Depth: {depth:.2f} cm, Age: {pickeddepth_ages_a['ages'][i]:.1f} years BP (+{pickeddepth_ages_a['pos_uncertainties'][i]:.1f} ; -{pickeddepth_ages_a['neg_uncertainties'][i]:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate interpolated ages for Core B using the function\n",
    "pickeddepth_ages_b = calculate_interpolated_ages(\n",
    "    # Input data\n",
    "    picked_depths=all_depths_b_cat1,                                     # depths to interpolate ages for\n",
    "    age_constraints_depths=age_data_b['depths'],                         # age constraint depths\n",
    "    age_constraints_ages=age_data_b['ages'],                             # age constraint ages\n",
    "    age_constraints_pos_errors=age_data_b['pos_errors'],                 # positive errors\n",
    "    age_constraints_neg_errors=age_data_b['neg_errors'],                 # negative errors\n",
    "    age_constraints_in_sequence_flags=age_data_b['in_sequence_flags'],   # in-sequence flags\n",
    "    age_constraint_source_core=age_data_b['core'],                       # source core for each constraint\n",
    "    # Core boundaries\n",
    "    top_bottom=True,                                                     # include top and bottom depths/ages\n",
    "    top_depth=0.0,                                                       # top of core depth\n",
    "    bottom_depth=md_b[-1],                                               # max depth of core b\n",
    "    top_age=0,                                                           # default age at top of core\n",
    "    top_age_pos_error=75,                                                # default positive uncertainty of top age\n",
    "    top_age_neg_error=75,                                                # default negative uncertainty of top age\n",
    "    # Uncertainty calculation\n",
    "    uncertainty_method='MonteCarlo',                                     # 'MonteCarlo', 'Linear', or 'Gaussian'\n",
    "    n_monte_carlo=10000,                                                 # number of Monte Carlo sampling iterations\n",
    "    # Visualization and output\n",
    "    show_plot=True,                                                      # display plot\n",
    "    core_name=CORE_B,                                                    # core name for plot title\n",
    "    export_csv=True                                                      # export results to CSV\n",
    ")\n",
    "\n",
    "# Print the age constraint data for Core B\n",
    "print(\"\\nAge Constraints for Core B:\")\n",
    "if len(age_data_b['depths']) > 0:\n",
    "    for i in range(len(age_data_b['depths'])):\n",
    "        depth_val = age_data_b['depths'].iloc[i] if isinstance(age_data_b['depths'], pd.Series) else age_data_b['depths'][i]\n",
    "        age_val = age_data_b['ages'][i]\n",
    "        pos_err_val = age_data_b['pos_errors'][i]\n",
    "        neg_err_val = age_data_b['neg_errors'][i]\n",
    "        in_seq = age_data_b['in_sequence_flags'][i]\n",
    "        \n",
    "        # Add source core and interpreted bed info if they exist\n",
    "        source_core_info = f\", Source Core: {age_data_b['core'][i]}\" if i < len(age_data_b['core']) else \"\"\n",
    "        bed_info = f\", Interpreted Bed: {age_data_b['interpreted_bed'][i]}\" if i < len(age_data_b['interpreted_bed']) else \"\"\n",
    "        \n",
    "        print(f\"Depth: {depth_val:.2f} cm, Age: {age_val:.1f} years BP (+{pos_err_val:.1f} ; -{neg_err_val:.1f}), In Sequence: {in_seq}{source_core_info}{bed_info}\")\n",
    "else:\n",
    "    print(f\"No age constraints available in {CORE_B}\")\n",
    "\n",
    "print(f\"\\nEstimated Ages for picked depths in {CORE_B}:\")\n",
    "for i, depth in enumerate(pickeddepth_ages_b['depths']):\n",
    "    print(f\"Depth: {depth:.2f} cm, Age: {pickeddepth_ages_b['ages'][i]:.1f} years BP (+{pickeddepth_ages_b['pos_uncertainties'][i]:.1f} ; -{pickeddepth_ages_b['neg_uncertainties'][i]:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the age data from CSV files\n",
    "\n",
    "# Load age data for Core A\n",
    "core_a_age_csv = f\"{CORE_A}_pickeddepth_age.csv\"\n",
    "if os.path.exists(core_a_age_csv):\n",
    "    df_ages_a = pd.read_csv(core_a_age_csv)\n",
    "    pickeddepth_ages_a = {\n",
    "        'depths': df_ages_a['picked_depths_cm'].values.astype('float32').tolist(),\n",
    "        'ages': df_ages_a['est_age'].values.astype('float32').tolist(),\n",
    "        'pos_uncertainties': df_ages_a['est_age_poserr'].values.astype('float32').tolist(),\n",
    "        'neg_uncertainties': df_ages_a['est_age_negerr'].values.astype('float32').tolist()\n",
    "    }\n",
    "    print(f\"Loaded age data for {CORE_A} from CSV file\")\n",
    "else:\n",
    "    print(f\"Warning: Could not find age data CSV for {CORE_A}\")\n",
    "\n",
    "# Load age data for Core B\n",
    "core_b_age_csv = f\"{CORE_B}_pickeddepth_age.csv\"\n",
    "if os.path.exists(core_b_age_csv):\n",
    "    df_ages_b = pd.read_csv(core_b_age_csv)\n",
    "    pickeddepth_ages_b = {\n",
    "        'depths': df_ages_b['picked_depths_cm'].values.astype('float32').tolist(),\n",
    "        'ages': df_ages_b['est_age'].values.astype('float32').tolist(),\n",
    "        'pos_uncertainties': df_ages_b['est_age_poserr'].values.astype('float32').tolist(),\n",
    "        'neg_uncertainties': df_ages_b['est_age_negerr'].values.astype('float32').tolist()\n",
    "    }\n",
    "    print(f\"Loaded age data for {CORE_B} from CSV file\")\n",
    "else:\n",
    "    print(f\"Warning: Could not find age data CSV for {CORE_B}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find out all segment pairs among boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file names for age consideration or not\n",
    "age_consideration=True\n",
    "restricted_age_correlation=True\n",
    "shortest_path_search=True\n",
    "\n",
    "if age_consideration:\n",
    "    if restricted_age_correlation:\n",
    "        YES_NO_AGE = 'restricted_age'\n",
    "    else:\n",
    "        YES_NO_AGE = 'loose_age'\n",
    "else:\n",
    "    YES_NO_AGE = 'no_age'\n",
    "\n",
    "if shortest_path_search:\n",
    "    SEARCH_METHOD = 'optimal'\n",
    "else:\n",
    "    SEARCH_METHOD = 'random'\n",
    "\n",
    "# Define whether to use independent DTW or not\n",
    "independent_dtw=False # If False (default), it performs dependent DTW. If True, it performs independent DTW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Example usage:\n",
    "# Set picked_depths_a and picked_depths_b to None to use auto-segmentation\n",
    "\n",
    "# Define the folder path\n",
    "frames_folder = \"outputs/SegmentPair_DTW_frames\"\n",
    "\n",
    "# Check if the folder exists\n",
    "if os.path.exists(frames_folder):\n",
    "    # Get all PNG files in the folderf d\n",
    "    png_files = glob.glob(os.path.join(frames_folder, \"*.png\"))\n",
    "    \n",
    "    # Delete each PNG file\n",
    "    for png_file in png_files:\n",
    "        try:\n",
    "            os.remove(png_file)\n",
    "            # print(f\"Deleted: {png_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {png_file}: {e}\")\n",
    "    \n",
    "    print(f\"Cleaned up {len(png_files)} PNG files from {frames_folder}\")\n",
    "else:\n",
    "    print(f\"Folder '{frames_folder}' does not exist. Creating it...\")\n",
    "    os.makedirs(frames_folder, exist_ok=True)\n",
    "    \n",
    "# Run comprehensive DTW analysis\n",
    "dtw_results, valid_dtw_pairs, segments_a, segments_b, depth_boundaries_a, depth_boundaries_b, dtw_distance_matrix_full = run_comprehensive_dtw_analysis(\n",
    "    # Input data\n",
    "    log_a,                                                      # Core A log data\n",
    "    log_b,                                                      # Core B log data\n",
    "    md_a,                                                       # Core A measured depth\n",
    "    md_b,                                                       # Core B measured depth\n",
    "    picked_depths_a=all_depths_a_cat1,                         # Selected depths for core A\n",
    "    picked_depths_b=all_depths_b_cat1,                         # Selected depths for core B\n",
    "    core_a_name=CORE_A,                                        # Name identifier for core A\n",
    "    core_b_name=CORE_B,                                        # Name identifier for core B\n",
    "    # Analysis parameters\n",
    "    top_bottom=True,                                            # Include top and bottom boundaries\n",
    "    top_depth=0.0,                                              # Starting depth for analysis\n",
    "    independent_dtw=independent_dtw,                            # Use independent DTW if True\n",
    "    exclude_deadend=True,                                       # Exclude dead-end segments\n",
    "    # Age constraints\n",
    "    age_consideration=age_consideration,                        # Include age constraints\n",
    "    ages_a=pickeddepth_ages_a,                                  # Age data for core A depths\n",
    "    ages_b=pickeddepth_ages_b,                                  # Age data for core B depths\n",
    "    restricted_age_correlation=restricted_age_correlation,      # Use strict age correlation\n",
    "    all_constraint_ages_a=age_data_a['in_sequence_ages'],      # All age constraints for core A\n",
    "    all_constraint_ages_b=age_data_b['in_sequence_ages'],      # All age constraints for core B\n",
    "    all_constraint_depths_a=age_data_a['in_sequence_depths'],  # All depth constraints for core A\n",
    "    all_constraint_depths_b=age_data_b['in_sequence_depths'],  # All depth constraints for core B\n",
    "    all_constraint_pos_errors_a=age_data_a['in_sequence_pos_errors'], # Positive age errors for core A\n",
    "    all_constraint_pos_errors_b=age_data_b['in_sequence_pos_errors'], # Positive age errors for core B\n",
    "    all_constraint_neg_errors_a=age_data_a['in_sequence_neg_errors'], # Negative age errors for core A\n",
    "    all_constraint_neg_errors_b=age_data_b['in_sequence_neg_errors'], # Negative age errors for core B\n",
    "    age_constraint_a_source_cores=age_data_a['core'],          # Source cores for age constraints A\n",
    "    age_constraint_b_source_cores=age_data_b['core'],          # Source cores for age constraints B\n",
    "    # Visualization\n",
    "    visualize_pairs=True,                                       # Create pair visualizations\n",
    "    visualize_segment_labels=False,                             # Show segment labels in plots\n",
    "    create_dtw_matrix=True,                                     # Generate DTW distance matrix\n",
    "    dtwmatrix_output_filename=f'SegmentPair_DTW_matrix_{CORE_A}_{CORE_B}_{YES_NO_AGE}_{SEARCH_METHOD}.png', # Matrix plot filename\n",
    "    creategif=True,                                             # Create animated GIF\n",
    "    gif_output_filename=f'SegmentPair_DTW_animation_{CORE_A}_{CORE_B}_{YES_NO_AGE}_{SEARCH_METHOD}.gif', # GIF filename\n",
    "    max_frames=50,                                              # Maximum frames in animation\n",
    "    color_interval_size=5,                                      # Color coding interval size\n",
    "    keep_frames=True,                                           # Save individual frames\n",
    "    # Debug and processing\n",
    "    debug=False                                                 # Enable debug output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_result = diagnose_chain_breaks(\n",
    "    # Input data\n",
    "    valid_dtw_pairs,                                        # Valid DTW segment pairs from analysis\n",
    "    segments_a,                                             # Segment definitions for core A\n",
    "    segments_b,                                             # Segment definitions for core Bss\n",
    "    depth_boundaries_a,                                     # Depth boundaries for core A segments\n",
    "    depth_boundaries_b                                      # Depth boundaries for core B segments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Search complete DTW paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_path_search_result = find_complete_core_paths(\n",
    "    # Input data\n",
    "    valid_dtw_pairs,                                                                # Valid DTW segment pairs from analysis\n",
    "    segments_a,                                                                     # Segment definitions for core A\n",
    "    segments_b,                                                                     # Segment definitions for core B\n",
    "    log_a,                                                                          # Log data for core A\n",
    "    log_b,                                                                          # Log data for core B\n",
    "    depth_boundaries_a,                                                             # Depth boundaries for core A segments\n",
    "    depth_boundaries_b,                                                             # Depth boundaries for core B segments\n",
    "    dtw_results,                                                                    # DTW analysis results\n",
    "    # Output settings\n",
    "    output_csv=f\"mappings_{CORE_A}_{CORE_B}_{YES_NO_AGE}_{SEARCH_METHOD}.csv\",     # Output CSV filename for mappings\n",
    "    # Search parameters\n",
    "    start_from_top_only=True,                                                       # Start path search from top segments only\n",
    "    shortest_path_search=shortest_path_search,                                      # Use shortest path search algorithm\n",
    "    shortest_path_level=2,                                                          # Path level preference (higher = more segments)\n",
    "    max_search_path=100000,                                                         # Maximum paths per segment pair to avoid memory issues\n",
    "    # Processing settings\n",
    "    batch_size=1000,                                                                # Processing batch size\n",
    "    n_jobs=-1,                                                                      # Number of CPU cores (-1 uses all available)\n",
    "    debug=False                                                                     # Enable debug output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "correlation_save_path=f'CombinedDTW_correlation_mappings_{CORE_A}_{CORE_B}_{YES_NO_AGE}_{SEARCH_METHOD}.gif'\n",
    "matrix_save_path=f'CombinedDTW_matrix_mappings_{CORE_A}_{CORE_B}_{YES_NO_AGE}_{SEARCH_METHOD}.gif'\n",
    "\n",
    "# 1. First, read all available mappings from a CSV (assuming it was created by find_all_sequential_mappings)\n",
    "sequential_mappings_csv = f\"outputs/mappings_{CORE_A}_{CORE_B}_{YES_NO_AGE}_{SEARCH_METHOD}.csv\"\n",
    "\n",
    "# 3. Visualize a representative subset of the mappings\n",
    "visualize_dtw_results_from_csv(\n",
    "    # Input data\n",
    "    sequential_mappings_csv,                                                        # CSV file with sequential mappings\n",
    "    log_a,                                                                          # Log data for core A\n",
    "    log_b,                                                                          # Log data for core B\n",
    "    md_a,                                                                           # Measured depth data for core A\n",
    "    md_b,                                                                           # Measured depth data for core B\n",
    "    dtw_results,                                                                    # DTW analysis results\n",
    "    valid_dtw_pairs,                                                                # Valid DTW segment pairs\n",
    "    segments_a,                                                                     # Segment definitions for core A\n",
    "    segments_b,                                                                     # Segment definitions for core B\n",
    "    depth_boundaries_a,                                                             # Depth boundaries for core A segments\n",
    "    depth_boundaries_b,                                                             # Depth boundaries for core B segments\n",
    "    dtw_distance_matrix_full,                                                       # Full DTW distance matrix\n",
    "    # Core identifiers\n",
    "    core_a_name=CORE_A,                                                             # Name identifier for core A\n",
    "    core_b_name=CORE_B,                                                             # Name identifier for core B\n",
    "    # Visualization settings\n",
    "    color_interval_size=5,                                                          # Color interval size for visualization\n",
    "    debug=False,                                                                    # Enable debug output\n",
    "    visualize_pairs=False,                                                          # Show DTW pairs in visualization\n",
    "    visualize_segment_labels=False,                                                 # Show segment labels in visualization\n",
    "    mark_depths=True,                                                               # Mark depth points in visualization\n",
    "    # GIF output settings\n",
    "    creategif=True,                                                                 # Create animated GIF output\n",
    "    correlation_gif_output_filename=correlation_save_path,                          # Output filename for correlation GIF\n",
    "    matrix_gif_output_filename=matrix_save_path,                                    # Output filename for matrix GIF\n",
    "    max_frames=50,                                                                  # Maximum number of frames in GIF\n",
    "    keep_frames=True,                                                               # Keep individual frames after GIF creation\n",
    "    # Age constraints\n",
    "    mark_ages=age_consideration,                                                    # Mark age constraints in visualization\n",
    "    ages_a=pickeddepth_ages_a,                                                      # Age data for core A\n",
    "    ages_b=pickeddepth_ages_b,                                                      # Age data for core B\n",
    "    all_constraint_depths_a=age_data_a['in_sequence_depths'],                       # Depth constraints for core A\n",
    "    all_constraint_depths_b=age_data_b['in_sequence_depths'],                       # Depth constraints for core B\n",
    "    all_constraint_ages_a=age_data_a['in_sequence_ages'],                           # Age constraints for core A\n",
    "    all_constraint_ages_b=age_data_b['in_sequence_ages'],                           # Age constraints for core B\n",
    "    all_constraint_pos_errors_a=age_data_a['in_sequence_pos_errors'],               # Positive age errors for core A\n",
    "    all_constraint_pos_errors_b=age_data_b['in_sequence_pos_errors'],               # Positive age errors for core B\n",
    "    all_constraint_neg_errors_a=age_data_a['in_sequence_neg_errors'],               # Negative age errors for core A\n",
    "    all_constraint_neg_errors_b=age_data_b['in_sequence_neg_errors'],               # Negative age errors for core B\n",
    "    age_constraint_a_source_cores=age_data_a['core'],                               # Source cores for age constraints A\n",
    "    age_constraint_b_source_cores=age_data_b['core']                                # Source cores for age constraints B\n",
    ")\n",
    "\n",
    "# Display the GIFs\n",
    "print(\"DTW Correlation Mappings GIF:\")\n",
    "display(IPImage(f\"outputs/{correlation_save_path}\"))\n",
    "\n",
    "print(\"DTW Matrix Mappings GIF:\")\n",
    "display(IPImage(f\"outputs/{matrix_save_path}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved DTW results\n",
    "sequential_mappings_csv = f'outputs/mappings_{CORE_A}_{CORE_B}_{YES_NO_AGE}_{SEARCH_METHOD}.csv'\n",
    "output_matrix_png_filename = f'CombinedDTW_matrix_mappings_colored_{CORE_A}_{CORE_B}_{YES_NO_AGE}_{SEARCH_METHOD}.png'\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "_ = plot_dtw_matrix_with_paths(\n",
    "    # Input data\n",
    "    dtw_distance_matrix_full,                                                       # Full DTW distance matrix\n",
    "    sequential_mappings_csv=sequential_mappings_csv,                                # CSV file with sequential mappings\n",
    "    # Core identifiers\n",
    "    core_a_name=CORE_A,                                                             # Name identifier for core A\n",
    "    core_b_name=CORE_B,                                                             # Name identifier for core B\n",
    "    md_a=md_a,                                                                      # Metadata for core A\n",
    "    md_b=md_b,                                                                      # Metadata for core B\n",
    "    # Visualization settings\n",
    "    mode='all_paths_colored',                                                       # Visualization mode\n",
    "    color_metric='perc_diag',                                                       # Metric used for coloring paths\n",
    "                                                                                    # Available options: 'corr_coef', 'norm_dtw', 'dtw_ratio', 'perc_diag', \n",
    "                                                                                    # 'variance_deviation', 'match_min', 'match_mean', 'perc_age_overlap', None (uses mapping_id)\n",
    "    output_filename=output_matrix_png_filename,                                     # Output filename for the plot\n",
    "    # Age constraint data\n",
    "    age_constraint_a_depths=age_data_a['in_sequence_depths'] if age_consideration else None,  # Depth constraints for core A\n",
    "    age_constraint_a_ages=age_data_a['in_sequence_ages'] if age_consideration else None,      # Age constraints for core A\n",
    "    age_constraint_a_source_cores=age_data_a['core'] if age_consideration else None,          # Source cores for age constraints A\n",
    "    age_constraint_b_depths=age_data_b['in_sequence_depths'] if age_consideration else None,  # Depth constraints for core B\n",
    "    age_constraint_b_ages=age_data_b['in_sequence_ages'] if age_consideration else None,      # Age constraints for core B\n",
    "    age_constraint_b_source_cores=age_data_b['core'] if age_consideration else None,          # Source cores for age constraints B\n",
    "    # Performance settings\n",
    "    n_jobs=-1                                                                       # Number of parallel jobs (-1 means use all processors)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom metric weights\n",
    "# Load the DTW results\n",
    "sequential_mappings_csv = f'outputs/mappings_{CORE_A}_{CORE_B}_{YES_NO_AGE}_{SEARCH_METHOD}.csv'\n",
    "dtw_results_df = pd.read_csv(sequential_mappings_csv)\n",
    "\n",
    "custom_weights = {\n",
    "    'corr_coef': 3.0,\n",
    "    'perc_diag': 1.0,\n",
    "    'norm_dtw': 1.0,\n",
    "    'dtw_ratio': 0.0,\n",
    "    'perc_age_overlap': 1.0,\n",
    "    'wrapping_deviation': 0.0,\n",
    "    'mean_matching_function': 0.0\n",
    "}\n",
    "\n",
    "top_mapping_ids, top_mapping_pairs, top_mappings_df = find_best_mappings(\n",
    "    csv_file_path=sequential_mappings_csv,\n",
    "    top_n=5,\n",
    "    filter_shortest_dtw=True,\n",
    "    metric_weight=custom_weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "visualize_pairs=False\n",
    "\n",
    "if visualize_pairs:\n",
    "    visualize_type='pairs'\n",
    "    visualize_segment_labels=True\n",
    "    mark_depths=True\n",
    "else:\n",
    "    visualize_type='fullpath'\n",
    "    visualize_segment_labels=False\n",
    "    mark_depths=False\n",
    "\n",
    "# Visualize the combined segments\n",
    "_, _, _, _ = visualize_combined_segments(\n",
    "    # Input data\n",
    "    log_a=log_a,                                # Core A log data\n",
    "    log_b=log_b,                                # Core B log data\n",
    "    md_a=md_a,                                  # Core A measured depths\n",
    "    md_b=md_b,                                  # Core B measured depths\n",
    "    dtw_results=dtw_results,                    # DTW alignment results\n",
    "    valid_dtw_pairs=valid_dtw_pairs,            # Valid DTW pairs\n",
    "    segments_a=segments_a,                      # Core A segments\n",
    "    segments_b=segments_b,                      # Core B segments\n",
    "    depth_boundaries_a=depth_boundaries_a,      # Core A depth boundaries\n",
    "    depth_boundaries_b=depth_boundaries_b,      # Core B depth boundaries\n",
    "    dtw_distance_matrix_full=dtw_distance_matrix_full,       # Full DTW distance matrix\n",
    "    segment_pairs_to_combine=top_mapping_pairs[0],         # Valid pairs to combine\n",
    "    # Visualization options\n",
    "    color_interval_size=5,                      # Size of color intervals\n",
    "    visualize_pairs=visualize_pairs,            # Whether to visualize pairs (True/False)\n",
    "    visualize_segment_labels=visualize_segment_labels, # Whether to show segment labels (True/False)\n",
    "    mark_depths=mark_depths,                    # Whether to mark depths (True/False)\n",
    "    # Output paths\n",
    "    correlation_save_path=f'CombinedDTW_correlation_{CORE_A}_{CORE_B}_{YES_NO_AGE}_{SEARCH_METHOD}_{top_mapping_ids[0]}_{visualize_type}.png',\n",
    "    matrix_save_path=f'CombinedDTW_matrix_{CORE_A}_{CORE_B}_{YES_NO_AGE}_{SEARCH_METHOD}_{top_mapping_ids[0]}_{visualize_type}.png',\n",
    "    # Age constraint parameters\n",
    "    mark_ages=age_consideration,                # Whether to mark ages (True/False)\n",
    "    ages_a=pickeddepth_ages_a if age_consideration else None, # Core A ages\n",
    "    ages_b=pickeddepth_ages_b if age_consideration else None, # Core B ages\n",
    "    all_constraint_ages_a=age_data_a['in_sequence_ages'] if age_consideration else None, # Core A constraint ages\n",
    "    all_constraint_ages_b=age_data_b['in_sequence_ages'] if age_consideration else None, # Core B constraint ages\n",
    "    all_constraint_depths_a=age_data_a['in_sequence_depths'] if age_consideration else None, # Core A constraint depths\n",
    "    all_constraint_depths_b=age_data_b['in_sequence_depths'] if age_consideration else None, # Core B constraint depths\n",
    "    all_constraint_pos_errors_a=age_data_a['in_sequence_pos_errors'] if age_consideration else None, # Core A positive errors\n",
    "    all_constraint_pos_errors_b=age_data_b['in_sequence_pos_errors'] if age_consideration else None, # Core B positive errors\n",
    "    all_constraint_neg_errors_a=age_data_a['in_sequence_neg_errors'] if age_consideration else None, # Core A negative errors\n",
    "    all_constraint_neg_errors_b=age_data_b['in_sequence_neg_errors'] if age_consideration else None, # Core B negative errors\n",
    "    age_constraint_a_source_cores=age_data_a['core'] if age_consideration else None, # Core A source cores\n",
    "    age_constraint_b_source_cores=age_data_b['core'] if age_consideration else None, # Core B source cores\n",
    "    # Core identifiers\n",
    "    core_a_name=CORE_A,                         # Name of Core A\n",
    "    core_b_name=CORE_B                          # Name of Core B\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available quality indices: 'corr_coef', 'norm_dtw', 'dtw_ratio', 'perc_diag', 'variance_deviation', 'match_min', 'match_mean', 'perc_age_overlap'\n",
    "targeted_quality_index = 'corr_coef' \n",
    "\n",
    "# Example usage:\n",
    "plot_correlation_distribution(\n",
    "    # Input parameters\n",
    "    csv_file=f'outputs/mappings_{CORE_A}_{CORE_B}_{YES_NO_AGE}_{SEARCH_METHOD}.csv',  # Path to mappings CSV file\n",
    "    target_mapping_id=top_mapping_ids[0],                                              # ID of mapping to analyze\n",
    "    quality_index=targeted_quality_index,                                             # Quality metric to plot\n",
    "    # Histogram parameters\n",
    "    no_bins=None,                                                                     # Number of bins (auto if None)\n",
    "    # Output parameters\n",
    "    save_png=True,                                                                    # Whether to save plot as PNG\n",
    "    png_filename=f'{\"r-values\" if targeted_quality_index == \"corr_coef\" else targeted_quality_index}_distribution_{CORE_A}_{CORE_B}_{YES_NO_AGE}_{SEARCH_METHOD}.png',  # Output filename\n",
    "    # Distribution fitting parameters\n",
    "    pdf_method='normal',                                                              # PDF fitting method: 'KDE', 'skew-normal', or 'normal'\n",
    "    kde_bandwidth=0.05                                                                # Bandwidth for KDE method\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "11-TimeSeriesCorrelation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
