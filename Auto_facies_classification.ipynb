{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto lithofacies classification (unsupervised cluster analysis)\n",
    "\n",
    "This notebook demonstrates how to read four CSV files (CT, RGB, MST, hi-res MS) for core M9907-22PC, merge them using a shared depth column, perform unsupervised K-Means clustering on selected log columns, and visualize the clustered intervals versus depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import random\n",
    "import matplotlib.collections as mcoll\n",
    "from scipy import stats\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.patches as mpatches\n",
    "# Resample each dataset to the common depth scale\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# from pyCoreRelator import plot_core_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: directories, config, file paths, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core parameters\n",
    "core_name = \"M9907-23PC\"  # Core name\n",
    "total_length_cm = 783     # Core length in cm\n",
    "\n",
    "# Configuration dictionaries\n",
    "data_config = {\n",
    "    'mother_dir': '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/',\n",
    "    'core_name': core_name,\n",
    "    'core_length': total_length_cm,\n",
    "    'data_folder': f'_compiled_logs/{core_name}/',\n",
    "    'clean_output_folder': f'_compiled_logs/{core_name}/ML_clean/',\n",
    "    'filled_output_folder': f'_compiled_logs/{core_name}/ML_filled/',\n",
    "    'thresholds': {\n",
    "        'ms': ['>', 150, 1],\n",
    "        'pwvel': ['>', 1085, 3],\n",
    "        'den': ['<', 1, 1],\n",
    "        'elecres': ['<', 0, 1],\n",
    "        'hiresms': ['<=', 24, 1]\n",
    "    }\n",
    "}\n",
    "\n",
    "mother_dir = data_config['mother_dir']\n",
    "core_length = data_config['core_length']\n",
    "clean_output_folder = data_config['clean_output_folder']\n",
    "filled_output_folder = data_config['filled_output_folder']\n",
    "\n",
    "# Load CT and RGB images\n",
    "ct_img_path = mother_dir + f'_compiled_logs/{core_name}/{core_name}_CT.tiff'\n",
    "rgb_img_path = mother_dir + f'_compiled_logs/{core_name}/{core_name}_RGB.tiff'\n",
    "\n",
    "ct_img = plt.imread(ct_img_path)\n",
    "rgb_img = plt.imread(rgb_img_path)\n",
    "\n",
    "# Configuration for log data column auto-selection within the functions\n",
    "column_configs = {\n",
    "    'ct': {\n",
    "        'data_col': 'CT',\n",
    "        'std_col': 'CT_std',\n",
    "        'depth_col': 'SB_DEPTH_cm'\n",
    "    },\n",
    "    'rgb': {\n",
    "        'data_cols': ['R', 'G', 'B', 'Lumin'],\n",
    "        'std_cols': ['R_std', 'G_std', 'B_std', 'Lumin_std'],\n",
    "        'depth_col': 'SB_DEPTH_cm'\n",
    "    },\n",
    "    'mst': {\n",
    "        'density':  {'data_col': 'Den_gm/cc',    'depth_col': 'SB_DEPTH_cm'},\n",
    "        'pwvel':    {'data_col': 'PWVel_m/s',    'depth_col': 'SB_DEPTH_cm'},\n",
    "        'pwamp':    {'data_col': 'PWAmp',        'depth_col': 'SB_DEPTH_cm'},\n",
    "        # 'elecres':  {'data_col': 'ElecRes_ohmm', 'depth_col': 'SB_DEPTH_cm'},\n",
    "        'ms':       {'data_col': 'MS',           'depth_col': 'SB_DEPTH_cm'},\n",
    "    },\n",
    "    'hrms': {\n",
    "        'data_col': 'hiresMS',\n",
    "        'depth_col': 'SB_DEPTH_cm'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Paths to each CSV\n",
    "clean_data_paths = {\n",
    "    'ct': mother_dir + clean_output_folder + f'{core_name}_CT_clean.csv',\n",
    "    'rgb': mother_dir + clean_output_folder + f'{core_name}_RGB_clean.csv', \n",
    "    'mst': mother_dir + clean_output_folder + f'{core_name}_MST_clean.csv',\n",
    "    'hrms': mother_dir + clean_output_folder + f'{core_name}_hiresMS_clean.csv'\n",
    "}\n",
    "\n",
    "filled_data_paths = {\n",
    "    'ct':  os.path.join(mother_dir, filled_output_folder, f'{core_name}_CT_MLfilled.csv'),\n",
    "    'rgb': os.path.join(mother_dir, filled_output_folder, f'{core_name}_RGB_MLfilled.csv'),\n",
    "    'mst': os.path.join(mother_dir, filled_output_folder, f'{core_name}_MST_MLfilled.csv'),\n",
    "    'hrms':os.path.join(mother_dir, filled_output_folder, f'{core_name}_hiresMS_MLfilled.csv')\n",
    "}\n",
    "\n",
    "df_ct = pd.read_csv(filled_data_paths['ct'])\n",
    "df_rgb = pd.read_csv(filled_data_paths['rgb'])\n",
    "df_mst = pd.read_csv(filled_data_paths['mst'])\n",
    "df_hrms = pd.read_csv(filled_data_paths['hrms'])\n",
    "\n",
    "print(\"Shapes of loaded DataFrames:\")\n",
    "print(\"df_ct:\", df_ct.shape)\n",
    "print(\"df_rgb:\", df_rgb.shape)\n",
    "print(\"df_mst:\", df_mst.shape)\n",
    "print(\"df_hrms:\", df_hrms.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and Rename Columns\n",
    "Use the `column_configs` dictionary to subset and rename columns for each dataset, ensuring a consistent `'depth'` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CT subset\n",
    "ct_depth_col = column_configs['ct']['depth_col']\n",
    "ct_data_col = column_configs['ct']['data_col']\n",
    "ct_std_col = column_configs['ct']['std_col']\n",
    "df_ct_subset = df_ct[[ct_depth_col, ct_data_col, ct_std_col]].copy()\n",
    "df_ct_subset.rename(columns={\n",
    "    ct_depth_col: 'depth',\n",
    "    ct_data_col: 'CT',\n",
    "    ct_std_col: 'CT_std'\n",
    "}, inplace=True)\n",
    "\n",
    "# RGB subset\n",
    "rgb_depth_col = column_configs['rgb']['depth_col']\n",
    "rgb_data_cols = column_configs['rgb']['data_cols']  # ['R','G','B','Lumin']\n",
    "rgb_std_cols = column_configs['rgb']['std_cols']   # ['R_std','G_std','B_std','Lumin_std']\n",
    "df_rgb_subset = df_rgb[[rgb_depth_col] + rgb_data_cols + rgb_std_cols].copy()\n",
    "df_rgb_subset.rename(columns={\n",
    "    rgb_depth_col: 'depth',\n",
    "    'R': 'R', 'G': 'G', 'B': 'B', 'Lumin': 'Lumin',\n",
    "    'R_std': 'R_std', 'G_std': 'G_std', 'B_std': 'B_std', 'Lumin_std': 'Lumin_std'\n",
    "}, inplace=True)\n",
    "\n",
    "# MST subset\n",
    "mst_config = column_configs['mst']\n",
    "mst_depth_col = mst_config['density']['depth_col']\n",
    "mst_cols = [\n",
    "    mst_config['density']['data_col'],\n",
    "    mst_config['pwvel']['data_col'],\n",
    "    mst_config['pwamp']['data_col'],\n",
    "    # mst_config['elecres']['data_col'],\n",
    "    mst_config['ms']['data_col']\n",
    "]\n",
    "df_mst_subset = df_mst[[mst_depth_col] + mst_cols].copy()\n",
    "df_mst_subset.rename(columns={\n",
    "    mst_depth_col: 'depth',\n",
    "    mst_config['density']['data_col']:  'Density',\n",
    "    mst_config['pwvel']['data_col']:    'PWVel',\n",
    "    mst_config['pwamp']['data_col']:    'PWAmp',\n",
    "    # mst_config['elecres']['data_col']:  'ElecRes',\n",
    "    mst_config['ms']['data_col']:       'MS'\n",
    "}, inplace=True)\n",
    "\n",
    "# hi-res MS subset\n",
    "hrms_depth_col = column_configs['hrms']['depth_col']\n",
    "hrms_data_col  = column_configs['hrms']['data_col']\n",
    "df_hrms_subset = df_hrms[[hrms_depth_col, hrms_data_col]].copy()\n",
    "df_hrms_subset.rename(columns={\n",
    "    hrms_depth_col: 'depth',\n",
    "    hrms_data_col:  'hiresMS'\n",
    "}, inplace=True)\n",
    "\n",
    "# # Create a common depth scale based on hrms (high-resolution magnetic susceptibility)\n",
    "# Generate a depth scale with 2 times higher resolution than hiresMS\n",
    "min_depth = df_hrms_subset['depth'].min()\n",
    "max_depth = df_hrms_subset['depth'].max()\n",
    "# Calculate the average step size in the original hiresMS data\n",
    "avg_step = np.mean(np.diff(np.sort(df_hrms_subset['depth'].unique())))\n",
    "# Create new depth scale with the same resolution as hiresMS\n",
    "common_depths = np.arange(min_depth, max_depth + avg_step/4, avg_step/2)   #resample to 2x of the hiresMS depth resolution\n",
    "\n",
    "# Create a common depth scale based on hrms (high-resolution magnetic susceptibility)\n",
    "# Generate a depth scale with 10 times higher resolution than hiresMS\n",
    "# min_depth = df_hrms_subset['depth'].min()\n",
    "# max_depth = df_hrms_subset['depth'].max()\n",
    "# # Calculate the average step size in the original hiresMS data\n",
    "# avg_step = np.mean(np.diff(np.sort(df_hrms_subset['depth'].unique())))\n",
    "# # Create new depth scale with twice the resolution (half the step size)\n",
    "# common_depths = np.arange(min_depth, max_depth + avg_step/225, avg_step/15)  #resample to 10x of the hiresMS depth resolution\n",
    "\n",
    "# CT data resampling\n",
    "ct_interp = interp1d(df_ct_subset['depth'], df_ct_subset['CT'], \n",
    "                    bounds_error=False, fill_value=np.nan)\n",
    "ct_std_interp = interp1d(df_ct_subset['depth'], df_ct_subset['CT_std'], \n",
    "                        bounds_error=False, fill_value=np.nan)\n",
    "\n",
    "# RGB data resampling\n",
    "r_interp = interp1d(df_rgb_subset['depth'], df_rgb_subset['R'], \n",
    "                   bounds_error=False, fill_value=np.nan)\n",
    "g_interp = interp1d(df_rgb_subset['depth'], df_rgb_subset['G'], \n",
    "                   bounds_error=False, fill_value=np.nan)\n",
    "b_interp = interp1d(df_rgb_subset['depth'], df_rgb_subset['B'], \n",
    "                   bounds_error=False, fill_value=np.nan)\n",
    "lumin_interp = interp1d(df_rgb_subset['depth'], df_rgb_subset['Lumin'], \n",
    "                       bounds_error=False, fill_value=np.nan)\n",
    "r_std_interp = interp1d(df_rgb_subset['depth'], df_rgb_subset['R_std'], \n",
    "                       bounds_error=False, fill_value=np.nan)\n",
    "g_std_interp = interp1d(df_rgb_subset['depth'], df_rgb_subset['G_std'], \n",
    "                       bounds_error=False, fill_value=np.nan)\n",
    "b_std_interp = interp1d(df_rgb_subset['depth'], df_rgb_subset['B_std'], \n",
    "                       bounds_error=False, fill_value=np.nan)\n",
    "lumin_std_interp = interp1d(df_rgb_subset['depth'], df_rgb_subset['Lumin_std'], \n",
    "                           bounds_error=False, fill_value=np.nan)\n",
    "\n",
    "# MST data resampling\n",
    "density_interp = interp1d(df_mst_subset['depth'], df_mst_subset['Density'], \n",
    "                         bounds_error=False, fill_value=np.nan)\n",
    "pwvel_interp = interp1d(df_mst_subset['depth'], df_mst_subset['PWVel'], \n",
    "                       bounds_error=False, fill_value=np.nan)\n",
    "pwamp_interp = interp1d(df_mst_subset['depth'], df_mst_subset['PWAmp'], \n",
    "                       bounds_error=False, fill_value=np.nan)\n",
    "ms_interp = interp1d(df_mst_subset['depth'], df_mst_subset['MS'], \n",
    "                    bounds_error=False, fill_value=np.nan)\n",
    "\n",
    "# Get interpolated values\n",
    "ct_values = ct_interp(common_depths)\n",
    "ct_std_values = ct_std_interp(common_depths)\n",
    "r_values = r_interp(common_depths)\n",
    "g_values = g_interp(common_depths)\n",
    "b_values = b_interp(common_depths)\n",
    "lumin_values = lumin_interp(common_depths)\n",
    "r_std_values = r_std_interp(common_depths)\n",
    "g_std_values = g_std_interp(common_depths)\n",
    "b_std_values = b_std_interp(common_depths)\n",
    "lumin_std_values = lumin_std_interp(common_depths)\n",
    "density_values = density_interp(common_depths)\n",
    "pwvel_values = pwvel_interp(common_depths)\n",
    "pwamp_values = pwamp_interp(common_depths)\n",
    "ms_values = ms_interp(common_depths)\n",
    "\n",
    "# Apply smoothing using a Savitzky-Golay filter\n",
    "window_length = 5  # Must be odd number\n",
    "polyorder = 3  # Polynomial order for fitting\n",
    "edge_points = window_length // 2  # Number of points to keep unsmoothed at each edge\n",
    "\n",
    "# Helper function to apply smoothing while preserving edges\n",
    "def smooth_with_edges(data, window_length, polyorder):\n",
    "    smoothed = savgol_filter(data, window_length, polyorder)\n",
    "    # Keep original values at edges\n",
    "    smoothed[:edge_points] = data[:edge_points]\n",
    "    smoothed[-edge_points:] = data[-edge_points:]\n",
    "    return smoothed\n",
    "\n",
    "# Apply smoothing to all values\n",
    "ct_values = smooth_with_edges(ct_values, window_length, polyorder)\n",
    "ct_std_values = smooth_with_edges(ct_std_values, window_length, polyorder)\n",
    "r_values = smooth_with_edges(r_values, window_length, polyorder)\n",
    "g_values = smooth_with_edges(g_values, window_length, polyorder)\n",
    "b_values = smooth_with_edges(b_values, window_length, polyorder)\n",
    "lumin_values = smooth_with_edges(lumin_values, window_length, polyorder)\n",
    "r_std_values = smooth_with_edges(r_std_values, window_length, polyorder)\n",
    "g_std_values = smooth_with_edges(g_std_values, window_length, polyorder)\n",
    "b_std_values = smooth_with_edges(b_std_values, window_length, polyorder)\n",
    "lumin_std_values = smooth_with_edges(lumin_std_values, window_length, polyorder)\n",
    "density_values = smooth_with_edges(density_values, window_length, polyorder)\n",
    "pwvel_values = smooth_with_edges(pwvel_values, window_length, polyorder)\n",
    "pwamp_values = smooth_with_edges(pwamp_values, window_length, polyorder)\n",
    "ms_values = smooth_with_edges(ms_values, window_length, polyorder)\n",
    "\n",
    "# Create resampled dataframes with smoothed data\n",
    "df_ct_resampled = pd.DataFrame({\n",
    "    'depth': common_depths,\n",
    "    'CT': ct_values,\n",
    "    'CT_std': ct_std_values\n",
    "})\n",
    "\n",
    "df_rgb_resampled = pd.DataFrame({\n",
    "    'depth': common_depths,\n",
    "    'R': r_values,\n",
    "    'G': g_values,\n",
    "    'B': b_values,\n",
    "    'Lumin': lumin_values,\n",
    "    'R_std': r_std_values,\n",
    "    'G_std': g_std_values,\n",
    "    'B_std': b_std_values,\n",
    "    'Lumin_std': lumin_std_values\n",
    "})\n",
    "\n",
    "df_mst_resampled = pd.DataFrame({\n",
    "    'depth': common_depths,\n",
    "    'Density': density_values,\n",
    "    'PWVel': pwvel_values,\n",
    "    'PWAmp': pwamp_values,\n",
    "    'MS': ms_values\n",
    "})\n",
    "\n",
    "# Also resample hiresMS to the new higher resolution depth scale\n",
    "hiresms_interp = interp1d(df_hrms_subset['depth'], df_hrms_subset['hiresMS'],\n",
    "                         bounds_error=False, fill_value=np.nan)\n",
    "df_hrms_subset = pd.DataFrame({\n",
    "    'depth': common_depths,\n",
    "    'hiresMS': hiresms_interp(common_depths)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Data\n",
    "Join the four subset DataFrames on the `'depth'` column (using outer merges) and sort by depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the resampled dataframes\n",
    "df_merged = df_ct_resampled.merge(df_rgb_resampled, on='depth', how='outer', suffixes=(None, '_rgb'))\n",
    "df_merged = df_merged.merge(df_mst_resampled, on='depth', how='outer', suffixes=(None, '_mst'))\n",
    "df_merged = df_merged.merge(df_hrms_subset, on='depth', how='outer')\n",
    "\n",
    "df_merged.sort_values(by='depth', inplace=True)\n",
    "df_merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Merged Data shape:\", df_merged.shape)\n",
    "df_merged.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Trying Different Numbers of Clusters\n",
    "\n",
    "You can loop over several k values (say, 2 to 8), compute the inertia (within-cluster sum of squares) or silhouette score, and then compare. Here’s an example of the elbow method.\n",
    "\n",
    "\t•\tInertia often decreases quickly at first, then levels off. That “elbow” can guide your choice of k.\n",
    "\t•\tThe silhouette score ranges between -1 (poor) and 1 (great). A higher value usually indicates more distinct clusters.\n",
    "\n",
    "Once you pick a k, you can run the final K-Means again (like you already do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to cluster on\n",
    "feature_cols = [\n",
    "    'CT', \n",
    "    # 'CT_std',\n",
    "    # 'R', 'G', 'B', 'Lumin',\n",
    "    'Lumin',\n",
    "    # 'R_std', 'G_std', 'B_std', 'Lumin_std',\n",
    "    'Density',\n",
    "    #'MS',\n",
    "    # 'ElecRes', \n",
    "    'hiresMS',\n",
    "    # Include PWVel and PWAmp but with lower weight\n",
    "    # 'PWVel', 'PWAmp'\n",
    "]\n",
    "\n",
    "# Check if there are any rows with complete data for the selected features\n",
    "df_for_clustering = df_merged.dropna(subset=feature_cols).copy()\n",
    "print(f\"Number of complete rows for clustering: {len(df_for_clustering)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elbow Method (using inertia):**\n",
    "- Inertia is the sum of squared distances between each data point and its closest cluster center. The code calculates inertia for different numbers of clusters (k=2 to k=8)\n",
    "- Lower inertia means the clusters are more compact (data points are closer to their cluster centers).\n",
    "- Look for the \"**elbow point**\" where adding more clusters doesn't significantly reduce inertia. This point represents a good balance between number of clusters and model complexity.\n",
    "\n",
    "**Silhouette Analysis:**\n",
    "- Measures how similar an object is to its own cluster compared to other clusters\n",
    "- Scores range from -1 to 1 (higher is better). **A high score indicates well-separated clusters.**\n",
    "- Look for the peak in silhouette scores, which indicates where the clusters are best defined and separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Only proceed with clustering if we have data\n",
    "if len(df_for_clustering) > 0:\n",
    "    # Create a copy of the data for scaling\n",
    "    X = df_for_clustering[feature_cols].values\n",
    "    depth_array = df_for_clustering['depth'].values\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply lower weights to PWVel, PWAmp, and MS\n",
    "    # Get the indices of PWVel, PWAmp, MS, and standard deviation columns in the feature_cols list\n",
    "    # pwvel_idx = feature_cols.index('PWVel')\n",
    "    # pwamp_idx = feature_cols.index('PWAmp')\n",
    "    # ms_idx = feature_cols.index('MS')\n",
    "    # ct_std_idx = feature_cols.index('CT_std')\n",
    "    # r_std_idx = feature_cols.index('R_std')\n",
    "    # g_std_idx = feature_cols.index('G_std')\n",
    "    # b_std_idx = feature_cols.index('B_std')\n",
    "    # lumin_std_idx = feature_cols.index('Lumin_std')\n",
    "    \n",
    "    # Apply weights to reduce influence of some low-resolution logs in the clustering analysis\n",
    "    # X_scaled[:, pwvel_idx] *= 0.05\n",
    "    # X_scaled[:, pwamp_idx] *= 0.05\n",
    "    # X_scaled[:, ms_idx] *= 0.25\n",
    "    \n",
    "    # # Apply lower weights to standard deviation columns\n",
    "    # X_scaled[:, ct_std_idx] *= 0.05\n",
    "    # X_scaled[:, r_std_idx] *= 0.05    \n",
    "    # X_scaled[:, g_std_idx] *= 0.05\n",
    "    # X_scaled[:, b_std_idx] *= 0.05\n",
    "    # X_scaled[:, lumin_std_idx] *= 0.05\n",
    "else:\n",
    "    print(\"Warning: No complete data rows found for the selected features.\")\n",
    "    # Create an empty DataFrame with the necessary columns for downstream code\n",
    "    df_for_clustering = pd.DataFrame(columns=feature_cols + ['depth', 'cluster_label', 'rock_type'])\n",
    "\n",
    "\n",
    "# Test different outlier thresholds to find the optimal clustering result\n",
    "\n",
    "outlier_thresholds = [i * 0.2 for i in range(1, 31)]\n",
    "silhouette_scores = {k: [] for k in [2, 3, 4]}\n",
    "inertia_values = {k: [] for k in [2, 3, 4]}\n",
    "sample_counts = []\n",
    "cluster_results = {k: [] for k in [2, 3, 4]}\n",
    "\n",
    "for threshold in outlier_thresholds:\n",
    "    # Select features and handle missing data\n",
    "    df_temp = df_merged.dropna(subset=feature_cols).copy()\n",
    "    \n",
    "    # Calculate z-scores and filter outliers based on current threshold\n",
    "    z_scores = stats.zscore(df_temp[feature_cols])\n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    outlier_mask = (abs_z_scores < threshold).all(axis=1)\n",
    "    df_filtered = df_temp[outlier_mask]\n",
    "    \n",
    "    # Store the sample count (only need to do this once per threshold)\n",
    "    sample_count = len(df_filtered)\n",
    "    sample_counts.append(sample_count)\n",
    "    \n",
    "    # Test each k value\n",
    "    for k in [2, 3, 4]:\n",
    "        # Only proceed if we have enough data\n",
    "        if sample_count > k:\n",
    "            # Scale the data\n",
    "            X = df_filtered[feature_cols].values\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            \n",
    "            # Apply K-means\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(X_scaled)\n",
    "            \n",
    "            # Calculate silhouette score and inertia\n",
    "            sil_score = silhouette_score(X_scaled, labels) if len(np.unique(labels)) > 1 else 0\n",
    "            silhouette_scores[k].append(sil_score)\n",
    "            inertia_values[k].append(kmeans.inertia_)\n",
    "            \n",
    "            # Store the clustering results for later visualization\n",
    "            cluster_results[k].append({\n",
    "                'threshold': threshold,\n",
    "                'X_scaled': X_scaled,\n",
    "                'labels': labels,\n",
    "                'silhouette': sil_score,\n",
    "                'sample_count': sample_count\n",
    "            })\n",
    "        else:\n",
    "            silhouette_scores[k].append(0)\n",
    "            inertia_values[k].append(float('inf'))\n",
    "\n",
    "# Plot the evaluation metrics in a 1x3 grid\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Line styles and colors for different k values\n",
    "styles = {\n",
    "    2: ('b-', 'k=2'),\n",
    "    3: ('r--', 'k=3'),\n",
    "    4: ('g:', 'k=4')\n",
    "}\n",
    "\n",
    "# Silhouette Score vs Outlier Threshold\n",
    "plt.subplot(1, 3, 1)\n",
    "for k in [2, 3, 4]:\n",
    "    style, label = styles[k]\n",
    "    plt.plot(outlier_thresholds, silhouette_scores[k], style, linewidth=2, label=label)\n",
    "plt.xlabel('Outlier Threshold', fontsize=12)\n",
    "plt.ylabel('Silhouette Score', fontsize=12)\n",
    "plt.title('Silhouette Score vs Outlier Threshold', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "# Sample Count vs Outlier Threshold\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(outlier_thresholds, sample_counts, 'k-', linewidth=2, label='All k')\n",
    "plt.xlabel('Outlier Threshold', fontsize=12)\n",
    "plt.ylabel('Number of Samples', fontsize=12)\n",
    "plt.title('Sample Count vs Outlier Threshold', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "# Inertia vs Outlier Threshold\n",
    "plt.subplot(1, 3, 3)\n",
    "for k in [2, 3, 4]:\n",
    "    style, label = styles[k]\n",
    "    valid_indices = [i for i, inertia in enumerate(inertia_values[k]) if inertia != float('inf')]\n",
    "    valid_thresholds = [outlier_thresholds[i] for i in valid_indices]\n",
    "    valid_inertias = [inertia_values[k][i] for i in valid_indices]\n",
    "    \n",
    "    if valid_inertias:\n",
    "        plt.plot(valid_thresholds, valid_inertias, style, linewidth=2, label=label)\n",
    "\n",
    "plt.xlabel('Outlier Threshold', fontsize=12)\n",
    "plt.ylabel('Inertia', fontsize=12)\n",
    "plt.title('Inertia vs Outlier Threshold', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print information about the best threshold for each k\n",
    "for k in [2, 3, 4]:\n",
    "    if silhouette_scores[k]:\n",
    "        best_idx = np.argmax(silhouette_scores[k])\n",
    "        best_threshold = outlier_thresholds[best_idx]\n",
    "        print(f\"\\nFor k={k}:\")\n",
    "        print(f\"Best outlier threshold: {best_threshold} with silhouette score: {silhouette_scores[k][best_idx]:.3f}\")\n",
    "        print(f\"Number of samples with best threshold: {sample_counts[best_idx]}\")\n",
    "        print(f\"Inertia with best threshold: {inertia_values[k][best_idx] if inertia_values[k][best_idx] != float('inf') else 'N/A'}\")\n",
    "    else:\n",
    "        print(f\"\\nFor k={k}: No valid clustering results were obtained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed with clustering if we have data\n",
    "if len(df_for_clustering) > 0:\n",
    "    # Create a copy of the data for scaling\n",
    "    X = df_for_clustering[feature_cols].values\n",
    "    depth_array = df_for_clustering['depth'].values\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply lower weights to PWVel, PWAmp, and MS\n",
    "    # Get the indices of PWVel, PWAmp, MS, and standard deviation columns in the feature_cols list\n",
    "    # pwvel_idx = feature_cols.index('PWVel')\n",
    "    # pwamp_idx = feature_cols.index('PWAmp')\n",
    "    # ms_idx = feature_cols.index('MS')\n",
    "    # ct_std_idx = feature_cols.index('CT_std')\n",
    "    # r_std_idx = feature_cols.index('R_std')\n",
    "    # g_std_idx = feature_cols.index('G_std')\n",
    "    # b_std_idx = feature_cols.index('B_std')\n",
    "    # lumin_std_idx = feature_cols.index('Lumin_std')\n",
    "    \n",
    "    # Apply weights to reduce influence of some low-resolution logs in the clustering analysis\n",
    "    # X_scaled[:, pwvel_idx] *= 0.00\n",
    "    # X_scaled[:, pwamp_idx] *= 0.00\n",
    "    # X_scaled[:, ms_idx] *= 0.25\n",
    "    \n",
    "    # # Apply lower weights to standard deviation columns\n",
    "    # X_scaled[:, ct_std_idx] *= 0.05\n",
    "    # X_scaled[:, r_std_idx] *= 0.05    \n",
    "    # X_scaled[:, g_std_idx] *= 0.05\n",
    "    # X_scaled[:, b_std_idx] *= 0.05\n",
    "    # X_scaled[:, lumin_std_idx] *= 0.05\n",
    "else:\n",
    "    print(\"Warning: No complete data rows found for the selected features.\")\n",
    "    # Create an empty DataFrame with the necessary columns for downstream code\n",
    "    df_for_clustering = pd.DataFrame(columns=feature_cols + ['depth', 'cluster_label', 'rock_type'])\n",
    "\n",
    "\n",
    "inertias = []\n",
    "sil_scores = []\n",
    "K_range = range(2, 11)\n",
    "thresholds = [2, 3, 4, 5]\n",
    "styles = ['g--', 'r:', 'm-.', 'b--']  # Different line styles for each threshold\n",
    "\n",
    "# Create dictionaries to store results for each threshold\n",
    "inertias_by_threshold = {t: [] for t in thresholds}\n",
    "sil_scores_by_threshold = {t: [] for t in thresholds}\n",
    "\n",
    "# Calculate metrics for each threshold and k\n",
    "for threshold in thresholds:\n",
    "    # Apply outlier filtering for this threshold\n",
    "    z_scores = stats.zscore(X_scaled)\n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    outlier_mask = (abs_z_scores < threshold).all(axis=1)\n",
    "    X_filtered = X_scaled[outlier_mask]\n",
    "    \n",
    "    if len(X_filtered) > 1:  # Ensure we have enough samples\n",
    "        for k in K_range:\n",
    "            kmeans_temp = KMeans(n_clusters=k, random_state=42)\n",
    "            kmeans_temp.fit(X_filtered)\n",
    "            \n",
    "            # Store inertia\n",
    "            inertias_by_threshold[threshold].append(kmeans_temp.inertia_)\n",
    "            \n",
    "            # Store silhouette score if we have enough samples\n",
    "            if len(X_filtered) > k:\n",
    "                labels_temp = kmeans_temp.labels_\n",
    "                score = silhouette_score(X_filtered, labels_temp)\n",
    "                sil_scores_by_threshold[threshold].append(score)\n",
    "            else:\n",
    "                sil_scores_by_threshold[threshold].append(np.nan)\n",
    "\n",
    "# Plot elbow curves\n",
    "plt.figure(figsize=(5,4))\n",
    "for threshold, style in zip(thresholds, styles):\n",
    "    if len(inertias_by_threshold[threshold]) > 0:\n",
    "        plt.plot(K_range, inertias_by_threshold[threshold], style, \n",
    "                linewidth=2, label=f'Threshold={threshold}')\n",
    "\n",
    "plt.xlabel(\"Number of clusters (k)\", fontsize=12)\n",
    "plt.ylabel(\"Inertia\", fontsize=12)\n",
    "plt.title(\"Elbow Method for Optimal k\", fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(5,4))\n",
    "for threshold, style in zip(thresholds, styles):\n",
    "    if len(sil_scores_by_threshold[threshold]) > 0:\n",
    "        plt.plot(K_range, sil_scores_by_threshold[threshold], style,\n",
    "                linewidth=2, label=f'Threshold={threshold}')\n",
    "\n",
    "plt.xlabel(\"Number of clusters (k)\", fontsize=12)\n",
    "plt.ylabel(\"Silhouette Score\", fontsize=12)\n",
    "plt.title(\"Silhouette Analysis\", fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal k for each threshold\n",
    "for threshold in thresholds:\n",
    "    if len(inertias_by_threshold[threshold]) > 0:\n",
    "        # Calculate the percentage change in inertia\n",
    "        inertias = inertias_by_threshold[threshold]\n",
    "        inertia_changes = np.diff(inertias) / np.array(inertias[:-1])\n",
    "        \n",
    "        # Find where the rate of change slows down significantly\n",
    "        elbow_candidates = np.where(np.abs(np.diff(inertia_changes)) > np.std(inertia_changes))[0]\n",
    "        if len(elbow_candidates) > 0:\n",
    "            optimal_k = K_range[elbow_candidates[0] + 1]\n",
    "        else:\n",
    "            # Fallback method\n",
    "            second_derivative = np.diff(np.diff(inertias))\n",
    "            optimal_k = K_range[np.argmax(np.abs(second_derivative)) + 2]\n",
    "            \n",
    "        # Find best silhouette score\n",
    "        sil_scores = sil_scores_by_threshold[threshold]\n",
    "        best_sil_k = K_range[np.nanargmax(sil_scores)]\n",
    "        \n",
    "        print(f\"\\nFor threshold {threshold}:\")\n",
    "        print(f\"Optimal k from elbow method: {optimal_k}\")\n",
    "        print(f\"Best k from silhouette score: {best_sil_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) K-Means Clustering\n",
    "Select numeric columns of interest, handle missing data, scale them, and run K-Means to find clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = 6.0\n",
    "k = 4\n",
    "\n",
    "# Use the optimal k and best outlier threshold found from previous analysis\n",
    "print(f\"Using optimal k={k} and best outlier threshold={best_threshold}\")\n",
    "\n",
    "# Check if there are any rows with complete data for the selected features\n",
    "df_for_clustering = df_merged.dropna(subset=feature_cols).copy()\n",
    "print(f\"Number of complete rows for clustering: {len(df_for_clustering)}\")\n",
    "\n",
    "# Apply outlier filtering using the newly defined best threshold\n",
    "z_scores = stats.zscore(df_for_clustering[feature_cols])\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outlier_mask = (abs_z_scores < best_threshold).all(axis=1)\n",
    "df_for_clustering = df_for_clustering[outlier_mask]\n",
    "\n",
    "print(f\"Number of rows after outlier removal: {len(df_for_clustering)}\")\n",
    "\n",
    "# Only proceed with clustering if we have data\n",
    "if len(df_for_clustering) > 0:\n",
    "    # Create a copy of the data for scaling\n",
    "    X = df_for_clustering[feature_cols].values\n",
    "    depth_array = df_for_clustering['depth'].values\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply lower weights to PWVel, PWAmp, and MS\n",
    "    # Get the indices of PWVel, PWAmp, MS, and standard deviation columns in the feature_cols list\n",
    "    # pwvel_idx = feature_cols.index('PWVel')\n",
    "    # pwamp_idx = feature_cols.index('PWAmp')\n",
    "    # ms_idx = feature_cols.index('MS')\n",
    "    # # ct_std_idx = feature_cols.index('CT_std')\n",
    "    # r_std_idx = feature_cols.index('R_std')\n",
    "    # g_std_idx = feature_cols.index('G_std')\n",
    "    # b_std_idx = feature_cols.index('B_std')\n",
    "    # lumin_std_idx = feature_cols.index('Lumin_std')\n",
    "    \n",
    "    # # Apply weights to reduce influence of some low-resolution logs in the clustering analysis\n",
    "    # X_scaled[:, pwvel_idx] *= 0.00\n",
    "    # X_scaled[:, pwamp_idx] *= 0.00\n",
    "    # X_scaled[:, ms_idx] *= 0.25\n",
    "    \n",
    "    # # Apply lower weights to standard deviation columns\n",
    "    # X_scaled[:, ct_std_idx] *= 0.05\n",
    "    # X_scaled[:, r_std_idx] *= 0.05    \n",
    "    # X_scaled[:, g_std_idx] *= 0.05\n",
    "    # X_scaled[:, b_std_idx] *= 0.05\n",
    "    # X_scaled[:, lumin_std_idx] *= 0.05\n",
    "\n",
    "    # Use the optimal k from previous analysis\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "\n",
    "    labels = kmeans.labels_\n",
    "    df_for_clustering['cluster_label'] = labels\n",
    "    \n",
    "    # Calculate cluster means to determine cluster characteristics\n",
    "    cluster_means = df_for_clustering.groupby('cluster_label')[feature_cols].mean()\n",
    "    print(\"Cluster means:\")\n",
    "    print(cluster_means)\n",
    "    \n",
    "    # Determine cluster names based on characteristics\n",
    "    # Initialize dictionaries for cluster names and colors\n",
    "    cluster_names = {}\n",
    "    cluster_colors = {}\n",
    "    \n",
    "    # Find the cluster with high hiresMS, density, MS, CT and low RGB/Lumin (Turbidite Sand)\n",
    "    # sand_indicators = ['hiresMS', 'Density', 'MS', 'CT']\n",
    "    sand_indicators = ['hiresMS', 'Density', 'CT']\n",
    "    sand_scores = cluster_means[sand_indicators].mean(axis=1)\n",
    "    \n",
    "    # Find the cluster with high RGB and Lumin (Pelagic Mud)\n",
    "    # pelagic_indicators = ['R', 'G', 'B', 'Lumin']\n",
    "    pelagic_indicators = ['Lumin']\n",
    "    pelagic_scores = cluster_means[pelagic_indicators].mean(axis=1)\n",
    "    \n",
    "    # Assign cluster names based on characteristics\n",
    "    turbidite_sand_cluster = sand_scores.idxmax()\n",
    "    pelagic_mud_cluster = pelagic_scores.idxmax()\n",
    "    \n",
    "    # The remaining cluster is Turbidite Mud\n",
    "    all_clusters = set(range(k))\n",
    "    assigned_clusters = {turbidite_sand_cluster, pelagic_mud_cluster}\n",
    "    remaining_clusters = all_clusters - assigned_clusters\n",
    "    \n",
    "    # If we have a conflict (same cluster identified for two types), resolve it\n",
    "    if len(remaining_clusters) < 1:\n",
    "        # Re-evaluate using more specific criteria\n",
    "        # For turbidite sand: high density and CT are most important\n",
    "        sand_specific = cluster_means[['Density', 'CT']].mean(axis=1)\n",
    "        turbidite_sand_cluster = sand_specific.idxmax()\n",
    "        \n",
    "        # For pelagic mud: high RGB values are most important\n",
    "        # pelagic_specific = cluster_means[['R', 'G', 'B']].mean(axis=1)\n",
    "        pelagic_specific = cluster_means[['Lumin']].mean(axis=1)\n",
    "        pelagic_mud_cluster = pelagic_specific.idxmax()\n",
    "        \n",
    "        # Recalculate remaining clusters\n",
    "        assigned_clusters = {turbidite_sand_cluster, pelagic_mud_cluster}\n",
    "        remaining_clusters = all_clusters - assigned_clusters\n",
    "    \n",
    "    # Assign names and colors based on number of clusters\n",
    "    if k == 2:\n",
    "        # For k=2, assign Turbidite Sand to cluster with low RGB/Lum and high MS/Density\n",
    "        # rgb_lum_scores = cluster_means[['R', 'G', 'B', 'Lumin']].mean(axis=1)\n",
    "        rgb_lum_scores = cluster_means[['Lumin']].mean(axis=1)\n",
    "        # ms_density_scores = cluster_means[['MS', 'hiresMS', 'Density']].mean(axis=1)\n",
    "        ms_density_scores = cluster_means[['Density']].mean(axis=1)\n",
    "        \n",
    "        # Normalize scores\n",
    "        rgb_lum_norm = (rgb_lum_scores - rgb_lum_scores.min()) / (rgb_lum_scores.max() - rgb_lum_scores.min())\n",
    "        ms_density_norm = (ms_density_scores - ms_density_scores.min()) / (ms_density_scores.max() - ms_density_scores.min())\n",
    "        \n",
    "        # Calculate combined score (low RGB/Lum and high MS/Density)\n",
    "        combined_score = ms_density_norm - rgb_lum_norm\n",
    "        turbidite_sand_cluster = combined_score.idxmax()\n",
    "        \n",
    "        # Assign names and colors\n",
    "        cluster_names[turbidite_sand_cluster] = 'Turbidite Sand'\n",
    "        cluster_colors[turbidite_sand_cluster] = 'goldenrod'\n",
    "        \n",
    "        # Other cluster is Mud\n",
    "        mud_cluster = 1 if turbidite_sand_cluster == 0 else 0\n",
    "        cluster_names[mud_cluster] = 'Mud'\n",
    "        cluster_colors[mud_cluster] = 'brown'\n",
    "        \n",
    "    else:\n",
    "        # For k>=3, first three clusters follow original naming\n",
    "        cluster_names[turbidite_sand_cluster] = 'Turbidite Sand'\n",
    "        cluster_colors[turbidite_sand_cluster] = 'goldenrod'\n",
    "        \n",
    "        cluster_names[pelagic_mud_cluster] = 'Pelagic Mud'\n",
    "        cluster_colors[pelagic_mud_cluster] = 'skyblue'\n",
    "        \n",
    "        # Assign the remaining cluster as Turbidite Mud\n",
    "        if remaining_clusters:\n",
    "            turbidite_mud_cluster = list(remaining_clusters)[0]\n",
    "            cluster_names[turbidite_mud_cluster] = 'Turbidite Mud'\n",
    "            cluster_colors[turbidite_mud_cluster] = 'brown'\n",
    "        \n",
    "        # For k=4, use a color between brown and skyblue\n",
    "        if k == 4:\n",
    "            for i in list(all_clusters):\n",
    "                if i not in cluster_names:\n",
    "                    cluster_names[i] = f'Turbidite Laminae'\n",
    "                    # Mix of brown and skyblue\n",
    "                    cluster_colors[i] = '#B38E50'  # A color between goldenrod and brown\n",
    "        \n",
    "        # For k>4, additional clusters get numbered names and random colors\n",
    "        elif k > 4:\n",
    "            for i in list(all_clusters):\n",
    "                if i not in cluster_names:\n",
    "                    cluster_names[i] = f'Cluster {i}'\n",
    "                    r = random.random()\n",
    "                    g = random.random()\n",
    "                    b = random.random()\n",
    "                    cluster_colors[i] = (r, g, b)\n",
    "    \n",
    "    # Add cluster names to the dataframe\n",
    "    df_for_clustering['rock_type'] = df_for_clustering['cluster_label'].map(cluster_names)\n",
    "\n",
    "    print(\"Cluster distribution:\")\n",
    "    for cluster_id, name in cluster_names.items():\n",
    "        count = (df_for_clustering['cluster_label'] == cluster_id).sum()\n",
    "        print(f\"{cluster_id} - {name}: {count}\")\n",
    "    \n",
    "    # Visualize cluster characteristics with a radar chart\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "    # Get feature names for the radar chart\n",
    "    feature_names = feature_cols\n",
    "    \n",
    "    # Number of variables\n",
    "    N = len(feature_names)\n",
    "    \n",
    "    # Create angles for each feature\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Create the plot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    \n",
    "    # Draw one axis per variable and add labels\n",
    "    plt.xticks(angles[:-1], feature_names, size=8)\n",
    "    \n",
    "    # Set ytick labels to x-small font size\n",
    "    plt.yticks(size='x-small')\n",
    "    \n",
    "    # Add a darker line for 0.0\n",
    "    ax.plot(np.linspace(0, 2*np.pi, 100), [0]*100, color='gray', linewidth=1.0, linestyle='-', alpha=0.7)\n",
    "    \n",
    "    # Draw the cluster profiles\n",
    "    for i in range(k):\n",
    "        values = cluster_centers[i].tolist()\n",
    "        values += values[:1]  # Close the loop\n",
    "        if i in cluster_names and i in cluster_colors:\n",
    "            ax.plot(angles, values, linewidth=1.0, linestyle='solid', label=cluster_names[i], color=cluster_colors[i])\n",
    "            ax.fill(angles, values, alpha=0.5, color=cluster_colors[i])\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(\"Cluster Profiles (Normalized Features)\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize cluster distribution\n",
    "    plt.figure(figsize=(13, 5))\n",
    "    \n",
    "    # Plot 1: Pie chart of cluster distribution\n",
    "    plt.subplot(1, 3, 1)\n",
    "    cluster_counts = df_for_clustering['cluster_label'].value_counts().sort_index()\n",
    "    # Fix: Make sure all cluster indices are in the cluster_colors dictionary\n",
    "    colors = [cluster_colors.get(i, (0.5, 0.5, 0.5)) for i in cluster_counts.index]\n",
    "    plt.pie(cluster_counts, labels=[cluster_names.get(i, f'Cluster {i}') for i in cluster_counts.index], \n",
    "            autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "    plt.title('Rock Type Distribution (Pie Chart)')\n",
    "    \n",
    "    # Plot 2: Bar chart of cluster distribution\n",
    "    plt.subplot(1, 3, 2)\n",
    "    cluster_counts.plot(kind='bar', color=colors)\n",
    "    plt.title('Rock Type Distribution (Bar Chart)')\n",
    "    plt.xlabel('Rock Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(len(cluster_counts)), [cluster_names.get(i, f'Cluster {i}') for i in cluster_counts.index], rotation=45)\n",
    "    \n",
    "    # Plot 3: Depth vs Cluster visualization\n",
    "    plt.subplot(1, 3, 3)\n",
    "    \n",
    "    # Create a scatter plot with consistent colors\n",
    "    for cluster_id in df_for_clustering['cluster_label'].unique():\n",
    "        cluster_data = df_for_clustering[df_for_clustering['cluster_label'] == cluster_id]\n",
    "        plt.scatter([cluster_id] * len(cluster_data), cluster_data['depth'],\n",
    "                   color=cluster_colors.get(cluster_id, (0.5, 0.5, 0.5)), \n",
    "                   label=cluster_names.get(cluster_id, f'Cluster {cluster_id}'))\n",
    "    \n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(f\"Rock Types Classification\")\n",
    "    plt.xlabel(\"Rock Type\")\n",
    "    plt.ylabel(\"Depth\")\n",
    "    \n",
    "    # Add a custom color bar with meaningful labels\n",
    "    plt.xticks(sorted(df_for_clustering['cluster_label'].unique()), \n",
    "              [cluster_names.get(i, f'Cluster {i}') for i in sorted(df_for_clustering['cluster_label'].unique())], \n",
    "              rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Warning: No complete data rows found for the selected features.\")\n",
    "    # Create an empty DataFrame with the necessary columns for downstream code\n",
    "    df_for_clustering = pd.DataFrame(columns=feature_cols + ['depth', 'cluster_label', 'rock_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Visualization\n",
    "Plot cluster labels versus depth with a simple scatter. Depth on the y-axis, cluster label on the x-axis, color-coded by label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual feature plots to visualize cluster separation\n",
    "if len(df_for_clustering) > 0:\n",
    "    # Use all features for visualization\n",
    "    vis_features = feature_cols\n",
    "    \n",
    "    # Calculate number of feature pairs\n",
    "    n_features = len(vis_features)\n",
    "    n_pairs = n_features * (n_features - 1) // 2  # Number of unique pairs\n",
    "    \n",
    "    # Calculate number of rows and columns for subplots\n",
    "    n_cols = 6 # You can adjust this for layout\n",
    "    n_rows = (n_pairs + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5))\n",
    "    \n",
    "    # Flatten axes array for easier indexing\n",
    "    axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]\n",
    "    \n",
    "    # Plot each feature pair\n",
    "    plot_idx = 0\n",
    "    for i in range(n_features):\n",
    "        for j in range(i+1, n_features):\n",
    "            if plot_idx < len(axes):\n",
    "                ax = axes[plot_idx]\n",
    "                \n",
    "                # Get feature names for this pair\n",
    "                feat_x = vis_features[i]\n",
    "                feat_y = vis_features[j]\n",
    "                \n",
    "                # Scatter plot of feature pair\n",
    "                for cluster_id in range(k):\n",
    "                    cluster_points = df_for_clustering[df_for_clustering['cluster_label'] == cluster_id]\n",
    "                    ax.scatter(cluster_points[feat_x], cluster_points[feat_y], \n",
    "                              s=10, alpha=0.6, color=cluster_colors[cluster_id], \n",
    "                              label=cluster_names[cluster_id])\n",
    "                \n",
    "                # Set labels\n",
    "                ax.set_xlabel(feat_x, fontsize=8)\n",
    "                ax.set_ylabel(feat_y, fontsize=8)\n",
    "                ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "                \n",
    "                plot_idx += 1\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(plot_idx, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    # Add a single legend for the entire figure\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    # Place legend outside the figure at the bottom\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -0.01), ncol=k)\n",
    "    \n",
    "    # Adjust figure margins to make room for the legend\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Feature Pair Visualization by Rock Type\", y=1.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create individual feature plots to visualize cluster separation\n",
    "if len(df_for_clustering) > 0:\n",
    "    # Use all features for visualization\n",
    "    vis_features = feature_cols\n",
    "    \n",
    "    # Calculate number of rows and columns for subplots\n",
    "    n_features = len(vis_features)\n",
    "    n_cols = 11  # You can adjust this for layout\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 10))\n",
    "    \n",
    "    # Flatten axes array for easier indexing\n",
    "    axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]\n",
    "    \n",
    "    # Plot each feature\n",
    "    for i, feat in enumerate(vis_features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Scatter plot of feature vs depth\n",
    "            for cluster_id in range(k):\n",
    "                cluster_points = df_for_clustering[df_for_clustering['cluster_label'] == cluster_id]\n",
    "                ax.scatter(cluster_points[feat], cluster_points['depth'], \n",
    "                          s=8, alpha=0.6, color=cluster_colors[cluster_id], \n",
    "                          label=cluster_names[cluster_id])\n",
    "            \n",
    "            # Set labels\n",
    "            ax.set_xlabel(feat, fontsize=8)\n",
    "            # Only show y-axis label for the first column\n",
    "            if i % n_cols == 0:\n",
    "                ax.set_ylabel('Depth', fontsize=8)\n",
    "            else:\n",
    "                ax.set_ylabel('', fontsize=8)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "            ax.invert_yaxis()  # Invert y-axis for depth\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    # Add a single legend for the entire figure\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower right', ncol=1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Individual Feature Visualization by Rock Type\", y=1.01)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colored Lithology-Style Column\n",
    "Generate a vertically oriented color column indicating cluster assignments at each depth interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_clustering_sorted = df_for_clustering.sort_values(by='depth').reset_index(drop=True)\n",
    "depth_vals = df_for_clustering_sorted['depth'].values\n",
    "labels_vals = df_for_clustering_sorted['cluster_label'].values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(1.2,10))\n",
    "\n",
    "# Use the same cluster colors defined earlier\n",
    "colors = [cluster_colors[i] for i in range(k)]\n",
    "\n",
    "for i in range(len(depth_vals)-1):\n",
    "    top = depth_vals[i]\n",
    "    bottom = depth_vals[i+1]\n",
    "    c_label = labels_vals[i]\n",
    "    color = cluster_colors[c_label]\n",
    "    ax.add_patch(\n",
    "        mpatches.Rectangle((0, top), 1, bottom - top,\n",
    "                           facecolor=color, edgecolor='none')\n",
    "    )\n",
    "\n",
    "# Add a legend showing rock types\n",
    "handles = [mpatches.Patch(color=cluster_colors[i], label=cluster_names[i]) for i in range(k)]\n",
    "ax.legend(handles=handles, loc='center right', bbox_to_anchor=(1.5, 0.5), fontsize='x-small')\n",
    "\n",
    "ax.set_ylim([depth_vals.max(), depth_vals.min()])\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title(\"Rock Type Column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_core_logs(data_paths, column_configs, core_length, ct_img=None, rgb_img=None, title='Combined Logs', cluster_data=None, figsize=(10, 16)):\n",
    "    \"\"\"\n",
    "    Plot core logs from multiple data sources.\n",
    "    \n",
    "    Args:\n",
    "        data_paths (dict): Dictionary containing paths to data files\n",
    "            e.g. {a\n",
    "                'ct': 'path/to/ct.csv',\n",
    "                'rgb': 'path/to/rgb.csv', \n",
    "                'mst': 'path/to/mst.csv',\n",
    "                'hrms': 'path/to/hrms.csv'\n",
    "            }\n",
    "        column_configs (dict): Configuration for columns to plot\n",
    "            e.g. {\n",
    "                'ct': {'data_col': 'CT', 'std_col': 'CT_std', 'depth_col': 'SB_DEPTH_cm'},\n",
    "                'rgb': {'data_cols': ['R','G','B'], 'std_cols': ['R_std','G_std','B_std'], 'depth_col': 'SB_DEPTH_cm'},\n",
    "                'mst': {\n",
    "                    'density': {'data_col': 'Den_gm/cc', 'depth_col': 'SB_DEPTH_cm'},\n",
    "                    'pwvel': {'data_col': 'PWVel_m/s', 'depth_col': 'SB_DEPTH_cm'},\n",
    "                    'pwamp': {'data_col': 'PWAmp', 'depth_col': 'SB_DEPTH_cm'},\n",
    "                    'elecres': {'data_col': 'ElecRes_ohmm', 'depth_col': 'SB_DEPTH_cm'},\n",
    "                    'ms': {'data_col': 'MS', 'depth_col': 'SB_DEPTH_cm'}\n",
    "                },\n",
    "                'hrms': {'data_col': 'hiresMS', 'depth_col': 'SB_DEPTH_cm'}\n",
    "            }\n",
    "        core_length (float): Length of the core in cm\n",
    "        ct_img (array, optional): CT image array\n",
    "        rgb_img (array, optional): RGB image array\n",
    "        title (str): Title for the plot\n",
    "        cluster_data (dict, optional): Dictionary containing cluster data\n",
    "            e.g. {\n",
    "                'depth_vals': depth_values_array,\n",
    "                'labels_vals': cluster_labels_array,\n",
    "                'k': number_of_clusters\n",
    "            }\n",
    "        figsize (tuple, optional): Figure size as (width, height). Default is (10, 16)\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = {}\n",
    "    for key, path in data_paths.items():\n",
    "        if path:\n",
    "            data[key] = pd.read_csv(path)\n",
    "            if 'SB_DEPTH_cm' not in data[key].columns:\n",
    "                raise ValueError(f\"SB_DEPTH_cm column missing in {key} data\")\n",
    "    \n",
    "    # Calculate number of plots needed\n",
    "    n_plots = 0  # Start with 0\n",
    "    \n",
    "    # Add cluster column if cluster data exists\n",
    "    if cluster_data is not None:\n",
    "        n_plots += 1\n",
    "    \n",
    "    # Add CT panels if CT image and data exist\n",
    "    if ct_img is not None and 'ct' in data:\n",
    "        n_plots += 2  # CT image and data\n",
    "        \n",
    "    # Add RGB panels if RGB image and data exist  \n",
    "    if rgb_img is not None and 'rgb' in data:\n",
    "        n_plots += 2  # RGB image and data\n",
    "        \n",
    "    # Add MS panel if either MST MS data or hiresMS data exists and has data\n",
    "    if ('mst' in data and column_configs['mst']['ms']['data_col'] in data['mst'].columns and not data['mst'][column_configs['mst']['ms']['data_col']].isna().all()) or ('hrms' in data and not data['hrms'][column_configs['hrms']['data_col']].isna().all()):\n",
    "        n_plots += 1\n",
    "        \n",
    "    # Add panels for other MST logs that exist and have data\n",
    "    if 'mst' in data:\n",
    "        for log_type, config in column_configs['mst'].items():\n",
    "            if log_type != 'ms':  # Skip MS since it's handled separately\n",
    "                if config['data_col'] in data['mst'].columns and not data['mst'][config['data_col']].isna().all():\n",
    "                    n_plots += 1\n",
    "\n",
    "    if n_plots == 0:\n",
    "        raise ValueError(\"No valid data to plot\")\n",
    "    \n",
    "    # Create figure and axes\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=figsize, sharey=True)\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable\n",
    "    fig.suptitle(title, fontweight='bold')\n",
    "    \n",
    "    current_ax = 0\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Plot Cluster Column if available\n",
    "    # ---------------------------\n",
    "    if cluster_data is not None:\n",
    "        depth_vals = cluster_data['depth_vals']\n",
    "        labels_vals = cluster_data['labels_vals']\n",
    "        k = cluster_data['k']\n",
    "        \n",
    "        # Use the same cluster colors defined earlier\n",
    "        colors = [cluster_colors[i] for i in range(k)]\n",
    "        \n",
    "        for i in range(len(depth_vals)-1):\n",
    "            top = depth_vals[i]\n",
    "            bottom = depth_vals[i+1]\n",
    "            c_label = labels_vals[i]\n",
    "            color = cluster_colors[c_label]\n",
    "            axes[current_ax].add_patch(\n",
    "                mpatches.Rectangle((0, top), 1, bottom - top,\n",
    "                                  facecolor=color, edgecolor='none')\n",
    "            )\n",
    "        \n",
    "        # Add a legend showing rock types\n",
    "        handles = [mpatches.Patch(color=cluster_colors[i], label=cluster_names[i]) for i in range(k)]\n",
    "        axes[current_ax].legend(handles=handles, loc='center right', fontsize='x-small')\n",
    "        \n",
    "        axes[current_ax].set_ylim([depth_vals.max(), depth_vals.min()])\n",
    "        axes[current_ax].set_xlim([0, 1])\n",
    "        axes[current_ax].set_xticks([])\n",
    "        axes[current_ax].set_xlabel('Rock Type\\nColumn', fontweight='bold', fontsize='small')\n",
    "        current_ax += 1\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Plot CT image and data\n",
    "    # ---------------------------\n",
    "    if ct_img is not None and 'ct' in data:\n",
    "        # Plot CT image\n",
    "        axes[current_ax].imshow(ct_img, aspect='auto', extent=[0, 0.5, core_length, 0])\n",
    "        axes[current_ax].set_ylabel('Depth')\n",
    "        axes[current_ax].set_xticks([])\n",
    "        axes[current_ax].set_xlabel('Sediment\\nCore\\nCT Scan', fontweight='bold', fontsize='small')\n",
    "        current_ax += 1\n",
    "        \n",
    "        # Plot CT data\n",
    "        ct_col = column_configs['ct']['data_col']\n",
    "        ct_std = column_configs['ct']['std_col']\n",
    "        ct_depth = data['ct'][column_configs['ct']['depth_col']].astype(np.float64)\n",
    "        \n",
    "        axes[current_ax].plot(data['ct'][ct_col].astype(np.float64), ct_depth, \n",
    "                              color='black', linewidth=0.7)\n",
    "        \n",
    "        # Standard deviation fill\n",
    "        axes[current_ax].fill_betweenx(\n",
    "            ct_depth,\n",
    "            data['ct'][ct_col].astype(np.float64) - data['ct'][ct_std].astype(np.float64),\n",
    "            data['ct'][ct_col].astype(np.float64) + data['ct'][ct_std].astype(np.float64),\n",
    "            color='black', alpha=0.2, linewidth=0\n",
    "        )\n",
    "        \n",
    "        # Color-coded CT values using PolyCollection\n",
    "        ct_values = data['ct'][ct_col].astype(np.float64).values\n",
    "        depths = ct_depth.values\n",
    "        norm = plt.Normalize(300, 1600)\n",
    "        cmap = plt.cm.jet\n",
    "        \n",
    "        ct_polys = []\n",
    "        ct_facecolors = []\n",
    "        for i in range(len(depths) - 1):\n",
    "            # Ignore segments with NaN values\n",
    "            if not (np.isnan(ct_values[i]) or np.isnan(ct_values[i+1])):\n",
    "                poly = [\n",
    "                    (0, depths[i]),\n",
    "                    (ct_values[i], depths[i]),\n",
    "                    (ct_values[i+1], depths[i+1]),\n",
    "                    (0, depths[i+1])\n",
    "                ]\n",
    "                ct_polys.append(poly)\n",
    "                # Use the average value for smoother color transition\n",
    "                avg_val = (ct_values[i] + ct_values[i+1]) / 2\n",
    "                ct_facecolors.append(cmap(norm(avg_val)))\n",
    "                \n",
    "        if ct_polys:\n",
    "            import matplotlib.collections as mcoll\n",
    "            pc_ct = mcoll.PolyCollection(ct_polys, facecolors=ct_facecolors, edgecolors='none', alpha=0.95)\n",
    "            axes[current_ax].add_collection(pc_ct)\n",
    "        \n",
    "        axes[current_ax].set_xlabel('CT#\\nBrightness', fontweight='bold', fontsize='small')\n",
    "        axes[current_ax].grid(True)\n",
    "        axes[current_ax].set_xlim(300, None)\n",
    "        axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "        current_ax += 1\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Plot RGB image and data\n",
    "    # ---------------------------\n",
    "    if rgb_img is not None and 'rgb' in data:\n",
    "        # Plot RGB image\n",
    "        axes[current_ax].imshow(rgb_img, aspect='auto', extent=[0, 0.5, core_length, 0])\n",
    "        axes[current_ax].set_xticks([])\n",
    "        axes[current_ax].set_xlabel('Sediment\\nCore\\nPhoto', fontweight='bold', fontsize='small')\n",
    "        current_ax += 1\n",
    "        \n",
    "        # Plot RGB data (R, G, B channels)\n",
    "        rgb_cols = column_configs['rgb']['data_cols']\n",
    "        rgb_stds = column_configs['rgb']['std_cols']\n",
    "        rgb_depth = data['rgb'][column_configs['rgb']['depth_col']].astype(np.float64)\n",
    "        colors = ['red', 'green', 'blue']\n",
    "        \n",
    "        for col, std, color in zip(rgb_cols[:3], rgb_stds[:3], colors):\n",
    "            axes[current_ax].plot(data['rgb'][col].astype(np.float64), rgb_depth,\n",
    "                                  color=color, linewidth=0.7)\n",
    "            axes[current_ax].fill_betweenx(\n",
    "                rgb_depth,\n",
    "                data['rgb'][col].astype(np.float64) - data['rgb'][std].astype(np.float64),\n",
    "                data['rgb'][col].astype(np.float64) + data['rgb'][std].astype(np.float64),\n",
    "                color=color, alpha=0.2, linewidth=0\n",
    "            )\n",
    "        \n",
    "        # Luminance plot using PolyCollection with Inferno colormap\n",
    "        lumin_values = data['rgb']['Lumin'].astype(np.float64).values\n",
    "        lumin_depths = rgb_depth.values\n",
    "        \n",
    "        # Compute normalization range ignoring NaNs\n",
    "        valid_lumin = lumin_values[~np.isnan(lumin_values)]\n",
    "        if len(valid_lumin) == 0:\n",
    "            vmin, vmax = 0, 1\n",
    "        else:\n",
    "            vmin, vmax = valid_lumin.min(), valid_lumin.max()\n",
    "            if np.isclose(vmin, vmax):\n",
    "                vmin, vmax = 0, 1\n",
    "        \n",
    "        lumin_norm = plt.Normalize(vmin, vmax)\n",
    "        cmap_inferno = plt.cm.inferno\n",
    "        \n",
    "        lumin_polys = []\n",
    "        lumin_facecolors = []\n",
    "        for i in range(len(lumin_depths) - 1):\n",
    "            # Only use segments with valid (non-NaN) endpoints\n",
    "            if not (np.isnan(lumin_values[i]) or np.isnan(lumin_values[i+1])):\n",
    "                poly = [\n",
    "                    (0, lumin_depths[i]),\n",
    "                    (lumin_values[i], lumin_depths[i]),\n",
    "                    (lumin_values[i+1], lumin_depths[i+1]),\n",
    "                    (0, lumin_depths[i+1])\n",
    "                ]\n",
    "                lumin_polys.append(poly)\n",
    "                # Use average value for color mapping\n",
    "                avg_val = (lumin_values[i] + lumin_values[i+1]) / 2\n",
    "                lumin_facecolors.append(cmap_inferno(lumin_norm(avg_val)))\n",
    "        if lumin_polys:\n",
    "            import matplotlib.collections as mcoll\n",
    "            pc_lumin = mcoll.PolyCollection(lumin_polys, facecolors=lumin_facecolors, edgecolors='none', alpha=0.95)\n",
    "            axes[current_ax].add_collection(pc_lumin)\n",
    "        \n",
    "        axes[current_ax].set_xlabel('RGB\\nLuminance', fontweight='bold', fontsize='small')\n",
    "        axes[current_ax].grid(True)\n",
    "        axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "        current_ax += 1\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Plot MS and MST data if available\n",
    "    # ---------------------------\n",
    "    if 'mst' in data:\n",
    "        # Plot MS data if available\n",
    "        ms_col = column_configs['mst']['ms']['data_col']\n",
    "        has_mst_ms = 'mst' in data and ms_col in data['mst'].columns and not data['mst'][ms_col].isna().all()\n",
    "        has_hrms = 'hrms' in data and not data['hrms'][column_configs['hrms']['data_col']].isna().all()\n",
    "\n",
    "        if has_mst_ms or has_hrms:\n",
    "            if has_mst_ms:\n",
    "                axes[current_ax].plot(\n",
    "                    data['mst'][ms_col].astype(np.float64), \n",
    "                    data['mst'][column_configs['mst']['ms']['depth_col']].astype(np.float64),\n",
    "                    color='darkgray', label='Lo-res', linewidth=0.7\n",
    "                )\n",
    "            if has_hrms:\n",
    "                axes[current_ax].plot(\n",
    "                    data['hrms'][column_configs['hrms']['data_col']].astype(np.float64), \n",
    "                    data['hrms'][column_configs['hrms']['depth_col']].astype(np.float64),\n",
    "                    color='black', label='Hi-res', linewidth=0.7\n",
    "                )\n",
    "            axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "            axes[current_ax].set_xlabel('Magnetic\\nSusceptibility', fontweight='bold')\n",
    "            axes[current_ax].grid(True)\n",
    "            current_ax += 1\n",
    "\n",
    "        # Plot remaining MST logs that have data\n",
    "        mst_labels = {\n",
    "            'density': 'Density\\n(g/cc)',\n",
    "            'pwvel': 'P-wave\\nVelocity\\n(m/s)',\n",
    "            'pwamp': 'P-wave\\nAmplitude',\n",
    "            'elecres': 'Electrical\\nResistivity\\n(ohm-m)'\n",
    "        }\n",
    "        \n",
    "        mst_colors = {\n",
    "            'density': 'orange',\n",
    "            'pwvel': 'purple',\n",
    "            'pwamp': 'purple',\n",
    "            'elecres': 'brown'\n",
    "        }\n",
    "\n",
    "        for log_type, config in column_configs['mst'].items():\n",
    "            if log_type != 'ms' and config['data_col'] in data['mst'].columns and not data['mst'][config['data_col']].isna().all():\n",
    "                axes[current_ax].plot(\n",
    "                    data['mst'][config['data_col']].astype(np.float64), \n",
    "                    data['mst'][config['depth_col']].astype(np.float64), \n",
    "                    color=mst_colors.get(log_type, 'black'), \n",
    "                    linewidth=0.7\n",
    "                )\n",
    "                axes[current_ax].set_xlabel(mst_labels[log_type], fontweight='bold', fontsize='small')\n",
    "                axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "                axes[current_ax].grid(True)\n",
    "                if log_type == 'density':\n",
    "                    axes[current_ax].set_xlim(1, 2)\n",
    "                current_ax += 1\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Set common y-axis properties\n",
    "    # ---------------------------\n",
    "    for ax in axes:\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_ylim(core_length, 0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot processed/cleaned logs\n",
    "plt_title = f'{core_name} K-means Auto Rocktype Classification'\n",
    "\n",
    "# Create cluster data dictionary for the plot_core_logs function\n",
    "cluster_data = {\n",
    "    'depth_vals': df_for_clustering_sorted['depth'].values,\n",
    "    'labels_vals': df_for_clustering_sorted['cluster_label'].values,\n",
    "    'k': k\n",
    "}\n",
    "\n",
    "fig, axes = plot_core_logs(filled_data_paths, column_configs, core_length, ct_img, rgb_img, \n",
    "                          title=plt_title, cluster_data=cluster_data, figsize=(10, 10))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
