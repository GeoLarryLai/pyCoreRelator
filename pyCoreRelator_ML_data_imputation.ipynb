{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Log Data Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as mcoll  # For PolyCollection\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from scipy.signal import correlate, find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Create interaction features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions for Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for cleaning artifacts and noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_peak_valley_shift(signal_a, signal_b, common_depth):\n",
    "    \"\"\"\n",
    "    Computes a candidate shift based on matching the 2 largest peaks.\n",
    "    \n",
    "    For peaks:\n",
    "      - Finds all local maxima in each signal\n",
    "      - Sorts them by amplitude (largest first), selects the top 2 peaks\n",
    "      - Sorts these peaks by depth and computes the average depth difference\n",
    "    \n",
    "    Args:\n",
    "        signal_a (ndarray): First signal (e.g., HRMS segment)\n",
    "        signal_b (ndarray): Second signal (e.g., density, MS, or CT)\n",
    "        common_depth (ndarray): Depth grid onto which the signals are interpolated\n",
    "        \n",
    "    Returns:\n",
    "        float: The candidate shift (in depth units) based on peak matching\n",
    "    \"\"\"\n",
    "    # Find peaks in each signal\n",
    "    peaks_a, _ = find_peaks(signal_a)\n",
    "    peaks_b, _ = find_peaks(signal_b)\n",
    "    \n",
    "    # If no peaks found, use global maximum\n",
    "    if len(peaks_a) == 0:\n",
    "        peaks_a = np.array([np.argmax(signal_a)])\n",
    "    if len(peaks_b) == 0:\n",
    "        peaks_b = np.array([np.argmax(signal_b)])\n",
    "    \n",
    "    # Sort peaks by amplitude (largest first) and take top 2\n",
    "    sorted_peaks_a = sorted(peaks_a, key=lambda i: signal_a[i], reverse=True)[:2]\n",
    "    sorted_peaks_b = sorted(peaks_b, key=lambda i: signal_b[i], reverse=True)[:2]\n",
    "    \n",
    "    # Sort selected peaks by depth\n",
    "    top_peaks_a = sorted(sorted_peaks_a)\n",
    "    top_peaks_b = sorted(sorted_peaks_b)\n",
    "    \n",
    "    # Calculate peak shifts\n",
    "    n_peaks = min(len(top_peaks_a), len(top_peaks_b))\n",
    "    if n_peaks > 0:\n",
    "        peak_shifts = [common_depth[top_peaks_b[i]] - common_depth[top_peaks_a[i]] \n",
    "                      for i in range(n_peaks)]\n",
    "        candidate_shift = np.mean(peak_shifts)\n",
    "    else:\n",
    "        candidate_shift = 0.0\n",
    "        \n",
    "    return candidate_shift\n",
    "\n",
    "def compute_candidate_shift(signal_a, signal_b, common_depth, \n",
    "                            w_corr=0.2, w_peak=0.7):\n",
    "    \"\"\"\n",
    "    Compute a candidate depth shift between two signals based on:\n",
    "      - Cross-correlation, and\n",
    "      - Matching top 2 peaks\n",
    "    \n",
    "    The final candidate shift is a weighted combination of these two methods.\n",
    "    \n",
    "    Args:\n",
    "        signal_a (ndarray): First signal (e.g., HRMS segment).\n",
    "        signal_b (ndarray): Second signal (e.g., density, MS, or CT).\n",
    "        common_depth (ndarray): Depth grid onto which both signals are interpolated.\n",
    "        w_corr (float): Weight for the cross-correlation candidate.\n",
    "        w_peak (float): Weight for the peak/valley candidate.\n",
    "        \n",
    "    Returns:\n",
    "        float: The weighted candidate shift (in depth units).\n",
    "    \"\"\"\n",
    "    # --- Candidate 1: Cross-correlation shift ---\n",
    "    # Apply smoothing using a Gaussian filter\n",
    "    window = 3  # Window size for smoothing\n",
    "    a_smoothed = gaussian_filter1d(signal_a, sigma=window)\n",
    "    b_smoothed = gaussian_filter1d(signal_b, sigma=window)\n",
    "    \n",
    "    # Detrend the smoothed signals\n",
    "    a_detrended = a_smoothed - np.mean(a_smoothed)\n",
    "    b_detrended = b_smoothed - np.mean(b_smoothed)\n",
    "    \n",
    "    # Calculate cross-correlation\n",
    "    corr = correlate(a_detrended, b_detrended, mode='full')\n",
    "    lags = np.arange(-len(common_depth) + 1, len(common_depth))\n",
    "    best_lag = lags[np.argmax(corr)]\n",
    "    \n",
    "    # Handle case where common_depth is too short\n",
    "    try:\n",
    "        depth_step = common_depth[1] - common_depth[0]\n",
    "        cross_corr_shift = best_lag * depth_step\n",
    "    except IndexError:\n",
    "        cross_corr_shift = 0.0\n",
    "        w_corr = 0.0  # Zero out the weight for cross-correlation\n",
    "\n",
    "    # --- Candidate 2: Peak & Valley shift ---\n",
    "    candidate_peak_valley = compute_peak_valley_shift(signal_a, signal_b, common_depth)\n",
    "\n",
    "    # Return the weighted combination.\n",
    "    return w_corr * cross_corr_shift + w_peak * candidate_peak_valley\n",
    "\n",
    "def preprocess_core_data(data_config, shift_limit_multiplier=3.0):\n",
    "    \"\"\"\n",
    "    Preprocess core data by cleaning and scaling depth values.\n",
    "    \n",
    "    Args:\n",
    "        data_config (dict): Dictionary containing configuration parameters including:\n",
    "            - mother_dir (str): Source mother directory path\n",
    "            - core_name (str): Name of the core\n",
    "            - core_length (float): Length of the core in cm\n",
    "            - data_folder (str): Path to input data folder\n",
    "            - clean_output_folder (str): Path to output folder for cleaned data\n",
    "            - thresholds (dict): Dictionary with measurement keys and [condition, value, buffer] values where:\n",
    "                                - Measurement keys can be: 'ct', 'r', 'g', 'b', 'lumin', 'ms', 'pwvel', 'pwamp', 'den', 'elecres', 'hiresms'\n",
    "                                - condition must be one of: '>', '<', '<=', '>='\n",
    "                                - value is the threshold number to compare against\n",
    "                                - buffer is the number of surrounding points to also mark as invalid\n",
    "                                Example: {'ct': ['>', 2000, 5], 'ms': ['>', 150, 1], 'pwvel': ['>', 1085, 3]}\n",
    "    \"\"\"\n",
    "    # Validate threshold conditions\n",
    "    valid_conditions = ['>', '<', '<=', '>=']\n",
    "    for param, threshold in data_config['thresholds'].items():\n",
    "        if threshold[0] not in valid_conditions:\n",
    "            raise ValueError(f\"Invalid condition '{threshold[0]}' for {param}. Must be one of: {valid_conditions}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(data_config['mother_dir'] + data_config['clean_output_folder'], exist_ok=True)\n",
    "    \n",
    "    # Initialize data variables\n",
    "    ct_data = None\n",
    "    rgb_data = None \n",
    "    mst_data = None\n",
    "    hrms_data = None\n",
    "\n",
    "    # Try to read each data file if it exists\n",
    "    ct_path = data_config['mother_dir'] + data_config['data_folder'] + f\"{data_config['core_name']}_CT.csv\"\n",
    "    if os.path.exists(ct_path):\n",
    "        ct_data = pd.read_csv(ct_path).astype('float64')\n",
    "        \n",
    "        # Process CT data\n",
    "        if ct_data is not None and 'ct' in data_config['thresholds']:\n",
    "            condition, threshold_value, buffer_size = data_config['thresholds']['ct']\n",
    "            invalid_ct = eval(f\"ct_data['CT'] {condition} {threshold_value}\")\n",
    "            buffer_indices_ct = []\n",
    "            for i in range(len(ct_data)):\n",
    "                if invalid_ct[i]:\n",
    "                    buffer_indices_ct.extend(range(max(0, i-buffer_size), min(len(ct_data), i+buffer_size+1)))\n",
    "            ct_data.loc[buffer_indices_ct, ['CT', 'CT_std']] = np.nan\n",
    "        \n",
    "        if ct_data is not None:\n",
    "            ct_depth_scale = data_config['core_length'] / ct_data['SB_DEPTH_cm'].max()\n",
    "            ct_data['SB_DEPTH_cm'] = ct_data['SB_DEPTH_cm'] * ct_depth_scale\n",
    "            ct_data.to_csv(data_config['mother_dir'] + data_config['clean_output_folder'] + f\"{data_config['core_name']}_CT_clean.csv\", index=False)\n",
    "\n",
    "    rgb_path = data_config['mother_dir'] + data_config['data_folder'] + f\"{data_config['core_name']}_RGB.csv\"\n",
    "    if os.path.exists(rgb_path):\n",
    "        rgb_data = pd.read_csv(rgb_path).astype('float64')\n",
    "        \n",
    "        # Process RGB data\n",
    "        if rgb_data is not None:\n",
    "            rgb_columns = ['R', 'G', 'B', 'Lumin']\n",
    "            buffer_indices_rgb = []\n",
    "            \n",
    "            for col in rgb_columns:\n",
    "                if col.lower() in data_config['thresholds']:\n",
    "                    condition, threshold_value, buffer_size = data_config['thresholds'][col.lower()]\n",
    "                    invalid_values = eval(f\"rgb_data['{col}'] {condition} {threshold_value}\")\n",
    "                    for i in range(len(rgb_data)):\n",
    "                        if invalid_values[i]:\n",
    "                            buffer_indices_rgb.extend(range(max(0, i-buffer_size), min(len(rgb_data), i+buffer_size+1)))\n",
    "            \n",
    "            if buffer_indices_rgb:\n",
    "                rgb_data.loc[buffer_indices_rgb, rgb_columns + [f'{col}_std' for col in rgb_columns]] = np.nan\n",
    "            \n",
    "            rgb_depth_scale = data_config['core_length'] / rgb_data['SB_DEPTH_cm'].max()\n",
    "            rgb_data['SB_DEPTH_cm'] = rgb_data['SB_DEPTH_cm'] * rgb_depth_scale\n",
    "            rgb_data.to_csv(data_config['mother_dir'] + data_config['clean_output_folder'] + f\"{data_config['core_name']}_RGB_clean.csv\", index=False)\n",
    "\n",
    "    # Determine subfolder paths based on core name\n",
    "    if data_config['core_name'].startswith('M99'):\n",
    "        mst_subfolder = \"OSU orignal dataset/R-V_Melville99/Calibrated_MST/\"\n",
    "        hrms_subfolder = \"OSU orignal dataset/R-V_Melville99/M9907_point_mag/\"\n",
    "    elif data_config['core_name'].startswith('RR02'):\n",
    "        mst_subfolder = \"OSU orignal dataset/R-V_Revelle02/Calibrated_MST/\"\n",
    "        hrms_subfolder = \"OSU orignal dataset/R-V_Revelle02/RR0207_point_mag/\"\n",
    "    else:\n",
    "        # Default to Melville99 for other core names\n",
    "        mst_subfolder = \"OSU orignal dataset/R-V_Melville99/Calibrated_MST/\"\n",
    "        hrms_subfolder = \"OSU orignal dataset/R-V_Melville99/M9907_point_mag/\"\n",
    "\n",
    "    mst_path = data_config['mother_dir'] + mst_subfolder + f\"{data_config['core_name']}_MST.csv\"\n",
    "    if os.path.exists(mst_path):\n",
    "        mst_data = pd.read_csv(mst_path).astype('float64')\n",
    "        \n",
    "        # Process MST data if file exists\n",
    "        if mst_data is not None:\n",
    "            mst_columns = {\n",
    "                'MS': 'ms',\n",
    "                'PWVel_m/s': 'pwvel',\n",
    "                'PWAmp': 'pwamp',\n",
    "                'Den_gm/cc': 'den', \n",
    "                'ElecRes_ohmm': 'elecres'\n",
    "            }\n",
    "\n",
    "            density_extreme_indices = []\n",
    "            if 'Den_gm/cc' in mst_data.columns and 'den' in data_config['thresholds']:\n",
    "                condition, threshold_value, buffer_size = data_config['thresholds']['den']\n",
    "                density_extreme = eval(f\"mst_data['Den_gm/cc'] {condition} {threshold_value}\")\n",
    "                for i in range(len(mst_data)):\n",
    "                    if density_extreme[i]:\n",
    "                        density_extreme_indices.extend(range(max(0, i-buffer_size), min(len(mst_data), i+buffer_size+1)))\n",
    "\n",
    "            for column, param_key in mst_columns.items():\n",
    "                if column in mst_data.columns and param_key in data_config['thresholds']:\n",
    "                    condition, threshold_value, buffer_size = data_config['thresholds'][param_key]\n",
    "                    extreme_values = eval(f\"mst_data[column] {condition} {threshold_value}\")\n",
    "                    \n",
    "                    buffer_indices = []\n",
    "                    for i in range(len(mst_data)):\n",
    "                        if extreme_values[i]:\n",
    "                            buffer_indices.extend(range(max(0, i-buffer_size), min(len(mst_data), i+buffer_size+1)))\n",
    "                    \n",
    "                    if column == 'MS':\n",
    "                        buffer_indices.extend(density_extreme_indices)\n",
    "                    \n",
    "                    mst_data.loc[buffer_indices, column] = np.nan\n",
    "\n",
    "            if not mst_data.drop('SB_DEPTH_cm', axis=1).isna().all().all():\n",
    "                mst_depth_scale = data_config['core_length'] / mst_data['SB_DEPTH_cm'].max()\n",
    "                mst_data['SB_DEPTH_cm'] = mst_data['SB_DEPTH_cm'] * mst_depth_scale\n",
    "                mst_data.to_csv(data_config['mother_dir'] + data_config['clean_output_folder'] + f\"{data_config['core_name']}_MST_clean.csv\", index=False)\n",
    "\n",
    "    hrms_path = data_config['mother_dir'] + hrms_subfolder + f\"{data_config['core_name']}_ptMS.csv\"\n",
    "    if os.path.exists(hrms_path):\n",
    "        hrms_data = pd.read_csv(hrms_path).astype('float64')\n",
    "        \n",
    "        # Process Hi-res MS data only if both HRMS data and at least one reference curve exist\n",
    "        if hrms_data is not None and (ct_data is not None or (mst_data is not None and 'Den_gm/cc' in mst_data.columns)):\n",
    "            hrms_depth = hrms_data['SB_DEPTH_cm'].values\n",
    "            \n",
    "            # Resample density data if available\n",
    "            density_resampled = None\n",
    "            if mst_data is not None and 'Den_gm/cc' in mst_data.columns:\n",
    "                density_resampled = np.interp(hrms_depth, \n",
    "                                            mst_data['SB_DEPTH_cm'].values,\n",
    "                                            mst_data['Den_gm/cc'].values)\n",
    "            else:\n",
    "                density_resampled = np.full_like(hrms_depth, np.nan)\n",
    "                \n",
    "            # Resample CT data if available\n",
    "            ct_resampled = None\n",
    "            if ct_data is not None:\n",
    "                ct_resampled = np.interp(hrms_depth,\n",
    "                                       ct_data['SB_DEPTH_cm'].values,\n",
    "                                       ct_data['CT'].values)\n",
    "            else:\n",
    "                ct_resampled = np.full_like(hrms_depth, np.nan)\n",
    "\n",
    "            # Apply thresholds to HRMS data\n",
    "            if 'hiresms' in data_config['thresholds']:\n",
    "                condition, threshold_value, buffer_size = data_config['thresholds']['hiresms']\n",
    "                extreme_values = eval(f\"hrms_data['hiresMS'] {condition} {threshold_value}\")\n",
    "                buffer_indices = []\n",
    "                for i in range(len(hrms_data)):\n",
    "                    if extreme_values[i]:\n",
    "                        buffer_indices.extend(range(max(0, i - buffer_size), min(len(hrms_data), i + buffer_size + 1)))\n",
    "                hrms_data.loc[buffer_indices, 'hiresMS'] = np.nan\n",
    "\n",
    "            # Identify continuous segments in HRMS data\n",
    "            valid_indices = hrms_data.index[hrms_data['hiresMS'].notna()].tolist()\n",
    "            segments = []\n",
    "            if valid_indices:\n",
    "                current_segment = [valid_indices[0]]\n",
    "                for idx in valid_indices[1:]:\n",
    "                    if idx == current_segment[-1] + 1:\n",
    "                        current_segment.append(idx)\n",
    "                    else:\n",
    "                        segments.append(current_segment)\n",
    "                        current_segment = [idx]\n",
    "                segments.append(current_segment)\n",
    "\n",
    "            # Process each HRMS segment\n",
    "            for seg in segments:\n",
    "                seg_depth = hrms_data.loc[seg, 'SB_DEPTH_cm']\n",
    "                seg_values = hrms_data.loc[seg, 'hiresMS']\n",
    "                if seg_values.empty:\n",
    "                    continue\n",
    "\n",
    "                # Get resampled data for this segment\n",
    "                seg_density = density_resampled[seg] if density_resampled is not None else None\n",
    "                seg_ct = ct_resampled[seg] if ct_resampled is not None else None\n",
    "\n",
    "                # Check if at least one reference curve has valid data\n",
    "                if (seg_density is None or np.all(np.isnan(seg_density))) and (seg_ct is None or np.all(np.isnan(seg_ct))):\n",
    "                    print(f\"Warning: No valid reference data for segment at depth {seg_depth.iloc[0]:.2f}\")\n",
    "                    continue\n",
    "\n",
    "                # ---- Candidate Shift from Density Curve ----\n",
    "                candidate_shift_density = 0.0\n",
    "                corr_density = 0.0\n",
    "                if seg_density is not None and not np.all(np.isnan(seg_density)):\n",
    "                    candidate_shift_density = compute_candidate_shift(seg_values.values, \n",
    "                                                                   seg_density,\n",
    "                                                                   seg_depth.values)\n",
    "                    corr_density = np.abs(np.corrcoef(seg_values.values, seg_density)[0,1])\n",
    "                    if np.isnan(corr_density): corr_density = 0.0\n",
    "\n",
    "                # ---- Candidate Shift from CT Curve ----\n",
    "                candidate_shift_ct = 0.0\n",
    "                corr_ct = 0.0\n",
    "                if seg_ct is not None and not np.all(np.isnan(seg_ct)):\n",
    "                    candidate_shift_ct = compute_candidate_shift(seg_values.values,\n",
    "                                                              seg_ct, \n",
    "                                                              seg_depth.values)\n",
    "                    corr_ct = np.abs(np.corrcoef(seg_values.values, seg_ct)[0,1])\n",
    "                    if np.isnan(corr_ct): corr_ct = 0.0\n",
    "\n",
    "                # Determine the maximum allowed shift based on neighboring gaps\n",
    "                if seg[0] > 0:\n",
    "                    gap_before = hrms_data.at[seg[0], 'SB_DEPTH_cm'] - hrms_data.at[seg[0]-1, 'SB_DEPTH_cm']\n",
    "                else:\n",
    "                    gap_before = np.inf\n",
    "                if seg[-1] < len(hrms_data) - 1:\n",
    "                    gap_after = hrms_data.at[seg[-1]+1, 'SB_DEPTH_cm'] - hrms_data.at[seg[-1], 'SB_DEPTH_cm']\n",
    "                else:\n",
    "                    gap_after = np.inf\n",
    "\n",
    "                # Calculate gap-based shift limit\n",
    "                gap_based_shift = min(gap_before, gap_after) * shift_limit_multiplier  # Allow 300% of gap size\n",
    "\n",
    "                # ---- Consensus Shift ----\n",
    "                # Set correlation to 0 if candidate shift exceeds gap-based shift or is negative\n",
    "                if abs(candidate_shift_density) > gap_based_shift or corr_density < 0:\n",
    "                    corr_density = 0.0\n",
    "                if abs(candidate_shift_ct) > gap_based_shift or corr_ct < 0:\n",
    "                    corr_ct = 0.0\n",
    "\n",
    "                # Calculate weights based on correlations\n",
    "                total_corr = corr_density + corr_ct\n",
    "                if total_corr > 0:\n",
    "                    w_density = corr_density / total_corr\n",
    "                    w_ct = corr_ct / total_corr\n",
    "                else:\n",
    "                    w_density, w_ct = 0.4, 0.6\n",
    "\n",
    "                consensus_shift = (w_density * candidate_shift_density + \n",
    "                                 w_ct * candidate_shift_ct)\n",
    "\n",
    "                # Apply the consensus shift by moving hiresMS values to new positions\n",
    "                if abs(consensus_shift) <= gap_based_shift:\n",
    "                    # Calculate target indices based on depth shift\n",
    "                    target_indices = []\n",
    "                    for idx in seg:\n",
    "                        current_depth = hrms_data.at[idx, 'SB_DEPTH_cm']\n",
    "                        target_depth = current_depth + consensus_shift\n",
    "                        # Find closest depth position\n",
    "                        target_idx = (hrms_data['SB_DEPTH_cm'] - target_depth).abs().idxmin()\n",
    "                        target_indices.append(target_idx)\n",
    "                    \n",
    "                    # Store original values\n",
    "                    original_values = hrms_data.loc[seg, 'hiresMS'].copy()\n",
    "                    \n",
    "                    # Clear original positions\n",
    "                    hrms_data.loc[seg, 'hiresMS'] = np.nan\n",
    "                    \n",
    "                    # Move values to new positions\n",
    "                    for orig_val, target_idx in zip(original_values, target_indices):\n",
    "                        hrms_data.at[target_idx, 'hiresMS'] = orig_val\n",
    "                else:\n",
    "                    print(f\"Warning: Computed shift ({consensus_shift:.2f}) exceeds gap-based shift limit ({gap_based_shift * 100}%) \"\n",
    "                          f\"for segment at depths {seg_depth.iloc[0]:.2f}-{seg_depth.iloc[-1]:.2f}. \"\n",
    "                          f\"Rejected shifts - Density: {candidate_shift_density:.2f}, CT: {candidate_shift_ct:.2f}\")\n",
    "\n",
    "            # Rescale the depth after shifting\n",
    "            depth_scale_factor = data_config['core_length'] / hrms_data['SB_DEPTH_cm'].max()\n",
    "            hrms_data['SB_DEPTH_cm'] = hrms_data['SB_DEPTH_cm'] * depth_scale_factor\n",
    "            hrms_data.to_csv(data_config['mother_dir'] + data_config['clean_output_folder'] +\n",
    "                             f\"{data_config['core_name']}_hiresMS_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for plotting cleanned core images and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_core_logs(data_paths, column_configs, core_length, ct_img=None, rgb_img=None, title='Combined Logs'):\n",
    "    \"\"\"\n",
    "    Plot core logs from multiple data sources.\n",
    "    \n",
    "    Args:\n",
    "        data_paths (dict): Dictionary containing paths to data files\n",
    "            e.g. {a\n",
    "                'ct': 'path/to/ct.csv',\n",
    "                'rgb': 'path/to/rgb.csv', \n",
    "                'mst': 'path/to/mst.csv',\n",
    "                'hrms': 'path/to/hrms.csv'\n",
    "            }\n",
    "        column_configs (dict): Configuration for columns to plot\n",
    "            e.g. {\n",
    "                'ct': {'data_col': 'CT', 'std_col': 'CT_std', 'depth_col': 'SB_DEPTH_cm'},\n",
    "                'rgb': {'data_cols': ['R','G','B'], 'std_cols': ['R_std','G_std','B_std'], 'depth_col': 'SB_DEPTH_cm'},\n",
    "                'mst': {\n",
    "                    'density': {'data_col': 'Den_gm/cc', 'depth_col': 'SB_DEPTH_cm'},\n",
    "                    'pwvel': {'data_col': 'PWVel_m/s', 'depth_col': 'SB_DEPTH_cm'},\n",
    "                    'pwamp': {'data_col': 'PWAmp', 'depth_col': 'SB_DEPTH_cm'},\n",
    "                    'elecres': {'data_col': 'ElecRes_ohmm', 'depth_col': 'SB_DEPTH_cm'},\n",
    "                    'ms': {'data_col': 'MS', 'depth_col': 'SB_DEPTH_cm'}\n",
    "                },\n",
    "                'hrms': {'data_col': 'hiresMS', 'depth_col': 'SB_DEPTH_cm'}\n",
    "            }\n",
    "        core_length (float): Length of the core in cm\n",
    "        ct_img (array, optional): CT image array\n",
    "        rgb_img (array, optional): RGB image array\n",
    "        title (str): Title for the plot\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = {}\n",
    "    for key, path in data_paths.items():\n",
    "        if path:\n",
    "            data[key] = pd.read_csv(path)\n",
    "            if 'SB_DEPTH_cm' not in data[key].columns:\n",
    "                raise ValueError(f\"SB_DEPTH_cm column missing in {key} data\")\n",
    "    \n",
    "    # Calculate number of plots needed\n",
    "    n_plots = 0  # Start with 0\n",
    "    \n",
    "    # Add CT panels if CT image and data exist\n",
    "    if ct_img is not None and 'ct' in data:\n",
    "        n_plots += 2  # CT image and data\n",
    "        \n",
    "    # Add RGB panels if RGB image and data exist  \n",
    "    if rgb_img is not None and 'rgb' in data:\n",
    "        n_plots += 2  # RGB image and data\n",
    "        \n",
    "    # Add MS panel if either MST MS data or hiresMS data exists and has data\n",
    "    if ('mst' in data and column_configs['mst']['ms']['data_col'] in data['mst'].columns and not data['mst'][column_configs['mst']['ms']['data_col']].isna().all()) or ('hrms' in data and not data['hrms'][column_configs['hrms']['data_col']].isna().all()):\n",
    "        n_plots += 1\n",
    "        \n",
    "    # Add panels for other MST logs that exist and have data\n",
    "    if 'mst' in data:\n",
    "        for log_type, config in column_configs['mst'].items():\n",
    "            if log_type != 'ms':  # Skip MS since it's handled separately\n",
    "                if config['data_col'] in data['mst'].columns and not data['mst'][config['data_col']].isna().all():\n",
    "                    n_plots += 1\n",
    "\n",
    "    if n_plots == 0:\n",
    "        raise ValueError(\"No valid data to plot\")\n",
    "    \n",
    "    # Create figure and axes\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(10, 16), sharey=True)\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable\n",
    "    fig.suptitle(title, fontweight='bold')\n",
    "    \n",
    "    current_ax = 0\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Plot CT image and data\n",
    "    # ---------------------------\n",
    "    if ct_img is not None and 'ct' in data:\n",
    "        # Plot CT image\n",
    "        axes[current_ax].imshow(ct_img, aspect='auto', extent=[0, 0.5, core_length, 0])\n",
    "        axes[current_ax].set_ylabel('Depth (cm)')\n",
    "        axes[current_ax].set_xticks([])\n",
    "        axes[current_ax].set_xlabel('Sediment\\nCore\\nCT Scan', fontweight='bold', fontsize='small')\n",
    "        current_ax += 1\n",
    "        \n",
    "        # Plot CT data\n",
    "        ct_col = column_configs['ct']['data_col']\n",
    "        ct_std = column_configs['ct']['std_col']\n",
    "        ct_depth = data['ct'][column_configs['ct']['depth_col']].astype(np.float64)\n",
    "        \n",
    "        axes[current_ax].plot(data['ct'][ct_col].astype(np.float64), ct_depth, \n",
    "                              color='black', linewidth=0.7)\n",
    "        \n",
    "        # Standard deviation fill\n",
    "        axes[current_ax].fill_betweenx(\n",
    "            ct_depth,\n",
    "            data['ct'][ct_col].astype(np.float64) - data['ct'][ct_std].astype(np.float64),\n",
    "            data['ct'][ct_col].astype(np.float64) + data['ct'][ct_std].astype(np.float64),\n",
    "            color='black', alpha=0.2, linewidth=0\n",
    "        )\n",
    "        \n",
    "        # Color-coded CT values using PolyCollection\n",
    "        ct_values = data['ct'][ct_col].astype(np.float64).values\n",
    "        depths = ct_depth.values\n",
    "        norm = plt.Normalize(300, 1600)\n",
    "        cmap = plt.cm.jet\n",
    "        \n",
    "        ct_polys = []\n",
    "        ct_facecolors = []\n",
    "        for i in range(len(depths) - 1):\n",
    "            # Ignore segments with NaN values\n",
    "            if not (np.isnan(ct_values[i]) or np.isnan(ct_values[i+1])):\n",
    "                poly = [\n",
    "                    (0, depths[i]),\n",
    "                    (ct_values[i], depths[i]),\n",
    "                    (ct_values[i+1], depths[i+1]),\n",
    "                    (0, depths[i+1])\n",
    "                ]\n",
    "                ct_polys.append(poly)\n",
    "                # Use the average value for smoother color transition\n",
    "                avg_val = (ct_values[i] + ct_values[i+1]) / 2\n",
    "                ct_facecolors.append(cmap(norm(avg_val)))\n",
    "                \n",
    "        if ct_polys:\n",
    "            pc_ct = mcoll.PolyCollection(ct_polys, facecolors=ct_facecolors, edgecolors='none', alpha=0.95)\n",
    "            axes[current_ax].add_collection(pc_ct)\n",
    "        \n",
    "        axes[current_ax].set_xlabel('CT#\\nBrightness', fontweight='bold', fontsize='small')\n",
    "        axes[current_ax].grid(True)\n",
    "        axes[current_ax].set_xlim(300, None)\n",
    "        axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "        current_ax += 1\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Plot RGB image and data\n",
    "    # ---------------------------\n",
    "    if rgb_img is not None and 'rgb' in data:\n",
    "        # Plot RGB image\n",
    "        axes[current_ax].imshow(rgb_img, aspect='auto', extent=[0, 0.5, core_length, 0])\n",
    "        axes[current_ax].set_xticks([])\n",
    "        axes[current_ax].set_xlabel('Sediment\\nCore\\nPhoto', fontweight='bold', fontsize='small')\n",
    "        current_ax += 1\n",
    "        \n",
    "        # Plot RGB data (R, G, B channels)\n",
    "        rgb_cols = column_configs['rgb']['data_cols']\n",
    "        rgb_stds = column_configs['rgb']['std_cols']\n",
    "        rgb_depth = data['rgb'][column_configs['rgb']['depth_col']].astype(np.float64)\n",
    "        colors = ['red', 'green', 'blue']\n",
    "        \n",
    "        for col, std, color in zip(rgb_cols[:3], rgb_stds[:3], colors):\n",
    "            axes[current_ax].plot(data['rgb'][col].astype(np.float64), rgb_depth,\n",
    "                                  color=color, linewidth=0.7)\n",
    "            axes[current_ax].fill_betweenx(\n",
    "                rgb_depth,\n",
    "                data['rgb'][col].astype(np.float64) - data['rgb'][std].astype(np.float64),\n",
    "                data['rgb'][col].astype(np.float64) + data['rgb'][std].astype(np.float64),\n",
    "                color=color, alpha=0.2, linewidth=0\n",
    "            )\n",
    "        \n",
    "        # Luminance plot using PolyCollection with Inferno colormap\n",
    "        lumin_values = data['rgb']['Lumin'].astype(np.float64).values\n",
    "        lumin_depths = rgb_depth.values\n",
    "        \n",
    "        # Compute normalization range ignoring NaNs\n",
    "        valid_lumin = lumin_values[~np.isnan(lumin_values)]\n",
    "        if len(valid_lumin) == 0:\n",
    "            vmin, vmax = 0, 1\n",
    "        else:\n",
    "            vmin, vmax = valid_lumin.min(), valid_lumin.max()\n",
    "            if np.isclose(vmin, vmax):\n",
    "                vmin, vmax = 0, 1\n",
    "        \n",
    "        lumin_norm = plt.Normalize(vmin, vmax)\n",
    "        cmap_inferno = plt.cm.inferno\n",
    "        \n",
    "        lumin_polys = []\n",
    "        lumin_facecolors = []\n",
    "        for i in range(len(lumin_depths) - 1):\n",
    "            # Only use segments with valid (non-NaN) endpoints\n",
    "            if not (np.isnan(lumin_values[i]) or np.isnan(lumin_values[i+1])):\n",
    "                poly = [\n",
    "                    (0, lumin_depths[i]),\n",
    "                    (lumin_values[i], lumin_depths[i]),\n",
    "                    (lumin_values[i+1], lumin_depths[i+1]),\n",
    "                    (0, lumin_depths[i+1])\n",
    "                ]\n",
    "                lumin_polys.append(poly)\n",
    "                # Use average value for color mapping\n",
    "                avg_val = (lumin_values[i] + lumin_values[i+1]) / 2\n",
    "                lumin_facecolors.append(cmap_inferno(lumin_norm(avg_val)))\n",
    "        if lumin_polys:\n",
    "            pc_lumin = mcoll.PolyCollection(lumin_polys, facecolors=lumin_facecolors, edgecolors='none', alpha=0.95)\n",
    "            axes[current_ax].add_collection(pc_lumin)\n",
    "        \n",
    "        axes[current_ax].set_xlabel('RGB\\nLuminance', fontweight='bold', fontsize='small')\n",
    "        axes[current_ax].grid(True)\n",
    "        axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "        current_ax += 1\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Plot MS and MST data if available\n",
    "    # ---------------------------\n",
    "    if 'mst' in data:\n",
    "        # Plot MS data if available\n",
    "        ms_col = column_configs['mst']['ms']['data_col']\n",
    "        has_mst_ms = 'mst' in data and ms_col in data['mst'].columns and not data['mst'][ms_col].isna().all()\n",
    "        has_hrms = 'hrms' in data and not data['hrms'][column_configs['hrms']['data_col']].isna().all()\n",
    "\n",
    "        if has_mst_ms or has_hrms:\n",
    "            if has_mst_ms:\n",
    "                axes[current_ax].plot(\n",
    "                    data['mst'][ms_col].astype(np.float64), \n",
    "                    data['mst'][column_configs['mst']['ms']['depth_col']].astype(np.float64),\n",
    "                    color='darkgray', label='Lo-res', linewidth=0.7\n",
    "                )\n",
    "            if has_hrms:\n",
    "                axes[current_ax].plot(\n",
    "                    data['hrms'][column_configs['hrms']['data_col']].astype(np.float64), \n",
    "                    data['hrms'][column_configs['hrms']['depth_col']].astype(np.float64),\n",
    "                    color='black', label='Hi-res', linewidth=0.7\n",
    "                )\n",
    "            axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "            axes[current_ax].set_xlabel('Magnetic\\nSusceptibility', fontweight='bold')\n",
    "            axes[current_ax].grid(True)\n",
    "            current_ax += 1\n",
    "\n",
    "        # Plot remaining MST logs that have data\n",
    "        mst_labels = {\n",
    "            'density': 'Density\\n(g/cc)',\n",
    "            'pwvel': 'P-wave\\nVelocity\\n(m/s)',\n",
    "            'pwamp': 'P-wave\\nAmplitude',\n",
    "            'elecres': 'Electrical\\nResistivity\\n(ohm-m)'\n",
    "        }\n",
    "        \n",
    "        mst_colors = {\n",
    "            'density': 'orange',\n",
    "            'pwvel': 'purple',\n",
    "            'pwamp': 'purple',\n",
    "            'elecres': 'brown'\n",
    "        }\n",
    "\n",
    "        for log_type, config in column_configs['mst'].items():\n",
    "            if log_type != 'ms' and config['data_col'] in data['mst'].columns and not data['mst'][config['data_col']].isna().all():\n",
    "                axes[current_ax].plot(\n",
    "                    data['mst'][config['data_col']].astype(np.float64), \n",
    "                    data['mst'][config['depth_col']].astype(np.float64), \n",
    "                    color=mst_colors.get(log_type, 'black'), \n",
    "                    linewidth=0.7\n",
    "                )\n",
    "                axes[current_ax].set_xlabel(mst_labels[log_type], fontweight='bold', fontsize='small')\n",
    "                axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "                axes[current_ax].grid(True)\n",
    "                if log_type == 'density':\n",
    "                    axes[current_ax].set_xlim(1, 2)\n",
    "                current_ax += 1\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Set common y-axis properties\n",
    "    # ---------------------------\n",
    "    for ax in axes:\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_ylim(core_length, 0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions for Machine Learning to fill data gaps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for plotting filled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filled_data(target_log, original_data, filled_data, core_length, core_name, ML_type = 'ML'):\n",
    "    \"\"\"\n",
    "    Plot original and ML-filled data for a given log.\n",
    "    \n",
    "    Args:\n",
    "        target_log (str): Name of the log to plot\n",
    "        original_data (pd.DataFrame): Original data containing the log\n",
    "        filled_data (pd.DataFrame): Data with ML-filled gaps\n",
    "        core_length (int): Length of the core in cm\n",
    "        core_name (str): Name of the core for plot title\n",
    "    \"\"\"\n",
    "    # Check if there are any gaps\n",
    "    has_gaps = original_data[target_log].isna().any()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(15, 3))\n",
    "    title_suffix = f'Use {ML_type} for Data Gap Filling' if has_gaps else \"(No Data Gap to be filled by ML)\"\n",
    "    fig.suptitle(f'{core_name} {target_log} Values {title_suffix}', fontweight='bold')\n",
    "\n",
    "    # Plot data with ML-predicted gaps only if gaps exist\n",
    "    if has_gaps:\n",
    "        ax.plot(filled_data['SB_DEPTH_cm'], filled_data[target_log], \n",
    "                color='red', label=f'ML Predicted {target_log}', linewidth=0.7, alpha=0.7)\n",
    "\n",
    "    # Plot original data\n",
    "    ax.plot(original_data['SB_DEPTH_cm'], original_data[target_log], \n",
    "            color='black', label=f'Original {target_log}', linewidth=0.7)\n",
    "\n",
    "    # Add uncertainty shade if std column exists\n",
    "    std_col = f'{target_log}_std'\n",
    "    if std_col in original_data.columns:\n",
    "        ax.fill_between(original_data['SB_DEPTH_cm'],\n",
    "                       original_data[target_log] - original_data[std_col],\n",
    "                       original_data[target_log] + original_data[std_col],\n",
    "                       color='black', alpha=0.2, linewidth=0)\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_ylabel(f'{target_log}\\nBrightness', fontweight='bold', fontsize='small')\n",
    "    ax.set_xlabel('Depth (cm)')\n",
    "    ax.grid(True)\n",
    "    ax.invert_xaxis()\n",
    "    ax.set_xlim(0, core_length)\n",
    "    ax.tick_params(axis='y', labelsize='x-small')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for **Random Forest** Regressor to fill gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_gaps_with_rfml(target_log=None, \n",
    "#                           All_logs=None, \n",
    "#                           output_csv=False, \n",
    "#                           output_dir=None, \n",
    "#                           core_name=None, \n",
    "#                           merge_tolerance=3.0):\n",
    "#     \"\"\"\n",
    "#     Fill gaps in target data using ML ensemble predictions from multiple features.\n",
    "    \n",
    "#     Args:\n",
    "#         target_log (str): Name of the target column to fill gaps in.\n",
    "#         All_logs (dict): Dictionary of dataframes containing feature data and target data.\n",
    "#                          Format: {'df_name': (dataframe, [column_names])}\n",
    "#         output_csv (bool): Whether to output filled data to CSV file.\n",
    "#         output_dir (str): Directory to save output CSV file.\n",
    "#         core_name (str): Name of the core for CSV filename.\n",
    "#         merge_tolerance (float): Maximum allowed difference in depth (SB_DEPTH_cm) for merging\n",
    "#                                  rows from different logs. If None, no tolerance is applied.\n",
    "        \n",
    "#     Returns:\n",
    "#         tuple: (target_data_filled, gap_mask)\n",
    "#     \"\"\"\n",
    "#     # Input validation\n",
    "#     if target_log is None or All_logs is None:\n",
    "#         raise ValueError(\"Both target_log and All_logs must be provided\")\n",
    "        \n",
    "#     if output_csv and (output_dir is None or core_name is None):\n",
    "#         raise ValueError(\"output_dir and core_name must be provided when output_csv is True\")\n",
    "        \n",
    "#     # Get target data from All_logs\n",
    "#     target_data = None\n",
    "#     for df, cols in All_logs.values():\n",
    "#         if target_log in cols:\n",
    "#             target_data = df.copy()\n",
    "#             break\n",
    "    \n",
    "#     if target_data is None:\n",
    "#         raise ValueError(f\"Target log '{target_log}' not found in any dataset\")\n",
    "\n",
    "#     # Create a copy of the original data to hold the interpolated results\n",
    "#     target_data_filled = target_data.copy()\n",
    "\n",
    "#     # Identify gaps in target data\n",
    "#     gap_mask = target_data[target_log].isna()\n",
    "    \n",
    "#     # If no gaps exist, save to CSV if requested and return original data\n",
    "#     if not gap_mask.any():\n",
    "#         if output_csv:\n",
    "#             output_path = os.path.join(output_dir, f'{core_name}_{target_log.split(\"_\")[0]}_MLfilled.csv')\n",
    "#             target_data_filled.to_csv(output_path, index=False)\n",
    "#         return target_data_filled, gap_mask\n",
    "\n",
    "#     # Convert SB_DEPTH_cm to float64 in target data\n",
    "#     target_data['SB_DEPTH_cm'] = target_data['SB_DEPTH_cm'].astype('float64')\n",
    "\n",
    "#     # Prepare training data by merging all available logs\n",
    "#     merged_data = target_data[['SB_DEPTH_cm', target_log]].copy()\n",
    "#     features = []\n",
    "    \n",
    "#     # Merge feature dataframes one by one, using their own SB_DEPTH_cm column\n",
    "#     for df_name, (df, cols) in All_logs.items():\n",
    "#         if target_log not in cols:  # Skip the target dataframe\n",
    "#             df = df.copy()\n",
    "#             df['SB_DEPTH_cm'] = df['SB_DEPTH_cm'].astype('float64')\n",
    "#             # Rename SB_DEPTH_cm temporarily to avoid conflicts during merging\n",
    "#             df = df.rename(columns={'SB_DEPTH_cm': f'SB_DEPTH_cm_{df_name}'})\n",
    "#             # Convert all numeric columns to float64\n",
    "#             for col in cols:\n",
    "#                 if col != 'SB_DEPTH_cm' and df[col].dtype.kind in 'biufc':\n",
    "#                     df[col] = df[col].astype('float64')\n",
    "#             # Rename feature columns for merging\n",
    "#             df_renamed = df.rename(columns={col: f'{df_name}_{col}' for col in cols if col != 'SB_DEPTH_cm'})\n",
    "#             df_renamed = df_renamed.sort_values(f'SB_DEPTH_cm_{df_name}')\n",
    "            \n",
    "#             # Perform merge_asof with tolerance for data alignment\n",
    "#             merged_data = pd.merge_asof(\n",
    "#                 merged_data.sort_values('SB_DEPTH_cm'),\n",
    "#                 df_renamed,\n",
    "#                 left_on='SB_DEPTH_cm',\n",
    "#                 right_on=f'SB_DEPTH_cm_{df_name}',\n",
    "#                 direction='nearest',\n",
    "#                 tolerance=merge_tolerance\n",
    "#             )\n",
    "            \n",
    "#             # Check for unmatched rows due to the tolerance constraint\n",
    "#             unmatched = merged_data[f'SB_DEPTH_cm_{df_name}'].isna().sum()\n",
    "#             if unmatched > 0:\n",
    "#                 warnings.warn(f\"{unmatched} rows did not have a matching depth within tolerance for log '{df_name}'.\")\n",
    "            \n",
    "#             # Add renamed feature columns to features list\n",
    "#             features.extend([f'{df_name}_{col}' for col in cols if col != 'SB_DEPTH_cm'])\n",
    "#             # Drop the temporary depth column used for merging\n",
    "#             merged_data = merged_data.drop(columns=[f'SB_DEPTH_cm_{df_name}'])\n",
    "    \n",
    "#     # Add SB_DEPTH_cm as a feature\n",
    "#     features.append('SB_DEPTH_cm')\n",
    "\n",
    "#     # Prepare features and target for ML\n",
    "#     X = merged_data[features].copy()\n",
    "#     y = merged_data[target_log].copy()\n",
    "\n",
    "#     # Convert all features to float64\n",
    "#     for col in X.columns:\n",
    "#         if X[col].dtype.kind in 'biufc':\n",
    "#             X[col] = X[col].astype('float64')\n",
    "#     y = y.astype('float64')\n",
    "\n",
    "#     # Split into training (non-gap) and prediction (gap) sets\n",
    "#     X_train = X[~gap_mask]\n",
    "#     y_train = y[~gap_mask]\n",
    "#     X_pred = X[gap_mask]\n",
    "\n",
    "#     # Handle outliers using IQR method\n",
    "#     quantile_cutoff = 0.025 #fraction at both tail ends to be neglected in ML training\n",
    "#     Q1 = y_train.quantile(quantile_cutoff)\n",
    "#     Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "#     IQR = Q3 - Q1\n",
    "#     outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "#     X_train = X_train[outlier_mask]\n",
    "#     y_train = y_train[outlier_mask]\n",
    "\n",
    "#     def train_model(model):\n",
    "#         model.fit(X_train, y_train)\n",
    "#         return model.predict(X_pred)\n",
    "\n",
    "#     # Initialize two ensemble models\n",
    "#     models = [\n",
    "#         RandomForestRegressor(n_estimators=1000,\n",
    "#                               max_depth=30,\n",
    "#                               min_samples_split=5,\n",
    "#                               min_samples_leaf=5,\n",
    "#                               max_features='sqrt',\n",
    "#                               bootstrap=True,\n",
    "#                               random_state=42,\n",
    "#                               n_jobs=-1),\n",
    "#         HistGradientBoostingRegressor(max_iter=800,\n",
    "#                                       learning_rate=0.05,\n",
    "#                                       max_depth=5,\n",
    "#                                       min_samples_leaf=50,\n",
    "#                                       l2_regularization=1.0,\n",
    "#                                       random_state=42,\n",
    "#                                       verbose=0)\n",
    "#     ]\n",
    "\n",
    "#     # Train models in parallel\n",
    "#     predictions = Parallel(n_jobs=-1)(delayed(train_model)(model) for model in models)\n",
    "\n",
    "#     # Ensemble predictions by averaging\n",
    "#     ensemble_predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "#     # Fill gaps with ensemble predictions\n",
    "#     target_data_filled.loc[gap_mask, target_log] = ensemble_predictions\n",
    "    \n",
    "#     # Save to CSV if requested\n",
    "#     if output_csv:\n",
    "#         output_path = os.path.join(output_dir, f\"{core_name}_{target_log.split('_')[0]}_MLfilled.csv\")\n",
    "#         target_data_filled.to_csv(output_path, index=False)\n",
    "\n",
    "#     return target_data_filled, gap_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for **Random Forest** Regressor + **Trend Constraints** in the ML Pipeline to fill gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_gaps_with_rftcml(target_log=None, \n",
    "#                           All_logs=None, \n",
    "#                           output_csv=False, \n",
    "#                           output_dir=None, \n",
    "#                           core_name=None, \n",
    "#                           merge_tolerance=3.0):\n",
    "#     \"\"\"\n",
    "#     Fill gaps in the target log using an ensemble of ML models and then adjust \n",
    "#     predictions to follow the trend between the edges of each gap.\n",
    "    \n",
    "#     This function:\n",
    "#       (1) Trains models on the available data from the target log together with \n",
    "#           features from other logs.\n",
    "#       (2) Predicts the missing values (gaps) in the target log.\n",
    "#       (3) For each contiguous gap segment that has valid data at both boundaries, \n",
    "#           blends the ML prediction with a linear interpolation between the boundary values.\n",
    "    \n",
    "#     Args:\n",
    "#         target_log (str): Name of the target column to fill gaps in.\n",
    "#         All_logs (dict): Dictionary of dataframes containing feature and target data.\n",
    "#                          Format: {'df_name': (dataframe, [column_names])}\n",
    "#         output_csv (bool): Whether to output the filled data to a CSV file.\n",
    "#         output_dir (str): Directory in which to save the CSV file.\n",
    "#         core_name (str): Core name used in naming the output file.\n",
    "#         merge_tolerance (float): Maximum allowed difference in depth (SB_DEPTH_cm) \n",
    "#                                  for merging rows from different logs.\n",
    "        \n",
    "#     Returns:\n",
    "#         tuple: (target_data_filled, gap_mask)\n",
    "#                target_data_filled: DataFrame with the target_log gap-filled.\n",
    "#                gap_mask: Boolean Series indicating where the original gaps were.\n",
    "#     \"\"\"\n",
    "#     # Input validation\n",
    "#     if target_log is None or All_logs is None:\n",
    "#         raise ValueError(\"Both target_log and All_logs must be provided\")\n",
    "        \n",
    "#     if output_csv and (output_dir is None or core_name is None):\n",
    "#         raise ValueError(\"output_dir and core_name must be provided when output_csv is True\")\n",
    "        \n",
    "#     # Get target data from All_logs\n",
    "#     target_data = None\n",
    "#     for df, cols in All_logs.values():\n",
    "#         if target_log in cols:\n",
    "#             target_data = df.copy()\n",
    "#             break\n",
    "#     if target_data is None:\n",
    "#         raise ValueError(f\"Target log '{target_log}' not found in any dataset\")\n",
    "    \n",
    "#     # Create a copy to hold the filled results\n",
    "#     target_data_filled = target_data.copy()\n",
    "    \n",
    "#     # Identify gaps in the target log\n",
    "#     gap_mask_orig = target_data[target_log].isna()\n",
    "#     if not gap_mask_orig.any():\n",
    "#         if output_csv:\n",
    "#             output_path = os.path.join(output_dir, f'{core_name}_{target_log.split(\"_\")[0]}_MLfilled.csv')\n",
    "#             target_data_filled.to_csv(output_path, index=False)\n",
    "#         return target_data_filled, gap_mask_orig\n",
    "\n",
    "#     # Ensure the depth column is float64\n",
    "#     target_data['SB_DEPTH_cm'] = target_data['SB_DEPTH_cm'].astype('float64')\n",
    "    \n",
    "#     # Begin merged_data with the target log (retaining SB_DEPTH_cm)\n",
    "#     merged_data = target_data[['SB_DEPTH_cm', target_log]].copy()\n",
    "#     features = []\n",
    "    \n",
    "#     # Merge additional logs (features) one by one using merge_asof on depth\n",
    "#     for df_name, (df, cols) in All_logs.items():\n",
    "#         if target_log in cols:  # Skip the target log itself\n",
    "#             continue\n",
    "#         df = df.copy()\n",
    "#         df['SB_DEPTH_cm'] = df['SB_DEPTH_cm'].astype('float64')\n",
    "#         # Rename SB_DEPTH_cm to avoid conflicts\n",
    "#         df = df.rename(columns={'SB_DEPTH_cm': f'SB_DEPTH_cm_{df_name}'})\n",
    "#         # Convert numeric columns to float64\n",
    "#         for col in cols:\n",
    "#             if col != 'SB_DEPTH_cm' and df[col].dtype.kind in 'biufc':\n",
    "#                 df[col] = df[col].astype('float64')\n",
    "#         # Rename feature columns for merging\n",
    "#         df_renamed = df.rename(columns={col: f'{df_name}_{col}' for col in cols if col != 'SB_DEPTH_cm'})\n",
    "#         df_renamed = df_renamed.sort_values(f'SB_DEPTH_cm_{df_name}')\n",
    "        \n",
    "#         # Merge using asof with specified tolerance\n",
    "#         merged_data = pd.merge_asof(\n",
    "#             merged_data.sort_values('SB_DEPTH_cm'),\n",
    "#             df_renamed,\n",
    "#             left_on='SB_DEPTH_cm',\n",
    "#             right_on=f'SB_DEPTH_cm_{df_name}',\n",
    "#             direction='nearest',\n",
    "#             tolerance=merge_tolerance\n",
    "#         )\n",
    "        \n",
    "#         unmatched = merged_data[f'SB_DEPTH_cm_{df_name}'].isna().sum()\n",
    "#         if unmatched > 0:\n",
    "#             warnings.warn(f\"{unmatched} rows did not have a matching depth within tolerance for log '{df_name}'.\")\n",
    "        \n",
    "#         features.extend([f'{df_name}_{col}' for col in cols if col != 'SB_DEPTH_cm'])\n",
    "#         merged_data = merged_data.drop(columns=[f'SB_DEPTH_cm_{df_name}'])\n",
    "    \n",
    "#     # Add depth as a feature\n",
    "#     features.append('SB_DEPTH_cm')\n",
    "    \n",
    "#     # It is best to work with merged_data sorted by depth\n",
    "#     merged_data = merged_data.sort_values('SB_DEPTH_cm').reset_index(drop=True)\n",
    "    \n",
    "#     # For consistency, recompute gap mask on merged_data\n",
    "#     gap_mask = merged_data[target_log].isna()\n",
    "    \n",
    "#     # Prepare feature matrix X and target vector y\n",
    "#     X = merged_data[features].copy()\n",
    "#     y = merged_data[target_log].copy()\n",
    "    \n",
    "#     # Ensure all numeric features are float64\n",
    "#     for col in X.columns:\n",
    "#         if X[col].dtype.kind in 'biufc':\n",
    "#             X[col] = X[col].astype('float64')\n",
    "#     y = y.astype('float64')\n",
    "    \n",
    "#     # Split into training (non-gap) and prediction (gap) sets\n",
    "#     X_train = X[~gap_mask]\n",
    "#     y_train = y[~gap_mask]\n",
    "#     X_pred = X[gap_mask]\n",
    "    \n",
    "#     # Handle outliers in training data using the IQR method\n",
    "#     quantile_cutoff = 0.15 #fraction at both tail ends to be neglected in ML training\n",
    "#     Q1 = y_train.quantile(quantile_cutoff)\n",
    "#     Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "#     IQR = Q3 - Q1\n",
    "#     outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "#     X_train = X_train[outlier_mask]\n",
    "#     y_train = y_train[outlier_mask]\n",
    "    \n",
    "#     def train_model(model):\n",
    "#         model.fit(X_train, y_train)\n",
    "#         return model.predict(X_pred)\n",
    "    \n",
    "#     # Initialize two ensemble models\n",
    "#     models = [\n",
    "#         RandomForestRegressor(n_estimators=1000,\n",
    "#                               max_depth=30,\n",
    "#                               min_samples_split=5,\n",
    "#                               min_samples_leaf=5,\n",
    "#                               max_features='sqrt',\n",
    "#                               bootstrap=True,\n",
    "#                               random_state=42,\n",
    "#                               n_jobs=-1),\n",
    "#         HistGradientBoostingRegressor(max_iter=800,\n",
    "#                                       learning_rate=0.05,\n",
    "#                                       max_depth=5,\n",
    "#                                       min_samples_leaf=50,\n",
    "#                                       l2_regularization=1.0,\n",
    "#                                       random_state=42,\n",
    "#                                       verbose=0)\n",
    "#     ]\n",
    "    \n",
    "#     # Train models in parallel and average their predictions\n",
    "#     predictions = Parallel(n_jobs=-1)(delayed(train_model)(model) for model in models)\n",
    "#     ensemble_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "#     # --- Incorporate Trend Constraints ---\n",
    "#     # Adjust the ensemble predictions so that, in each contiguous gap segment\n",
    "#     # with valid left and right boundaries, the final prediction is a blend of:\n",
    "#     #   (a) a linear interpolation between the observed boundary values, and\n",
    "#     #   (b) the ML prediction.\n",
    "#     # The blending weight is 0 at the boundaries and increases toward the center.\n",
    "#     def adjust_gap_predictions(df, gap_mask, ml_preds, target_log):\n",
    "#         \"\"\"\n",
    "#         Adjust ML predictions for gap rows in 'df' so that for each contiguous gap\n",
    "#         segment (with both left and right boundaries available) the predictions are\n",
    "#         blended with the linear interpolation between the boundary values.\n",
    "        \n",
    "#         Args:\n",
    "#             df (DataFrame): The merged DataFrame sorted by 'SB_DEPTH_cm'.\n",
    "#             gap_mask (Series of bool): Boolean mask for missing target values.\n",
    "#             ml_preds (np.array): ML predictions corresponding to rows where gap_mask is True.\n",
    "#             target_log (str): The target column name.\n",
    "            \n",
    "#         Returns:\n",
    "#             np.array: Adjusted predictions (in the same order as ml_preds).\n",
    "#         \"\"\"\n",
    "#         # Get the integer positions (row numbers) of missing values\n",
    "#         gap_positions = np.where(gap_mask.values)[0]\n",
    "#         # Create a Series for easier handling; index = positions in df\n",
    "#         preds_series = pd.Series(ml_preds, index=gap_positions)\n",
    "        \n",
    "#         # Identify contiguous segments in the gap positions\n",
    "#         segments = np.split(gap_positions, np.where(np.diff(gap_positions) != 1)[0] + 1)\n",
    "        \n",
    "#         adjusted = preds_series.copy()\n",
    "#         for seg in segments:\n",
    "#             # seg is an array of row positions (in df) for a contiguous gap segment.\n",
    "#             start_pos = seg[0]\n",
    "#             end_pos = seg[-1]\n",
    "            \n",
    "#             # Enforce trend constraints only if both boundaries exist.\n",
    "#             if start_pos == 0 or end_pos == len(df) - 1:\n",
    "#                 continue  # Skip segments at the very beginning or end.\n",
    "            \n",
    "#             # Retrieve boundary (observed) values and depths\n",
    "#             left_value = df.iloc[start_pos - 1][target_log]\n",
    "#             right_value = df.iloc[end_pos + 1][target_log]\n",
    "#             # Skip if boundaries are missing (should not happen if gap_mask is correct)\n",
    "#             if pd.isna(left_value) or pd.isna(right_value):\n",
    "#                 continue\n",
    "#             left_depth = df.iloc[start_pos - 1]['SB_DEPTH_cm']\n",
    "#             right_depth = df.iloc[end_pos + 1]['SB_DEPTH_cm']\n",
    "            \n",
    "#             # For each gap row in the segment, blend the ML prediction with linear interpolation\n",
    "#             for pos in seg:\n",
    "#                 current_depth = df.iloc[pos]['SB_DEPTH_cm']\n",
    "#                 # Normalize the depth position (x in [0, 1])\n",
    "#                 if right_depth == left_depth:\n",
    "#                     x = 0.5\n",
    "#                 else:\n",
    "#                     x = (current_depth - left_depth) / (right_depth - left_depth)\n",
    "#                 # Compute the linear interpolation value at this depth\n",
    "#                 interp_val = left_value + (right_value - left_value) * x\n",
    "#                 # Define a weight that is 0 at the boundaries and 1 at the middle.\n",
    "#                 # Here we use: weight = 1 - 2*|x - 0.5|\n",
    "#                 weight = 1 - 2 * abs(x - 0.5)\n",
    "#                 weight = max(0, min(weight, 1))  # Ensure weight is between 0 and 1\n",
    "#                 # Blend: final = interpolation + weight*(ML_prediction - interpolation)\n",
    "#                 adjusted[pos] = interp_val + weight * (preds_series.loc[pos] - interp_val)\n",
    "#         return adjusted.values\n",
    "\n",
    "#     # Adjust the ensemble predictions using trend constraints.\n",
    "#     adjusted_predictions = adjust_gap_predictions(merged_data, gap_mask, ensemble_predictions, target_log)\n",
    "    \n",
    "#     # Fill the gaps in the original (filled) target data.\n",
    "#     # Since merged_data was built from target_data, we use its index (which corresponds to sorted order).\n",
    "#     # To maintain the original index order, we update target_data_filled at the rows corresponding to missing values.\n",
    "#     # First, create a mapping from the sorted merged_data index to the original index.\n",
    "#     # Here we assume that the target_data_filled index is the same as the index of merged_data before sorting.\n",
    "#     # If not, additional reindexing may be needed.\n",
    "#     # For simplicity, we assume the order is now sorted by depth.\n",
    "#     target_data_filled.loc[merged_data.index[gap_mask], target_log] = adjusted_predictions\n",
    "    \n",
    "#     # Optionally, output to CSV\n",
    "#     if output_csv:\n",
    "#         output_path = os.path.join(output_dir, f\"{core_name}_{target_log.split('_')[0]}_MLfilled.csv\")\n",
    "#         target_data_filled.to_csv(output_path, index=False)\n",
    "    \n",
    "#     return target_data_filled, gap_mask_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for **XGBoost** ML with customized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_gaps_with_xgboost(target_log=None, All_logs=None, output_csv=False, output_dir=None, core_name=None, merge_tolerance=3.0):\n",
    "#     \"\"\"\n",
    "#     Fill gaps in target data using XGBoost predictions from multiple features.\n",
    "    \n",
    "#     Args:\n",
    "#         target_log (str): Name of the target column to fill gaps in\n",
    "#         All_logs (dict): Dictionary of dataframes containing feature data and target data\n",
    "#             Format: {'df_name': (dataframe, [column_names])}\n",
    "#         output_csv (bool): Whether to output filled data to CSV file\n",
    "#         output_dir (str): Directory to save output CSV file\n",
    "#         core_name (str): Name of the core for CSV filename\n",
    "#         merge_tolerance (float): Maximum allowed difference in depth (SB_DEPTH_cm) for merging\n",
    "#                                rows from different logs. If None, no tolerance is applied.\n",
    "        \n",
    "#     Returns:\n",
    "#         tuple: (target_data_filled, gap_mask)\n",
    "#     \"\"\"\n",
    "#     # Input validation\n",
    "#     if target_log is None or All_logs is None:\n",
    "#         raise ValueError(\"Both target_log and All_logs must be provided\")\n",
    "        \n",
    "#     if output_csv and (output_dir is None or core_name is None):\n",
    "#         raise ValueError(\"output_dir and core_name must be provided when output_csv is True\")\n",
    "        \n",
    "#     # Get target data from All_logs\n",
    "#     target_data = None\n",
    "#     for df, cols in All_logs.values():\n",
    "#         if target_log in cols:\n",
    "#             target_data = df.copy()\n",
    "#             break\n",
    "    \n",
    "#     if target_data is None:\n",
    "#         raise ValueError(f\"Target log '{target_log}' not found in any dataset\")\n",
    "\n",
    "#     # Create a copy of the original data to hold the interpolated results\n",
    "#     target_data_filled = target_data.copy()\n",
    "\n",
    "#     # Convert target column to float32\n",
    "#     target_data_filled[target_log] = target_data_filled[target_log].astype('float32')\n",
    "\n",
    "#     # Identify gaps in target data\n",
    "#     gap_mask = target_data[target_log].isna()\n",
    "    \n",
    "#     # If no gaps exist, save to CSV if requested and return original data\n",
    "#     if not gap_mask.any():\n",
    "#         if output_csv:\n",
    "#             output_path = os.path.join(output_dir, f\"{core_name}_{target_log.split('_')[0]}_MLfilled.csv\")\n",
    "#             target_data_filled.to_csv(output_path, index=False)\n",
    "#         return target_data_filled, gap_mask\n",
    "\n",
    "#     # Define feature groups and weights\n",
    "#     feature_weights = {\n",
    "#         'RGB': {\n",
    "#             'R': 0.1,\n",
    "#             'G': 0.1,\n",
    "#             'B': 0.1,\n",
    "#             'Lumin': 0.32\n",
    "#         },\n",
    "#         'MS': {\n",
    "#             'hiresMS': 3.0,\n",
    "#             'MS': 0.5\n",
    "#         },\n",
    "#         'Physical': {\n",
    "#             'PWVel_m/s': 0.05,\n",
    "#             'PWAmp': 0.05,\n",
    "#             'Den_gm/cc': 3.0\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     # Prepare training data by merging all available logs\n",
    "#     merged_data = target_data[['SB_DEPTH_cm', target_log]].copy()\n",
    "#     merged_data['SB_DEPTH_cm'] = merged_data['SB_DEPTH_cm'].astype('float32')\n",
    "#     features = []\n",
    "    \n",
    "#     # Merge feature dataframes one by one\n",
    "#     for df_name, (df, cols) in All_logs.items():\n",
    "#         if target_log not in cols:  # Skip the target dataframe\n",
    "#             df = df.copy()\n",
    "#             df['SB_DEPTH_cm'] = df['SB_DEPTH_cm'].astype('float32')\n",
    "#             # Rename SB_DEPTH_cm temporarily to avoid conflicts during merging\n",
    "#             df = df.rename(columns={'SB_DEPTH_cm': f'SB_DEPTH_cm_{df_name}'})\n",
    "#             # Convert feature columns to float32\n",
    "#             for col in cols:\n",
    "#                 if col != 'SB_DEPTH_cm' and df[col].dtype.kind in 'biufc':\n",
    "#                     df[col] = df[col].astype('float32')\n",
    "#             # Rename feature columns for merging\n",
    "#             df_renamed = df.rename(columns={col: f'{df_name}_{col}' for col in cols if col != 'SB_DEPTH_cm'})\n",
    "#             df_renamed = df_renamed.sort_values(f'SB_DEPTH_cm_{df_name}')\n",
    "            \n",
    "#             # Merge using asof with tolerance\n",
    "#             merged_data = pd.merge_asof(\n",
    "#                 merged_data.sort_values('SB_DEPTH_cm'),\n",
    "#                 df_renamed,\n",
    "#                 left_on='SB_DEPTH_cm',\n",
    "#                 right_on=f'SB_DEPTH_cm_{df_name}',\n",
    "#                 direction='nearest',\n",
    "#                 tolerance=merge_tolerance\n",
    "#             )\n",
    "            \n",
    "#             # Check for unmatched rows due to tolerance constraint\n",
    "#             unmatched = merged_data[f'SB_DEPTH_cm_{df_name}'].isna().sum()\n",
    "#             if unmatched > 0:\n",
    "#                 warnings.warn(f\"{unmatched} rows did not have a matching depth within tolerance for log '{df_name}'.\")\n",
    "            \n",
    "#             # Add renamed feature columns to features list\n",
    "#             features.extend([f'{df_name}_{col}' for col in cols if col != 'SB_DEPTH_cm'])\n",
    "#             # Drop temporary depth column\n",
    "#             merged_data = merged_data.drop(columns=[f'SB_DEPTH_cm_{df_name}'])\n",
    "\n",
    "#     # Add SB_DEPTH_cm as a feature\n",
    "#     features.append('SB_DEPTH_cm')\n",
    "\n",
    "#     # Prepare features and target\n",
    "#     X = merged_data[features].copy()\n",
    "#     y = merged_data[target_log].copy()\n",
    "\n",
    "#     # Apply weights to features\n",
    "#     X_weighted = X.copy()\n",
    "#     for group, weights in feature_weights.items():\n",
    "#         for feature, weight in weights.items():\n",
    "#             if feature in X_weighted.columns:\n",
    "#                 X_weighted[feature] = (X_weighted[feature] * weight).astype('float32')\n",
    "\n",
    "#     # Split into training and prediction sets\n",
    "#     X_train = X_weighted[~gap_mask]\n",
    "#     y_train = y[~gap_mask]\n",
    "#     X_pred = X_weighted[gap_mask]\n",
    "\n",
    "#     # Handle outliers using IQR method\n",
    "#     quantile_cutoff = 0.025 #fraction at both tail ends to be neglected in ML training\n",
    "#     Q1 = y_train.quantile(quantile_cutoff)\n",
    "#     Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "#     IQR = Q3 - Q1\n",
    "#     outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "#     X_train = X_train[outlier_mask]\n",
    "#     y_train = y_train[outlier_mask]\n",
    "\n",
    "#     # Create feature pipeline\n",
    "#     feature_pipeline = Pipeline([\n",
    "#         ('imputer', SimpleImputer(strategy='median')),\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=True)),\n",
    "#         ('selector', SelectKBest(score_func=f_regression, k='all'))\n",
    "#     ])\n",
    "\n",
    "#     # Process features\n",
    "#     X_train_processed = feature_pipeline.fit_transform(X_train, y_train)\n",
    "#     X_pred_processed = feature_pipeline.transform(X_pred)\n",
    "\n",
    "#     # Convert processed arrays to float32\n",
    "#     X_train_processed = X_train_processed.astype('float32')\n",
    "#     X_pred_processed = X_pred_processed.astype('float32')\n",
    "#     y_train = y_train.astype('float32')\n",
    "\n",
    "#     # Initialize and train XGBoost model with optimized hyperparameters for better accuracy\n",
    "#     model = xgb.XGBRegressor(\n",
    "#         n_estimators=5000,           # More trees for better learning\n",
    "#         learning_rate=0.003,         # Slower learning rate to prevent overfitting\n",
    "#         max_depth=10,                # Reduced to prevent overfitting\n",
    "#         min_child_weight=5,          # Increased to make model more conservative\n",
    "#         subsample=0.75,              # sample ratio\n",
    "#         colsample_bytree=0.75,       # feature sampling\n",
    "#         gamma=0.2,                   # Added minimum loss reduction\n",
    "#         reg_alpha=0.3,               # L1 regularization to reduce overfitting\n",
    "#         reg_lambda=3.0,              # Increased L2 regularization\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1,\n",
    "#     )\n",
    "    \n",
    "#     model.fit(X_train_processed, y_train)\n",
    "\n",
    "#     # Make predictions\n",
    "#     predictions = model.predict(X_pred_processed).astype('float32')\n",
    "\n",
    "#     # Fill gaps with predictions\n",
    "#     target_data_filled.loc[gap_mask, target_log] = predictions\n",
    "\n",
    "#     # Save to CSV if requested\n",
    "#     if output_csv:\n",
    "#         output_path = os.path.join(output_dir, f\"{core_name}_{target_log.split('_')[0]}_MLfilled.csv\")\n",
    "#         # Create output directory if it doesn't exist\n",
    "#         os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "#         target_data_filled.to_csv(output_path, index=False)\n",
    "\n",
    "#     return target_data_filled, gap_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for **XGBoost** + **LightGBM** ML with customized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_gaps_with_xgblgbm(target_log=None, All_logs=None, output_csv=False, output_dir=None, core_name=None, merge_tolerance=3.0):\n",
    "#     \"\"\"\n",
    "#     Fill gaps in target data using XGBoost + LightGBM predictions from multiple features.\n",
    "    \n",
    "#     Args:\n",
    "#         target_log (str): Name of the target column to fill gaps in\n",
    "#         All_logs (dict): Dictionary of dataframes containing feature data and target data\n",
    "#             Format: {'df_name': (dataframe, [column_names])}\n",
    "#         output_csv (bool): Whether to output filled data to CSV file\n",
    "#         output_dir (str): Directory to save output CSV file\n",
    "#         core_name (str): Name of the core for CSV filename\n",
    "#         merge_tolerance (float): Maximum allowed difference in depth (SB_DEPTH_cm) for merging\n",
    "#                                rows from different logs. If None, no tolerance is applied.\n",
    "        \n",
    "#     Returns:\n",
    "#         tuple: (target_data_filled, gap_mask)\n",
    "#     \"\"\"\n",
    "#     # Inp\n",
    "#     # Input validation\n",
    "#     if target_log is None or All_logs is None:\n",
    "#         raise ValueError(\"Both target_log and All_logs must be provided\")\n",
    "        \n",
    "#     if output_csv and (output_dir is None or core_name is None):\n",
    "#         raise ValueError(\"output_dir and core_name must be provided when output_csv is True\")\n",
    "    \n",
    "#     # Import required additional libraries\n",
    "#     import lightgbm as lgb\n",
    "#     import numpy as np\n",
    "    \n",
    "#     # Get target data from All_logs\n",
    "#     target_data = None\n",
    "#     for df, cols in All_logs.values():\n",
    "#         if target_log in cols:\n",
    "#             target_data = df.copy()\n",
    "#             break\n",
    "    \n",
    "#     if target_data is None:\n",
    "#         raise ValueError(f\"Target log '{target_log}' not found in any dataset\")\n",
    "\n",
    "#     # Create a copy of the original data to hold the interpolated results\n",
    "#     target_data_filled = target_data.copy()\n",
    "#     target_data_filled[target_log] = target_data_filled[target_log].astype('float32')\n",
    "#     gap_mask = target_data[target_log].isna()\n",
    "    \n",
    "#     if not gap_mask.any():\n",
    "#         if output_csv:\n",
    "#             output_path = os.path.join(output_dir, f\"{core_name}_{target_log.split('_')[0]}_MLfilled.csv\")\n",
    "#             target_data_filled.to_csv(output_path, index=False)\n",
    "#         return target_data_filled, gap_mask\n",
    "\n",
    "#     # [Previous feature_weights dictionary remains the same]\n",
    "#     feature_weights = {\n",
    "#         'RGB': {\n",
    "#             'R': 0.3,\n",
    "#             'G': 0.3,\n",
    "#             'B': 0.3,\n",
    "#             'Lumin': 0.3\n",
    "#         },\n",
    "#         'MS': {\n",
    "#             'hiresMS': 3.0,\n",
    "#             'MS': 0.05\n",
    "#         },\n",
    "#         'Physical': {\n",
    "#             'PWVel_m/s': 0.01,\n",
    "#             'PWAmp': 0.01,\n",
    "#             'Den_gm/cc': 0.5\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     # Prepare training data by merging all available logs\n",
    "#     merged_data = target_data[['SB_DEPTH_cm', target_log]].copy()\n",
    "#     merged_data['SB_DEPTH_cm'] = merged_data['SB_DEPTH_cm'].astype('float32')\n",
    "#     features = []\n",
    "    \n",
    "#     # [Previous merging code remains the same]\n",
    "#     for df_name, (df, cols) in All_logs.items():\n",
    "#         if target_log not in cols:\n",
    "#             df = df.copy()\n",
    "#             df['SB_DEPTH_cm'] = df['SB_DEPTH_cm'].astype('float32')\n",
    "#             df = df.rename(columns={'SB_DEPTH_cm': f'SB_DEPTH_cm_{df_name}'})\n",
    "#             for col in cols:\n",
    "#                 if col != 'SB_DEPTH_cm' and df[col].dtype.kind in 'biufc':\n",
    "#                     df[col] = df[col].astype('float32')\n",
    "#             df_renamed = df.rename(columns={col: f'{df_name}_{col}' for col in cols if col != 'SB_DEPTH_cm'})\n",
    "#             df_renamed = df_renamed.sort_values(f'SB_DEPTH_cm_{df_name}')\n",
    "            \n",
    "#             merged_data = pd.merge_asof(\n",
    "#                 merged_data.sort_values('SB_DEPTH_cm'),\n",
    "#                 df_renamed,\n",
    "#                 left_on='SB_DEPTH_cm',\n",
    "#                 right_on=f'SB_DEPTH_cm_{df_name}',\n",
    "#                 direction='nearest',\n",
    "#                 tolerance=merge_tolerance\n",
    "#             )\n",
    "            \n",
    "#             unmatched = merged_data[f'SB_DEPTH_cm_{df_name}'].isna().sum()\n",
    "#             if unmatched > 0:\n",
    "#                 warnings.warn(f\"{unmatched} rows did not have a matching depth within tolerance for log '{df_name}'.\")\n",
    "            \n",
    "#             features.extend([f'{df_name}_{col}' for col in cols if col != 'SB_DEPTH_cm'])\n",
    "#             merged_data = merged_data.drop(columns=[f'SB_DEPTH_cm_{df_name}'])\n",
    "\n",
    "#     # Add SB_DEPTH_cm as a feature\n",
    "#     features.append('SB_DEPTH_cm')\n",
    "\n",
    "#     # Add cyclical features based on depth\n",
    "#     def add_cyclical_features(df, col, period):\n",
    "#         df[f'{col}_sin'] = np.sin(2 * np.pi * df[col] / period)\n",
    "#         df[f'{col}_cos'] = np.cos(2 * np.pi * df[col] / period)\n",
    "#         return df\n",
    "\n",
    "#     # Add cyclical features for different periods (adjust periods based on your data)\n",
    "#     periods = [10, 50, 100]  # Example periods in cm\n",
    "#     for period in periods:\n",
    "#         merged_data = add_cyclical_features(merged_data, 'SB_DEPTH_cm', period)\n",
    "#         features.extend([f'SB_DEPTH_cm_sin', f'SB_DEPTH_cm_cos'])\n",
    "\n",
    "#     # Prepare features and target\n",
    "#     X = merged_data[features].copy()\n",
    "#     y = merged_data[target_log].copy()\n",
    "\n",
    "#     # Apply weights to features\n",
    "#     X_weighted = X.copy()\n",
    "#     for group, weights in feature_weights.items():\n",
    "#         for feature, weight in weights.items():\n",
    "#             matching_cols = [col for col in X_weighted.columns if feature in col]\n",
    "#             for col in matching_cols:\n",
    "#                 X_weighted[col] = (X_weighted[col] * weight).astype('float32')\n",
    "\n",
    "#     # Split into training and prediction sets\n",
    "#     X_train = X_weighted[~gap_mask]\n",
    "#     y_train = y[~gap_mask]\n",
    "#     X_pred = X_weighted[gap_mask]\n",
    "\n",
    "#     # Create feature pipeline\n",
    "#     feature_pipeline = Pipeline([\n",
    "#         ('imputer', SimpleImputer(strategy='median')),\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=True)),\n",
    "#         ('selector', SelectKBest(score_func=f_regression, k=min(50, X_train.shape[0]//10)))  # Limit number of features\n",
    "#     ])\n",
    "\n",
    "\n",
    "#     # Process features\n",
    "#     X_train_processed = feature_pipeline.fit_transform(X_train, y_train)\n",
    "#     X_pred_processed = feature_pipeline.transform(X_pred)\n",
    "\n",
    "#     # Convert processed arrays to float32\n",
    "#     X_train_processed = X_train_processed.astype('float32')\n",
    "#     X_pred_processed = X_pred_processed.astype('float32')\n",
    "#     y_train = y_train.astype('float32')\n",
    "\n",
    "#     # Initialize models\n",
    "#     xgb_model = xgb.XGBRegressor(\n",
    "#         n_estimators=3000,           # More trees for better learning\n",
    "#         learning_rate=0.003,         # Slower learning rate to prevent overfitting\n",
    "#         max_depth=10,                # Reduced to prevent overfitting\n",
    "#         min_child_weight=5,          # Increased to make model more conservative\n",
    "#         subsample=0.75,              # sample ratio\n",
    "#         colsample_bytree=0.75,       # feature sampling\n",
    "#         gamma=0.2,                   # Added minimum loss reduction\n",
    "#         reg_alpha=0.3,               # L1 regularization to reduce overfitting\n",
    "#         reg_lambda=3.0,              # Increased L2 regularization\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1,\n",
    "#     )\n",
    "\n",
    "#     lgb_model = lgb.LGBMRegressor(\n",
    "#         n_estimators=3000,           # Increased for more stable predictions\n",
    "#         learning_rate=0.003,         # Reduced for more stable learning\n",
    "#         max_depth=6,                 # Further reduced depth to prevent overfitting\n",
    "#         num_leaves=20,               # Reduced leaves for simpler trees\n",
    "#         min_child_samples=50,        # Increased to require more samples per leaf\n",
    "#         subsample=0.9,               # Increased sample ratio for stability\n",
    "#         colsample_bytree=0.9,        # Increased feature sampling for stability\n",
    "#         reg_alpha=0.3,               # Increased L1 regularization\n",
    "#         reg_lambda=3.0,              # Increased L2 regularization\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1,\n",
    "#         force_col_wise=True,         # Remove threading overhead\n",
    "#         verbose=-1                   # Reduce verbosity\n",
    "#     )\n",
    "\n",
    "#     # Train both models\n",
    "#     xgb_model.fit(X_train_processed, y_train)\n",
    "#     lgb_model.fit(X_train_processed, y_train)\n",
    "\n",
    "#     # Make predictions with both models\n",
    "#     xgb_predictions = xgb_model.predict(X_pred_processed).astype('float32')\n",
    "#     lgb_predictions = lgb_model.predict(X_pred_processed).astype('float32')\n",
    "\n",
    "#     # Ensemble predictions (simple average)\n",
    "#     predictions = (xgb_predictions + lgb_predictions) / 2\n",
    "\n",
    "#     # Fill gaps with predictions\n",
    "#     target_data_filled.loc[gap_mask, target_log] = predictions\n",
    "\n",
    "#     # Save to CSV if requested\n",
    "#     if output_csv:\n",
    "#         output_path = os.path.join(output_dir, f\"{core_name}_{target_log.split('_')[0]}_MLfilled.csv\")\n",
    "#         target_data_filled.to_csv(output_path, index=False)\n",
    "\n",
    "#     return target_data_filled, gap_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Machine Learning Data Gap filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for fill_gaps_with_ml\n",
    "\n",
    "def prepare_feature_data(target_log, All_logs, merge_tolerance):\n",
    "    \"\"\"Prepare merged feature data for ML training.\"\"\"\n",
    "    # Get target data from All_logs\n",
    "    target_data = None\n",
    "    for df, cols in All_logs.values():\n",
    "        if target_log in cols:\n",
    "            target_data = df.copy()\n",
    "            break\n",
    "    \n",
    "    if target_data is None:\n",
    "        raise ValueError(f\"Target log '{target_log}' not found in any dataset\")\n",
    "\n",
    "    # Convert SB_DEPTH_cm to float64 in target data\n",
    "    target_data['SB_DEPTH_cm'] = target_data['SB_DEPTH_cm'].astype('float64')\n",
    "\n",
    "    # Prepare training data by merging all available logs\n",
    "    merged_data = target_data[['SB_DEPTH_cm', target_log]].copy()\n",
    "    features = []\n",
    "    \n",
    "    # Merge feature dataframes one by one, using their own SB_DEPTH_cm column\n",
    "    for df_name, (df, cols) in All_logs.items():\n",
    "        if target_log not in cols:  # Skip the target dataframe\n",
    "            df = df.copy()\n",
    "            df['SB_DEPTH_cm'] = df['SB_DEPTH_cm'].astype('float64')\n",
    "            # Rename SB_DEPTH_cm temporarily to avoid conflicts during merging\n",
    "            df = df.rename(columns={'SB_DEPTH_cm': f'SB_DEPTH_cm_{df_name}'})\n",
    "            # Convert all numeric columns to float64\n",
    "            for col in cols:\n",
    "                if col != 'SB_DEPTH_cm' and df[col].dtype.kind in 'biufc':\n",
    "                    df[col] = df[col].astype('float64')\n",
    "            # Rename feature columns for merging\n",
    "            df_renamed = df.rename(columns={col: f'{df_name}_{col}' for col in cols if col != 'SB_DEPTH_cm'})\n",
    "            df_renamed = df_renamed.sort_values(f'SB_DEPTH_cm_{df_name}')\n",
    "            \n",
    "            # Perform merge_asof with tolerance for data alignment\n",
    "            merged_data = pd.merge_asof(\n",
    "                merged_data.sort_values('SB_DEPTH_cm'),\n",
    "                df_renamed,\n",
    "                left_on='SB_DEPTH_cm',\n",
    "                right_on=f'SB_DEPTH_cm_{df_name}',\n",
    "                direction='nearest',\n",
    "                tolerance=merge_tolerance\n",
    "            )\n",
    "            \n",
    "            # Check for unmatched rows due to the tolerance constraint\n",
    "            unmatched = merged_data[f'SB_DEPTH_cm_{df_name}'].isna().sum()\n",
    "            if unmatched > 0:\n",
    "                warnings.warn(f\"{unmatched} rows did not have a matching depth within tolerance for log '{df_name}'.\")\n",
    "            \n",
    "            # Add renamed feature columns to features list\n",
    "            features.extend([f'{df_name}_{col}' for col in cols if col != 'SB_DEPTH_cm'])\n",
    "            # Drop the temporary depth column used for merging\n",
    "            merged_data = merged_data.drop(columns=[f'SB_DEPTH_cm_{df_name}'])\n",
    "    \n",
    "    # Add SB_DEPTH_cm as a feature\n",
    "    features.append('SB_DEPTH_cm')\n",
    "    \n",
    "    return target_data, merged_data, features\n",
    "\n",
    "\n",
    "def apply_feature_weights(X, method):\n",
    "    \"\"\"Apply feature weights for XGBoost methods.\"\"\"\n",
    "    if method == 'xgb':\n",
    "        feature_weights = {\n",
    "            'RGB': {\n",
    "                'R': 0.1,\n",
    "                'G': 0.1,\n",
    "                'B': 0.1,\n",
    "                'Lumin': 0.32\n",
    "            },\n",
    "            'MS': {\n",
    "                'hiresMS': 3.0,\n",
    "                'MS': 0.5\n",
    "            },\n",
    "            'Physical': {\n",
    "                'PWVel_m/s': 0.05,\n",
    "                'PWAmp': 0.05,\n",
    "                'Den_gm/cc': 3.0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        X_weighted = X.copy()\n",
    "        for group, weights in feature_weights.items():\n",
    "            for feature, weight in weights.items():\n",
    "                if feature in X_weighted.columns:\n",
    "                    X_weighted[feature] = (X_weighted[feature] * weight).astype('float32')\n",
    "        return X_weighted\n",
    "    \n",
    "    elif method == 'xgblgbm':\n",
    "        feature_weights = {\n",
    "            'RGB': {\n",
    "                'R': 0.3,\n",
    "                'G': 0.3,\n",
    "                'B': 0.3,\n",
    "                'Lumin': 0.3\n",
    "            },\n",
    "            'MS': {\n",
    "                'hiresMS': 3.0,\n",
    "                'MS': 0.05\n",
    "            },\n",
    "            'Physical': {\n",
    "                'PWVel_m/s': 0.01,\n",
    "                'PWAmp': 0.01,\n",
    "                'Den_gm/cc': 0.5\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        X_weighted = X.copy()\n",
    "        for group, weights in feature_weights.items():\n",
    "            for feature, weight in weights.items():\n",
    "                matching_cols = [col for col in X_weighted.columns if feature in col]\n",
    "                for col in matching_cols:\n",
    "                    X_weighted[col] = (X_weighted[col] * weight).astype('float32')\n",
    "        return X_weighted\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def adjust_gap_predictions(df, gap_mask, ml_preds, target_log):\n",
    "    \"\"\"\n",
    "    Adjust ML predictions for gap rows in 'df' so that for each contiguous gap\n",
    "    segment (with both left and right boundaries available) the predictions are\n",
    "    blended with the linear interpolation between the boundary values.\n",
    "    \"\"\"\n",
    "    # Get the integer positions (row numbers) of missing values\n",
    "    gap_positions = np.where(gap_mask.values)[0]\n",
    "    # Create a Series for easier handling; index = positions in df\n",
    "    preds_series = pd.Series(ml_preds, index=gap_positions)\n",
    "    \n",
    "    # Identify contiguous segments in the gap positions\n",
    "    segments = np.split(gap_positions, np.where(np.diff(gap_positions) != 1)[0] + 1)\n",
    "    \n",
    "    adjusted = preds_series.copy()\n",
    "    for seg in segments:\n",
    "        # seg is an array of row positions (in df) for a contiguous gap segment.\n",
    "        start_pos = seg[0]\n",
    "        end_pos = seg[-1]\n",
    "        \n",
    "        # Enforce trend constraints only if both boundaries exist.\n",
    "        if start_pos == 0 or end_pos == len(df) - 1:\n",
    "            continue  # Skip segments at the very beginning or end.\n",
    "        \n",
    "        # Retrieve boundary (observed) values and depths\n",
    "        left_value = df.iloc[start_pos - 1][target_log]\n",
    "        right_value = df.iloc[end_pos + 1][target_log]\n",
    "        # Skip if boundaries are missing (should not happen if gap_mask is correct)\n",
    "        if pd.isna(left_value) or pd.isna(right_value):\n",
    "            continue\n",
    "        left_depth = df.iloc[start_pos - 1]['SB_DEPTH_cm']\n",
    "        right_depth = df.iloc[end_pos + 1]['SB_DEPTH_cm']\n",
    "        \n",
    "        # For each gap row in the segment, blend the ML prediction with linear interpolation\n",
    "        for pos in seg:\n",
    "            current_depth = df.iloc[pos]['SB_DEPTH_cm']\n",
    "            # Normalize the depth position (x in [0, 1])\n",
    "            if right_depth == left_depth:\n",
    "                x = 0.5\n",
    "            else:\n",
    "                x = (current_depth - left_depth) / (right_depth - left_depth)\n",
    "            # Compute the linear interpolation value at this depth\n",
    "            interp_val = left_value + (right_value - left_value) * x\n",
    "            # Define a weight that is 0 at the boundaries and 1 at the middle.\n",
    "            # Here we use: weight = 1 - 2*|x - 0.5|\n",
    "            weight = 1 - 2 * abs(x - 0.5)\n",
    "            weight = max(0, min(weight, 1))  # Ensure weight is between 0 and 1\n",
    "            # Blend: final = interpolation + weight*(ML_prediction - interpolation)\n",
    "            adjusted[pos] = interp_val + weight * (preds_series.loc[pos] - interp_val)\n",
    "    return adjusted.values\n",
    "\n",
    "\n",
    "def train_model(model):\n",
    "    \"\"\"Helper function for parallel model training.\"\"\"\n",
    "    def train_wrapper(X_train, y_train, X_pred):\n",
    "        model.fit(X_train, y_train)\n",
    "        return model.predict(X_pred)\n",
    "    return train_wrapper\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Consolidated Function\n",
    "def fill_gaps_with_ml(target_log=None, \n",
    "                      All_logs=None, \n",
    "                      output_csv=False, \n",
    "                      output_dir=None, \n",
    "                      core_name=None, \n",
    "                      merge_tolerance=3.0,\n",
    "                      ml_method='xgblgbm'):\n",
    "    \"\"\"\n",
    "    Fill gaps in target data using specified ML method.\n",
    "    \n",
    "    Args:\n",
    "        target_log (str): Name of the target column to fill gaps in.\n",
    "        All_logs (dict): Dictionary of dataframes containing feature data and target data.\n",
    "                         Format: {'df_name': (dataframe, [column_names])}\n",
    "        output_csv (bool): Whether to output filled data to CSV file.\n",
    "        output_dir (str): Directory to save output CSV file.\n",
    "        core_name (str): Name of the core for CSV filename.\n",
    "        merge_tolerance (float): Maximum allowed difference in depth (SB_DEPTH_cm) for merging\n",
    "                                 rows from different logs.\n",
    "        ml_method (str): ML method to use - 'rf', 'rftc', 'xgb', 'xgblgbm' (default)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (target_data_filled, gap_mask)\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if target_log is None or All_logs is None:\n",
    "        raise ValueError(\"Both target_log and All_logs must be provided\")\n",
    "        \n",
    "    if output_csv and (output_dir is None or core_name is None):\n",
    "        raise ValueError(\"output_dir and core_name must be provided when output_csv is True\")\n",
    "    \n",
    "    if ml_method not in ['rf', 'rftc', 'xgb', 'xgblgbm']:\n",
    "        raise ValueError(\"ml_method must be one of: 'rf', 'rftc', 'xgb', 'xgblgbm'\")\n",
    "    \n",
    "    # Prepare feature data\n",
    "    target_data, merged_data, features = prepare_feature_data(target_log, All_logs, merge_tolerance)\n",
    "    \n",
    "    # Create a copy of the original data to hold the interpolated results\n",
    "    target_data_filled = target_data.copy()\n",
    "\n",
    "    # Identify gaps in target data\n",
    "    gap_mask = target_data[target_log].isna()\n",
    "    \n",
    "    # If no gaps exist, save to CSV if requested and return original data\n",
    "    if not gap_mask.any():\n",
    "        if output_csv:\n",
    "            output_path = os.path.join(output_dir, f'{core_name}_{target_log.split(\"_\")[0]}_MLfilled.csv')\n",
    "            target_data_filled.to_csv(output_path, index=False)\n",
    "        return target_data_filled, gap_mask\n",
    "\n",
    "    if ml_method == 'rf':\n",
    "        # Convert all features to float64\n",
    "        for col in merged_data[features].columns:\n",
    "            if merged_data[features][col].dtype.kind in 'biufc':\n",
    "                merged_data[features][col] = merged_data[features][col].astype('float64')\n",
    "        merged_data[target_log] = merged_data[target_log].astype('float64')\n",
    "\n",
    "        # Prepare features and target for ML\n",
    "        X = merged_data[features].copy()\n",
    "        y = merged_data[target_log].copy()\n",
    "\n",
    "        # Split into training (non-gap) and prediction (gap) sets\n",
    "        X_train = X[~gap_mask]\n",
    "        y_train = y[~gap_mask]\n",
    "        X_pred = X[gap_mask]\n",
    "\n",
    "        # Handle outliers using IQR method\n",
    "        quantile_cutoff = 0.025 #fraction at both tail ends to be neglected in ML training\n",
    "        Q1 = y_train.quantile(quantile_cutoff)\n",
    "        Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "        X_train = X_train[outlier_mask]\n",
    "        y_train = y_train[outlier_mask]\n",
    "\n",
    "        # Initialize two ensemble models\n",
    "        models = [\n",
    "            RandomForestRegressor(n_estimators=1000,\n",
    "                                  max_depth=30,\n",
    "                                  min_samples_split=5,\n",
    "                                  min_samples_leaf=5,\n",
    "                                  max_features='sqrt',\n",
    "                                  bootstrap=True,\n",
    "                                  random_state=42,\n",
    "                                  n_jobs=-1),\n",
    "            HistGradientBoostingRegressor(max_iter=800,\n",
    "                                          learning_rate=0.05,\n",
    "                                          max_depth=5,\n",
    "                                          min_samples_leaf=50,\n",
    "                                          l2_regularization=1.0,\n",
    "                                          random_state=42,\n",
    "                                          verbose=0)\n",
    "        ]\n",
    "\n",
    "        # Train models in parallel\n",
    "        predictions = Parallel(n_jobs=-1)(delayed(train_model)(model) for model in models)\n",
    "\n",
    "        # Ensemble predictions by averaging\n",
    "        ensemble_predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "        # Fill gaps with ensemble predictions\n",
    "        target_data_filled.loc[gap_mask, target_log] = ensemble_predictions\n",
    "        \n",
    "    elif ml_method == 'rftc':\n",
    "        # It is best to work with merged_data sorted by depth\n",
    "        merged_data = merged_data.sort_values('SB_DEPTH_cm').reset_index(drop=True)\n",
    "        \n",
    "        # For consistency, recompute gap mask on merged_data\n",
    "        gap_mask = merged_data[target_log].isna()\n",
    "        \n",
    "        # Prepare feature matrix X and target vector y\n",
    "        X = merged_data[features].copy()\n",
    "        y = merged_data[target_log].copy()\n",
    "        \n",
    "        # Ensure all numeric features are float64\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype.kind in 'biufc':\n",
    "                X[col] = X[col].astype('float64')\n",
    "        y = y.astype('float64')\n",
    "        \n",
    "        # Split into training (non-gap) and prediction (gap) sets\n",
    "        X_train = X[~gap_mask]\n",
    "        y_train = y[~gap_mask]\n",
    "        X_pred = X[gap_mask]\n",
    "        \n",
    "        # Handle outliers in training data using the IQR method\n",
    "        quantile_cutoff = 0.15 #fraction at both tail ends to be neglected in ML training\n",
    "        Q1 = y_train.quantile(quantile_cutoff)\n",
    "        Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "        X_train = X_train[outlier_mask]\n",
    "        y_train = y_train[outlier_mask]\n",
    "        \n",
    "        # Initialize two ensemble models\n",
    "        models = [\n",
    "            RandomForestRegressor(n_estimators=1000,\n",
    "                                  max_depth=30,\n",
    "                                  min_samples_split=5,\n",
    "                                  min_samples_leaf=5,\n",
    "                                  max_features='sqrt',\n",
    "                                  bootstrap=True,\n",
    "                                  random_state=42,\n",
    "                                  n_jobs=-1),\n",
    "            HistGradientBoostingRegressor(max_iter=800,\n",
    "                                          learning_rate=0.05,\n",
    "                                          max_depth=5,\n",
    "                                          min_samples_leaf=50,\n",
    "                                          l2_regularization=1.0,\n",
    "                                          random_state=42,\n",
    "                                          verbose=0)\n",
    "        ]\n",
    "        \n",
    "        # Train models in parallel and average their predictions\n",
    "        predictions = Parallel(n_jobs=-1)(delayed(train_model)(model) for model in models)\n",
    "        ensemble_predictions = np.mean(predictions, axis=0)\n",
    "        \n",
    "        # Adjust the ensemble predictions using trend constraints.\n",
    "        adjusted_predictions = adjust_gap_predictions(merged_data, gap_mask, ensemble_predictions, target_log)\n",
    "        \n",
    "        # Fill the gaps in the original (filled) target data.\n",
    "        target_data_filled.loc[merged_data.index[gap_mask], target_log] = adjusted_predictions\n",
    "\n",
    "    elif ml_method == 'xgb':\n",
    "        # Convert target column to float32\n",
    "        target_data_filled[target_log] = target_data_filled[target_log].astype('float32')\n",
    "\n",
    "        # Prepare training data by merging all available logs\n",
    "        merged_data['SB_DEPTH_cm'] = merged_data['SB_DEPTH_cm'].astype('float32')\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = merged_data[features].copy()\n",
    "        y = merged_data[target_log].copy()\n",
    "\n",
    "        # Apply weights to features\n",
    "        X_weighted = apply_feature_weights(X, 'xgb')\n",
    "\n",
    "        # Split into training and prediction sets\n",
    "        X_train = X_weighted[~gap_mask]\n",
    "        y_train = y[~gap_mask]\n",
    "        X_pred = X_weighted[gap_mask]\n",
    "\n",
    "        # Handle outliers using IQR method\n",
    "        quantile_cutoff = 0.025 #fraction at both tail ends to be neglected in ML training\n",
    "        Q1 = y_train.quantile(quantile_cutoff)\n",
    "        Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "        X_train = X_train[outlier_mask]\n",
    "        y_train = y_train[outlier_mask]\n",
    "\n",
    "        # Create feature pipeline\n",
    "        feature_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=True)),\n",
    "            ('selector', SelectKBest(score_func=f_regression, k='all'))\n",
    "        ])\n",
    "\n",
    "        # Process features\n",
    "        X_train_processed = feature_pipeline.fit_transform(X_train, y_train)\n",
    "        X_pred_processed = feature_pipeline.transform(X_pred)\n",
    "\n",
    "        # Convert processed arrays to float32\n",
    "        X_train_processed = X_train_processed.astype('float32')\n",
    "        X_pred_processed = X_pred_processed.astype('float32')\n",
    "        y_train = y_train.astype('float32')\n",
    "\n",
    "        # Initialize and train XGBoost model with optimized hyperparameters for better accuracy\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=5000,           # More trees for better learning\n",
    "            learning_rate=0.003,         # Slower learning rate to prevent overfitting\n",
    "            max_depth=10,                # Reduced to prevent overfitting\n",
    "            min_child_weight=5,          # Increased to make model more conservative\n",
    "            subsample=0.75,              # sample ratio\n",
    "            colsample_bytree=0.75,       # feature sampling\n",
    "            gamma=0.2,                   # Added minimum loss reduction\n",
    "            reg_alpha=0.3,               # L1 regularization to reduce overfitting\n",
    "            reg_lambda=3.0,              # Increased L2 regularization\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train_processed, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model.predict(X_pred_processed).astype('float32')\n",
    "\n",
    "        # Fill gaps with predictions\n",
    "        target_data_filled.loc[gap_mask, target_log] = predictions\n",
    "\n",
    "    elif ml_method == 'xgblgbm':\n",
    "        target_data_filled[target_log] = target_data_filled[target_log].astype('float32')\n",
    "        \n",
    "        # Add cyclical features based on depth\n",
    "        periods = [10, 50, 100]  # Example periods in cm\n",
    "        for period in periods:\n",
    "            merged_data[f'SB_DEPTH_cm_sin'] = np.sin(2 * np.pi * merged_data['SB_DEPTH_cm'] / period)\n",
    "            merged_data[f'SB_DEPTH_cm_cos'] = np.cos(2 * np.pi * merged_data['SB_DEPTH_cm'] / period)\n",
    "            features.extend([f'SB_DEPTH_cm_sin', f'SB_DEPTH_cm_cos'])\n",
    "\n",
    "        # Prepare training data by merging all available logs\n",
    "        merged_data['SB_DEPTH_cm'] = merged_data['SB_DEPTH_cm'].astype('float32')\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = merged_data[features].copy()\n",
    "        y = merged_data[target_log].copy()\n",
    "\n",
    "        # Apply weights to features\n",
    "        X_weighted = apply_feature_weights(X, 'xgblgbm')\n",
    "\n",
    "        # Split into training and prediction sets\n",
    "        X_train = X_weighted[~gap_mask]\n",
    "        y_train = y[~gap_mask]\n",
    "        X_pred = X_weighted[gap_mask]\n",
    "\n",
    "        # Create feature pipeline\n",
    "        feature_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=True)),\n",
    "            ('selector', SelectKBest(score_func=f_regression, k=min(50, X_train.shape[0]//10)))  # Limit number of features\n",
    "        ])\n",
    "\n",
    "        # Process features\n",
    "        X_train_processed = feature_pipeline.fit_transform(X_train, y_train)\n",
    "        X_pred_processed = feature_pipeline.transform(X_pred)\n",
    "\n",
    "        # Convert processed arrays to float32\n",
    "        X_train_processed = X_train_processed.astype('float32')\n",
    "        X_pred_processed = X_pred_processed.astype('float32')\n",
    "        y_train = y_train.astype('float32')\n",
    "\n",
    "        # Initialize models\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            n_estimators=3000,           # More trees for better learning\n",
    "            learning_rate=0.003,         # Slower learning rate to prevent overfitting\n",
    "            max_depth=10,                # Reduced to prevent overfitting\n",
    "            min_child_weight=5,          # Increased to make model more conservative\n",
    "            subsample=0.75,              # sample ratio\n",
    "            colsample_bytree=0.75,       # feature sampling\n",
    "            gamma=0.2,                   # Added minimum loss reduction\n",
    "            reg_alpha=0.3,               # L1 regularization to reduce overfitting\n",
    "            reg_lambda=3.0,              # Increased L2 regularization\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=3000,           # Increased for more stable predictions\n",
    "            learning_rate=0.003,         # Reduced for more stable learning\n",
    "            max_depth=6,                 # Further reduced depth to prevent overfitting\n",
    "            num_leaves=20,               # Reduced leaves for simpler trees\n",
    "            min_child_samples=50,        # Increased to require more samples per leaf\n",
    "            subsample=0.9,               # Increased sample ratio for stability\n",
    "            colsample_bytree=0.9,        # Increased feature sampling for stability\n",
    "            reg_alpha=0.3,               # Increased L1 regularization\n",
    "            reg_lambda=3.0,              # Increased L2 regularization\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            force_col_wise=True,         # Remove threading overhead\n",
    "            verbose=-1                   # Reduce verbosity\n",
    "        )\n",
    "\n",
    "        # Train both models\n",
    "        xgb_model.fit(X_train_processed, y_train)\n",
    "        lgb_model.fit(X_train_processed, y_train)\n",
    "\n",
    "        # Make predictions with both models\n",
    "        xgb_predictions = xgb_model.predict(X_pred_processed).astype('float32')\n",
    "        lgb_predictions = lgb_model.predict(X_pred_processed).astype('float32')\n",
    "\n",
    "        # Ensemble predictions (simple average)\n",
    "        predictions = (xgb_predictions + lgb_predictions) / 2\n",
    "\n",
    "        # Fill gaps with predictions\n",
    "        target_data_filled.loc[gap_mask, target_log] = predictions\n",
    "\n",
    "    # Save to CSV if requested\n",
    "    if output_csv:\n",
    "        output_path = os.path.join(output_dir, f\"{core_name}_{target_log.split('_')[0]}_MLfilled.csv\")\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        target_data_filled.to_csv(output_path, index=False)\n",
    "\n",
    "    return target_data_filled, gap_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to process and fill logs with chosen ML methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_fill_logs(mother_dir, core_name, core_length, clean_output_folder, filled_output_folder, ml_method='xgblgbm'):\n",
    "    \"\"\"\n",
    "    Process and fill gaps in core log data using specified ML method.\n",
    "    \n",
    "    Args:\n",
    "        mother_dir (str): Path to the mother directory containing the data\n",
    "        core_name (str): Name of the core being processed\n",
    "        core_length (int): Length of the core (In any length unit. Just be sure it is consistent across the data files)\n",
    "        clean_output_folder (str): Relative path to folder containing cleaned data files\n",
    "        filled_output_folder (str): Relative path to folder for saving gap-filled data files\n",
    "        ml_method (str): Machine learning method to use - 'rf' for Random Forest, \n",
    "                                                          'rftc' for Random Forest with trend constraints,\n",
    "                                                          'xgb' for XGBoost, \n",
    "                                                          'xgblgbm' for XGBoost + LightGBM (default)\n",
    "                                            \n",
    "    \"\"\"\n",
    "    # Create filled output folder if it doesn't exist\n",
    "    filled_output_path = os.path.join(mother_dir, filled_output_folder)\n",
    "    os.makedirs(filled_output_path, exist_ok=True)\n",
    "    \n",
    "    # Read processed data files and check if they exist\n",
    "    data_files = {\n",
    "        'ct': f'{core_name}_CT_clean.csv',\n",
    "        'rgb': f'{core_name}_RGB_clean.csv',\n",
    "        'mst': f'{core_name}_MST_clean.csv',\n",
    "        'hrms': f'{core_name}_hiresMS_clean.csv'\n",
    "    }\n",
    "    \n",
    "    data_dict = {}\n",
    "    for key, filename in data_files.items():\n",
    "        filepath = os.path.join(mother_dir, clean_output_folder, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            data = pd.read_csv(filepath)\n",
    "            if not data.empty:\n",
    "                data_dict[key] = data\n",
    "\n",
    "    # Create feature data dictionary using existing data\n",
    "    feature_data = {}\n",
    "    if 'ct' in data_dict and 'CT' in data_dict['ct'].columns and not data_dict['ct'][['SB_DEPTH_cm', 'CT']].empty:\n",
    "        feature_data['ct'] = (data_dict['ct'], ['SB_DEPTH_cm', 'CT'])\n",
    "    if 'rgb' in data_dict:\n",
    "        rgb_cols = ['SB_DEPTH_cm', 'R', 'G', 'B', 'Lumin']\n",
    "        valid_rgb_cols = ['SB_DEPTH_cm'] + [col for col in rgb_cols[1:] if col in data_dict['rgb'].columns and not data_dict['rgb'][col].empty]\n",
    "        if len(valid_rgb_cols) > 1:  # Must have at least depth and one measurement\n",
    "            feature_data['rgb'] = (data_dict['rgb'], valid_rgb_cols)\n",
    "    if 'mst' in data_dict:\n",
    "        mst_cols = ['SB_DEPTH_cm', 'MS', 'PWVel_m/s', 'PWAmp', 'Den_gm/cc', 'ElecRes_ohmm']\n",
    "        valid_mst_cols = ['SB_DEPTH_cm'] + [col for col in mst_cols[1:] if col in data_dict['mst'].columns and not data_dict['mst'][col].empty]\n",
    "        if len(valid_mst_cols) > 1:\n",
    "            feature_data['mst'] = (data_dict['mst'], valid_mst_cols)\n",
    "    if 'hrms' in data_dict and 'hiresMS' in data_dict['hrms'].columns and not data_dict['hrms'][['SB_DEPTH_cm', 'hiresMS']].empty:\n",
    "        feature_data['hiresMS'] = (data_dict['hrms'], ['SB_DEPTH_cm', 'hiresMS'])\n",
    "\n",
    "    # Define logs to process based on available data\n",
    "    logs_to_process = []\n",
    "    if 'ct' in data_dict and 'CT' in data_dict['ct'].columns:\n",
    "        logs_to_process.append(('CT', 'CT', data_dict['ct']))\n",
    "    if 'rgb' in data_dict:\n",
    "        for col in ['R', 'G', 'B', 'Lumin']:\n",
    "            if col in data_dict['rgb'].columns and not data_dict['rgb'][col].empty:\n",
    "                logs_to_process.append((col, col, data_dict['rgb']))\n",
    "    if 'mst' in data_dict:\n",
    "        for col in ['MS', 'PWVel_m/s', 'PWAmp', 'Den_gm/cc', 'ElecRes_ohmm']:\n",
    "            if col in data_dict['mst'].columns and not data_dict['mst'][col].empty:\n",
    "                logs_to_process.append((col, col, data_dict['mst']))\n",
    "    if 'hrms' in data_dict and 'hiresMS' in data_dict['hrms'].columns:\n",
    "        logs_to_process.append(('hiresMS', 'hiresMS', data_dict['hrms']))\n",
    "\n",
    "    # Loop through each log and apply ML gap filling\n",
    "    for target_log, plot_name, data in logs_to_process:\n",
    "        # For RGB and Lumin logs, use specific feature set\n",
    "        if target_log in ['R', 'G', 'B', 'Lumin']:\n",
    "            # Create filtered feature data with only desired columns\n",
    "            filtered_features = {}\n",
    "            priority_features = ['hiresMS', 'ct', 'rgb']\n",
    "            \n",
    "            for key in priority_features:\n",
    "                if key in feature_data:\n",
    "                    if key == 'rgb':\n",
    "                        # Only include R,G,B,Lumin columns that exist\n",
    "                        df, cols = feature_data[key]\n",
    "                        valid_cols = ['SB_DEPTH_cm'] + [c for c in ['R','G','B','Lumin'] if c in cols and c in df.columns]\n",
    "                        if len(valid_cols) > 1:\n",
    "                            filtered_features[key] = (df, valid_cols)\n",
    "                    elif key == 'ct':\n",
    "                        filtered_features[key] = feature_data[key]\n",
    "                    elif key == 'hiresMS':\n",
    "                        filtered_features['hiresMS'] = feature_data['hiresMS']\n",
    "            \n",
    "            # Add density if available\n",
    "            if 'mst' in feature_data:\n",
    "                df, cols = feature_data['mst']\n",
    "                if 'Den_gm/cc' in cols and 'Den_gm/cc' in df.columns:\n",
    "                    filtered_features['mst'] = (df, ['SB_DEPTH_cm', 'Den_gm/cc'])\n",
    "                    \n",
    "            filled_data, gap_mask = fill_gaps_with_ml(\n",
    "                target_log=target_log,\n",
    "                All_logs=filtered_features,\n",
    "                output_csv=True,\n",
    "                output_dir=mother_dir + filled_output_folder,\n",
    "                core_name=core_name,\n",
    "                ml_method=ml_method\n",
    "            )\n",
    "        else:\n",
    "            # Use all available features for other logs\n",
    "            filled_data, gap_mask = fill_gaps_with_ml(\n",
    "                target_log=target_log,\n",
    "                All_logs=feature_data,\n",
    "                output_csv=True,\n",
    "                output_dir=mother_dir + filled_output_folder,\n",
    "                core_name=core_name,\n",
    "                ml_method=ml_method\n",
    "            )\n",
    "            \n",
    "        # Plot filled data\n",
    "        plot_filled_data(\n",
    "            plot_name,\n",
    "            data,\n",
    "            filled_data,\n",
    "            core_length,\n",
    "            core_name\n",
    "        )\n",
    "\n",
    "    # Update RGB data if it exists\n",
    "    if 'rgb' in data_dict:\n",
    "        rgb_data = data_dict['rgb']\n",
    "        rgb_columns = ['R', 'G', 'B', 'Lumin']\n",
    "        for col in rgb_columns:\n",
    "            if col in rgb_data.columns:\n",
    "                filled_file = mother_dir + filled_output_folder + f'{core_name}_{col}_MLfilled.csv'\n",
    "                if os.path.exists(filled_file):\n",
    "                    filled_data = pd.read_csv(filled_file)\n",
    "                    if col in filled_data.columns:\n",
    "                        rgb_data[col] = filled_data[col]\n",
    "                        print(f\"Filled data from {os.path.basename(filled_file)} has replaced the original {col} column\")\n",
    "        rgb_data.to_csv(mother_dir + filled_output_folder + f'{core_name}_RGB_MLfilled.csv', index=False)\n",
    "        # Remove individual RGB filled files\n",
    "        for col in rgb_columns:\n",
    "            filled_file = mother_dir + filled_output_folder + f'{core_name}_{col}_MLfilled.csv'\n",
    "            if os.path.exists(filled_file):\n",
    "                os.remove(filled_file)\n",
    "                print(f\"Removed {os.path.basename(filled_file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Define core\n",
    "\n",
    "### **M9907-22PC**\n",
    "#### Load variables and data structure for plotting and ML data filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define core variables\n",
    "# cruise_name = \"M9907\"\n",
    "# core_name = \"22PC\"\n",
    "# total_length_cm = 501  # Core length in cm\n",
    "\n",
    "# # Define M9907-22PC data configuration for data cleaning\n",
    "# data_config = {\n",
    "#     'mother_dir': '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/',  # Root directory containing core data\n",
    "#     'core_name': f'{cruise_name}-{core_name}',  # Name of the core being processed\n",
    "#     'core_length': total_length_cm,  # Total length of the core in cm\n",
    "#     'data_folder': f'_compiled_logs/{cruise_name}-{core_name}/',  # Folder containing raw data files\n",
    "#     'clean_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_clean/',  # Output folder for cleaned files\n",
    "#     'filled_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_filled/',  # Output folder for files wtih ML data-filling\n",
    "#     'thresholds': {\n",
    "#         'ms': ['>', 150, 1],      # [condition, threshold, buffer] for magnetic susceptibility\n",
    "#         'pwvel': ['>', 1085, 3],  # [condition, threshold, buffer] for P-wave velocity\n",
    "#         'den': ['<', 1, 1],       # [condition, threshold, buffer] for density\n",
    "#         'elecres': ['<', 0, 1],   # [condition, threshold, buffer] for electrical resistivity\n",
    "#         'hiresms': ['<=', 24, 1]  # [condition, threshold, buffer] for high-resolution magnetic susceptibility\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **M9907-23PC**\n",
    "#### Load variables and data structure for plotting and ML data filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define variables for the core\n",
    "# cruise_name = \"M9907\"\n",
    "# core_name = \"23PC\"\n",
    "# total_length_cm = 783  # Core length in cm\n",
    "\n",
    "# # Define data configuration for data cleaning\n",
    "# data_config = {\n",
    "#     'mother_dir': '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/',\n",
    "#     'core_name': f'{cruise_name}-{core_name}',\n",
    "#     'core_length': total_length_cm,\n",
    "#     'data_folder': f'_compiled_logs/{cruise_name}-{core_name}/',\n",
    "#     'clean_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_clean/',  # Output folder for cleaned files\n",
    "#     'filled_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_filled/',  # Output folder for files wtih ML data-filling\n",
    "#     'thresholds': {\n",
    "#         'ms': ['>', 150, 1],\n",
    "#         'pwvel': ['>=', 1075, 2], \n",
    "#         'den': ['<', 1.15, 1],\n",
    "#         'elecres': ['<', 0, 1],\n",
    "#         'hiresms': ['<=', 13, 1]\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **M9907-22TC**\n",
    "#### Load variables and data structure for plotting and ML data filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define M9907-22TC data configuration for data cleaning\n",
    "# data_config = {\n",
    "#     'mother_dir': '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/',\n",
    "#     'core_name': 'M9907-22TC',\n",
    "#     'core_length': 173,\n",
    "#     'data_folder': '_compiled_logs/M9907-22TC/',\n",
    "#     'clean_output_folder': '_compiled_logs/M9907-22TC/ML_clean/',  # Output folder for cleaned files\n",
    "#     'filled_output_folder': '_compiled_logs/M9907-22TC/ML_filled/',  # Output folder for files wtih ML data-filling\n",
    "#     'thresholds': {\n",
    "#         'ms': ['>', 150, 1],\n",
    "#         'pwvel': ['>=', 1060, 1], \n",
    "#         'pwamp': ['>', 40, 1], \n",
    "#         'den': ['<', 1.1, 1],\n",
    "#         'elecres': ['<', 0, 1]\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **M9907-25PC**\n",
    "#### Load variables and data structure for plotting and ML data filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define variables for M9907-25PC\n",
    "# cruise_name = \"M9907\"\n",
    "# core_name = \"25PC\"\n",
    "# total_length_cm = 797  # Core length in cm\n",
    "\n",
    "# # Define M9907-25PC data configuration for data cleaning\n",
    "# data_config = {\n",
    "#     'mother_dir': '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/',\n",
    "#     'core_name': f'{cruise_name}-{core_name}',\n",
    "#     'core_length': total_length_cm,\n",
    "#     'data_folder': f'_compiled_logs/{cruise_name}-{core_name}/',\n",
    "#     'clean_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_clean/',  # Output folder for cleaned files\n",
    "#     'filled_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_filled/',  # Output folder for files wtih ML data-filling\n",
    "#     'thresholds': {\n",
    "#         'ms': ['>', 180, 1],\n",
    "#         'pwvel': ['>=', 1077, 1], \n",
    "#         'den': ['<', 1.14, 1],\n",
    "#         'elecres': ['<', 0, 1],\n",
    "#         'hiresms': ['<=', 18, 1]\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **M9907-11PC**\n",
    "#### Load variables and data structure for plotting and ML data filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define variables \n",
    "# cruise_name = \"M9907\"\n",
    "# core_name = \"11PC\"\n",
    "# total_length_cm = 439  # Core length in cm\n",
    "\n",
    "# # Define data configuration for data cleaning\n",
    "# data_config = {\n",
    "#     'mother_dir': '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/',\n",
    "#     'core_name': f'{cruise_name}-{core_name}',\n",
    "#     'core_length': total_length_cm,\n",
    "#     'data_folder': f'_compiled_logs/{cruise_name}-{core_name}/',\n",
    "#     'clean_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_clean/',  # Output folder for cleaned files\n",
    "#     'filled_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_filled/',  # Output folder for files wtih ML data-filling\n",
    "#     'thresholds': {\n",
    "#         'ms': ['>', 180, 1],\n",
    "#         'pwvel': ['>=', 1077, 1], \n",
    "#         'den': ['<', 1.14, 1],\n",
    "#         'elecres': ['<', 0, 1],\n",
    "#         'hiresms': ['<=', 18, 1]\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **M9907-12PC**\n",
    "#### Load variables and data structure for plotting and ML data filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define variables \n",
    "# cruise_name = \"M9907\"\n",
    "# core_name = \"12PC\"\n",
    "# total_length_cm = 488  # Core length in cm\n",
    "\n",
    "# # Define data configuration for data cleaning\n",
    "# data_config = {\n",
    "#     'mother_dir': '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/',\n",
    "#     'core_name': f'{cruise_name}-{core_name}',\n",
    "#     'core_length': total_length_cm,\n",
    "#     'data_folder': f'_compiled_logs/{cruise_name}-{core_name}/',\n",
    "#     'clean_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_clean/',  # Output folder for cleaned files\n",
    "#     'filled_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_filled/',  # Output folder for files wtih ML data-filling\n",
    "#     'thresholds': {\n",
    "#         'ms': ['>', 180, 1],\n",
    "#         'pwvel': ['>=', 1075, 1], \n",
    "#         'den': ['<', 1.14, 1],\n",
    "#         'elecres': ['<', 0, 1],\n",
    "#         'hiresms': ['<=', 8, 1]\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RR0207-56PC**\n",
    "#### Load variables and data structure for plotting and ML data filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define variables \n",
    "# cruise_name = \"RR0207\"\n",
    "# core_name = \"56PC\"\n",
    "# total_length_cm = 794  # Core length in cm\n",
    "\n",
    "# # Define data configuration for data cleaning\n",
    "# data_config = {\n",
    "#     'mother_dir': '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/',\n",
    "#     'core_name': f'{cruise_name}-{core_name}',\n",
    "#     'core_length': total_length_cm,\n",
    "#     'data_folder': f'_compiled_logs/{cruise_name}-{core_name}/',\n",
    "#     'clean_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_clean/',  # Output folder for cleaned files\n",
    "#     'filled_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_filled/',  # Output folder for files wtih ML data-filling\n",
    "#     'thresholds': {\n",
    "#         'ms': ['>', 180, 1],\n",
    "#         'pwvel': ['<', 1450, 1], \n",
    "#         'pwamp': ['>', 20, 1],\n",
    "#         'den': ['<', 1.14, 1],\n",
    "#         'elecres': ['<', 0, 1],\n",
    "#         'hiresms': ['<=', 17, 1]\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **M9907-14TC**\n",
    "#### Load variables and data structure for plotting and ML data filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define variables \n",
    "# cruise_name = \"M9907\"\n",
    "# core_name = \"14TC\"\n",
    "# total_length_cm = 199  # Core length in cm\n",
    "\n",
    "# # Define data configuration for data cleaning\n",
    "# data_config = {\n",
    "#     'mother_dir': '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/',\n",
    "#     'core_name': f'{cruise_name}-{core_name}',\n",
    "#     'core_length': total_length_cm,\n",
    "#     'data_folder': f'_compiled_logs/{cruise_name}-{core_name}/',\n",
    "#     'clean_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_clean/',  # Output folder for cleaned files\n",
    "#     'filled_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_filled/',  # Output folder for files wtih ML data-filling\n",
    "#     'thresholds': {\n",
    "#         'ms': ['>', 180, 1],\n",
    "#         'pwvel': ['>=', 1075, 1], \n",
    "#         'den': ['<', 1.14, 1],\n",
    "#         'elecres': ['<', 0, 1],\n",
    "#         'hiresms': ['<=', 5, 1]\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **M9907-30PC**\n",
    "#### Load variables and data structure for plotting and ML data filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define variables \n",
    "# cruise_name = \"M9907\"\n",
    "# core_name = \"30PC\"\n",
    "# total_length_cm = 781  # Core length in cm\n",
    "\n",
    "# # Define data configuration for data cleaning\n",
    "# data_config = {\n",
    "#     'mother_dir': '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/',\n",
    "#     'core_name': f'{cruise_name}-{core_name}',\n",
    "#     'core_length': total_length_cm,\n",
    "#     'data_folder': f'_compiled_logs/{cruise_name}-{core_name}/',\n",
    "#     'clean_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_clean/',  # Output folder for cleaned files\n",
    "#     'filled_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_filled/',  # Output folder for files wtih ML data-filling\n",
    "#     'thresholds': {\n",
    "#         'ms': ['>', 150, 1],\n",
    "#         'pwvel': ['>=', 1075, 1], \n",
    "#         'den': ['<', 1.2, 1],\n",
    "#         'elecres': ['<', 0, 1],\n",
    "#         'hiresms': ['<=', 2, 1]\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **M9907-31PC**\n",
    "#### Load variables and data structure for plotting and ML data filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables \n",
    "cruise_name = \"M9907\"\n",
    "core_name = \"31PC\"\n",
    "total_length_cm = 767  # Core length in cm\n",
    "\n",
    "# Define data configuration for data cleaning\n",
    "data_config = {\n",
    "    'mother_dir': '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/',\n",
    "    'core_name': f'{cruise_name}-{core_name}',\n",
    "    'core_length': total_length_cm,\n",
    "    'data_folder': f'_compiled_logs/{cruise_name}-{core_name}/',\n",
    "    'clean_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_clean/',  # Output folder for cleaned files\n",
    "    'filled_output_folder': f'_compiled_logs/{cruise_name}-{core_name}/ML_filled/',  # Output folder for files wtih ML data-filling\n",
    "    'thresholds': {\n",
    "        'ms': ['>', 150, 1],\n",
    "        'pwvel': ['>=', 1075, 1], \n",
    "        'den': ['<', 1.2, 1],\n",
    "        'elecres': ['<', 0, 1],\n",
    "        'hiresms': ['<=', 1, 1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## **Execution**\n",
    "\n",
    "#### Define data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directories from data config\n",
    "core_name = data_config['core_name']\n",
    "mother_dir = data_config['mother_dir']\n",
    "core_length = data_config['core_length']\n",
    "clean_output_folder = data_config['clean_output_folder']\n",
    "filled_output_folder = data_config['filled_output_folder']\n",
    "\n",
    "# # Create clean output directory if it doesn't exist\n",
    "# clean_dir_path = mother_dir + clean_output_folder\n",
    "# if not os.path.exists(clean_dir_path):\n",
    "#     os.makedirs(clean_dir_path)\n",
    "#     print(f\"Created directory: {clean_dir_path}\")\n",
    "\n",
    "# # Create filled output directory if it doesn't exist\n",
    "# filled_dir_path = mother_dir + filled_output_folder\n",
    "# if not os.path.exists(filled_dir_path):\n",
    "#     os.makedirs(filled_dir_path)\n",
    "#     print(f\"Created directory: {filled_dir_path}\")\n",
    "\n",
    "clean_data_paths = {\n",
    "    'ct': mother_dir + clean_output_folder + f'{core_name}_CT_clean.csv',\n",
    "    'rgb': mother_dir + clean_output_folder + f'{core_name}_RGB_clean.csv', \n",
    "    'mst': mother_dir + clean_output_folder + f'{core_name}_MST_clean.csv',\n",
    "    'hrms': mother_dir + clean_output_folder + f'{core_name}_hiresMS_clean.csv'\n",
    "}\n",
    "\n",
    "# Filled Data paths for plotting\n",
    "filled_data_paths = {\n",
    "    'ct': mother_dir + filled_output_folder + f'{core_name}_CT_MLfilled.csv',\n",
    "    'rgb': mother_dir + filled_output_folder + f'{core_name}_RGB_MLfilled.csv',\n",
    "    'mst': mother_dir + filled_output_folder + f'{core_name}_MST_MLfilled.csv',\n",
    "    'hrms': mother_dir + filled_output_folder + f'{core_name}_hiresMS_MLfilled.csv'\n",
    "}\n",
    "\n",
    "### Variables for data cleaning\n",
    "# Load CT and RGB images\n",
    "ct_img_path = mother_dir + '_compiled_logs/' + core_name + '/' + core_name + '_CT.tiff'\n",
    "rgb_img_path = mother_dir + '_compiled_logs/' + core_name + '/' + core_name + '_RGB.tiff'\n",
    "\n",
    "ct_img = plt.imread(ct_img_path)\n",
    "rgb_img = plt.imread(rgb_img_path)\n",
    "\n",
    "# Column configurations (data structure) for plotting logs\n",
    "column_configs = {\n",
    "    'ct': {'data_col': 'CT', 'std_col': 'CT_std', 'depth_col': 'SB_DEPTH_cm'},\n",
    "    'rgb': {\n",
    "        'data_cols': ['R', 'G', 'B', 'Lumin'],\n",
    "        'std_cols': ['R_std', 'G_std', 'B_std', 'Lumin_std'],\n",
    "        'depth_col': 'SB_DEPTH_cm'\n",
    "    },\n",
    "    'mst': {\n",
    "        'density': {'data_col': 'Den_gm/cc', 'depth_col': 'SB_DEPTH_cm'},\n",
    "        'pwvel': {'data_col': 'PWVel_m/s', 'depth_col': 'SB_DEPTH_cm'},\n",
    "        'pwamp': {'data_col': 'PWAmp', 'depth_col': 'SB_DEPTH_cm'},\n",
    "        'elecres': {'data_col': 'ElecRes_ohmm', 'depth_col': 'SB_DEPTH_cm'},\n",
    "        'ms': {'data_col': 'MS', 'depth_col': 'SB_DEPTH_cm'}\n",
    "    },\n",
    "    'hrms': {'data_col': 'hiresMS', 'depth_col': 'SB_DEPTH_cm'}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run data cleaning function\n",
    "preprocess_core_data(data_config, shift_limit_multiplier=3.0)\n",
    "\n",
    "# Plot processed logs\n",
    "plt_title = f'{core_name} Cleaned Logs'\n",
    "fig, axes = plot_core_logs(clean_data_paths, column_configs, core_length, ct_img, rgb_img, title=plt_title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-based data gap filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_fill_logs(mother_dir=mother_dir,                      # Directory containing the core data\n",
    "                      core_name=core_name,                        # Name of the core being processed\n",
    "                      core_length=core_length,                    # Total length of the core\n",
    "                      clean_output_folder=clean_output_folder,    # Folder for cleaned data output\n",
    "                      filled_output_folder=filled_output_folder,  # Folder for filled data output\n",
    "                      ml_method='xgblgbm')                        # Available ml_method options: 'rf', 'rftc', 'xgb', 'xgblgbm'\n",
    "                                                                        # - 'rf': Random Forest ML\n",
    "                                                                        # - 'rftc': Random Forest ML with trend constraints\n",
    "                                                                        # - 'xgb': XGBoost ML\n",
    "                                                                        # - 'xgblgbm': XGBoost + LightGBM ML\n",
    "\n",
    "plt_title = f'{core_name} XGBoost + LightGBM ML-Filled Logs'      # Plot title for the filled logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ML-based gap-filled log diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot filled logs\n",
    "fig, axes = plot_core_logs(filled_data_paths, column_configs, core_length, ct_img, rgb_img, title=plt_title)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
