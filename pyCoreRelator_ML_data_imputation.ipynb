{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Log Data Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as mcoll  # For PolyCollection\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from scipy.signal import correlate, find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from PIL import Image\n",
    "\n",
    "# Create interaction features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions for Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for cleaning artifacts and noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_core_data(data_config):\n",
    "    \"\"\"\n",
    "    Preprocess core data by cleaning and scaling depth values using configurable parameters.\n",
    "    All processing actions are driven by the data_config content.\n",
    "    \"\"\"\n",
    "    # Get primary depth column from config\n",
    "    depth_col = data_config['depth_column']\n",
    "    \n",
    "    # Validate threshold conditions from column configs\n",
    "    valid_conditions = ['>', '<', '<=', '>=']\n",
    "    \n",
    "    # Validate thresholds for all data types that have them\n",
    "    for data_type, type_config in data_config['column_configs'].items():\n",
    "        if isinstance(type_config, dict):\n",
    "            # For nested configs like MST\n",
    "            if 'threshold' in type_config:\n",
    "                threshold = type_config['threshold']\n",
    "                if threshold[0] not in valid_conditions:\n",
    "                    raise ValueError(f\"Invalid condition '{threshold[0]}' for {data_type}.\")\n",
    "            elif 'rgb_threshold' in type_config:\n",
    "                # RGB threshold format is [min_val, max_val, buffer_size]\n",
    "                pass  # RGB thresholds don't use condition operators\n",
    "            else:\n",
    "                # Check nested configs\n",
    "                for sub_type, sub_config in type_config.items():\n",
    "                    if isinstance(sub_config, dict) and 'threshold' in sub_config:\n",
    "                        threshold = sub_config['threshold']\n",
    "                        if threshold[0] not in valid_conditions:\n",
    "                            raise ValueError(f\"Invalid condition '{threshold[0]}' for {data_type}.{sub_type}.\")\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(data_config['mother_dir'] + data_config['clean_output_folder'], exist_ok=True)\n",
    "\n",
    "    # Process data types that exist in both input_file_paths and column_configs\n",
    "    input_paths = data_config.get('input_file_paths', {})\n",
    "    clean_paths = data_config.get('clean_file_paths', {})\n",
    "    available_columns = data_config.get('column_configs', {})\n",
    "    \n",
    "    # Only process data types that have all necessary configurations\n",
    "    valid_data_types = set(input_paths.keys()) & set(clean_paths.keys()) & set(available_columns.keys())\n",
    "    \n",
    "    # Process each valid data type\n",
    "    for data_type in valid_data_types:\n",
    "        # Get data input path from config\n",
    "        data_path = data_config['mother_dir'] + input_paths[data_type]\n",
    "        \n",
    "        if os.path.exists(data_path):\n",
    "            print(f\"Processing {data_type} data...\")\n",
    "            data = pd.read_csv(data_path).astype('float32')\n",
    "            \n",
    "            if data is not None:\n",
    "                # Get type-specific configuration\n",
    "                type_config = available_columns[data_type]\n",
    "                \n",
    "                # Apply data-type specific processing based on config structure\n",
    "                if isinstance(type_config, dict):\n",
    "                    # Handle different config structures\n",
    "                    if 'data_cols' in type_config:\n",
    "                        # RGB-like data with multiple columns\n",
    "                        data_columns = type_config['data_cols']\n",
    "                        \n",
    "                        # Apply RGB-specific processing if threshold is defined\n",
    "                        if 'rgb_threshold' in type_config:\n",
    "                            min_val, max_val, buffer_size = type_config['rgb_threshold']\n",
    "                            buffer_indices = []\n",
    "                            for col in data_columns:\n",
    "                                if col in data.columns:\n",
    "                                    extreme_values = (data[col] <= min_val) | (data[col] >= max_val)\n",
    "                                    for i in range(len(data)):\n",
    "                                        if extreme_values[i]:\n",
    "                                            buffer_indices.extend(range(max(0, i-buffer_size), min(len(data), i+buffer_size+1)))\n",
    "                         \n",
    "                            if buffer_indices:\n",
    "                                std_columns = type_config.get('std_cols', [])\n",
    "                                data.loc[buffer_indices, data_columns + std_columns] = np.nan\n",
    "                    \n",
    "                    elif 'data_col' in type_config:\n",
    "                        # Single column data (CT, HRMS)\n",
    "                        data_col = type_config['data_col']\n",
    "                        \n",
    "                        # Apply threshold if defined\n",
    "                        if 'threshold' in type_config:\n",
    "                            condition, threshold_value, buffer_size = type_config['threshold']\n",
    "                            extreme_values = eval(f\"data['{data_col}'] {condition} {threshold_value}\")\n",
    "                            \n",
    "                            extreme_indices = []\n",
    "                            for i in range(len(data)):\n",
    "                                if extreme_values[i]:\n",
    "                                    extreme_indices.extend(range(max(0, i-buffer_size), min(len(data), i+buffer_size+1)))\n",
    "                            \n",
    "                            if extreme_indices:\n",
    "                                data.loc[extreme_indices, data_col] = np.nan\n",
    "                    \n",
    "                    else:\n",
    "                        # MST-like nested configuration\n",
    "                        # Map original column names to config keys for threshold lookup\n",
    "                        column_to_config_map = {}\n",
    "                        density_extreme_indices = []\n",
    "                        \n",
    "                        for log_type, config in type_config.items():\n",
    "                            if isinstance(config, dict) and 'data_col' in config:\n",
    "                                column_to_config_map[config['data_col']] = log_type\n",
    "                                \n",
    "                                # Special handling for density column (if exists)\n",
    "                                if log_type == 'density' and 'threshold' in config:\n",
    "                                    density_col = config['data_col']\n",
    "                                    if density_col in data.columns:\n",
    "                                        condition, threshold_value, buffer_size = config['threshold']\n",
    "                                        density_extreme = eval(f\"data['{density_col}'] {condition} {threshold_value}\")\n",
    "                                        for i in range(len(data)):\n",
    "                                            if density_extreme[i]:\n",
    "                                                density_extreme_indices.extend(range(max(0, i-buffer_size), min(len(data), i+buffer_size+1)))\n",
    "\n",
    "                        # Process each column using config-based thresholds\n",
    "                        for column in data.columns:\n",
    "                            if column in column_to_config_map:\n",
    "                                config_key = column_to_config_map[column]\n",
    "                                if 'threshold' in type_config[config_key]:\n",
    "                                    condition, threshold_value, buffer_size = type_config[config_key]['threshold']\n",
    "                                    extreme_values = eval(f\"data['{column}'] {condition} {threshold_value}\")\n",
    "                                    \n",
    "                                    extreme_indices = []\n",
    "                                    for i in range(len(data)):\n",
    "                                        if extreme_values[i]:\n",
    "                                            extreme_indices.extend(range(max(0, i-buffer_size), min(len(data), i+buffer_size+1)))\n",
    "                                    \n",
    "                                    # Combine with density extreme indices if applicable\n",
    "                                    all_extreme_indices = list(set(extreme_indices + density_extreme_indices))\n",
    "                                    if all_extreme_indices:\n",
    "                                        data.loc[all_extreme_indices, column] = np.nan\n",
    "\n",
    "                # Scale depth using configurable depth column\n",
    "                depth_scale = data_config['core_length'] / data[depth_col].max()\n",
    "                data[depth_col] = data[depth_col] * depth_scale\n",
    "                \n",
    "                # Use direct file path from config\n",
    "                output_path = data_config['mother_dir'] + data_config['clean_output_folder'] + clean_paths[data_type]\n",
    "                data.to_csv(output_path, index=False)\n",
    "                print(f\"Saved cleaned {data_type} data to: {output_path}\")\n",
    "        else:\n",
    "            print(f\"Raw file not found for {data_type}: {data_path}\")\n",
    "    \n",
    "    print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for plotting cleanned core images and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_core_logs(data_config, file_type='clean', title=None):\n",
    "    \"\"\"\n",
    "    Plot core logs using fully configurable parameters from data_config.\n",
    "    All plotting decisions are driven by column_configs content.\n",
    "    \"\"\"\n",
    "    # Get primary depth column from config\n",
    "    depth_col = data_config['depth_column']\n",
    "    \n",
    "    # Get file paths based on type\n",
    "    if file_type == 'clean':\n",
    "        data_paths = data_config.get('clean_file_paths', {})\n",
    "        output_folder = data_config['clean_output_folder']\n",
    "    else:\n",
    "        data_paths = data_config.get('filled_file_paths', {})\n",
    "        output_folder = data_config['filled_output_folder']\n",
    "    \n",
    "    # Get available column configs\n",
    "    available_columns = data_config.get('column_configs', {})\n",
    "    \n",
    "    # Only process data types that have both file path and column config\n",
    "    valid_data_types = set(data_paths.keys()) & set(available_columns.keys())\n",
    "    \n",
    "    # Build full file paths and load data\n",
    "    data = {}\n",
    "    for data_type in valid_data_types:\n",
    "        full_path = data_config['mother_dir'] + output_folder + data_paths[data_type]\n",
    "        if os.path.exists(full_path):\n",
    "            loaded_data = pd.read_csv(full_path)\n",
    "            if depth_col in loaded_data.columns:\n",
    "                data[data_type] = loaded_data\n",
    "    \n",
    "    if not data:\n",
    "        raise ValueError(\"No valid data files found to plot\")\n",
    "    \n",
    "    # Load Core Length and Name\n",
    "    core_length = data_config['core_length']\n",
    "    core_name = data_config['core_name']\n",
    "    \n",
    "    if title is None:\n",
    "        file_type_title = 'Cleaned' if file_type == 'clean' else 'ML-Filled'\n",
    "        title = f'{core_name} {file_type_title} Logs'\n",
    "    \n",
    "    # Determine plot structure based on column_configs\n",
    "    plot_panels = []\n",
    "    \n",
    "    # Process each data type according to its configuration in the order defined in column_configs\n",
    "    for data_type in available_columns.keys():\n",
    "        if data_type not in valid_data_types or data_type not in data:\n",
    "            continue\n",
    "            \n",
    "        type_config = available_columns[data_type]\n",
    "        \n",
    "        # Check for image configuration\n",
    "        if 'image_path' in type_config:\n",
    "            image_path = data_config['mother_dir'] + type_config['image_path']\n",
    "            if os.path.exists(image_path):\n",
    "                # Add image panel\n",
    "                plot_panels.append({\n",
    "                    'type': 'image',\n",
    "                    'data_type': data_type,\n",
    "                    'image_path': image_path,\n",
    "                    'colormap': type_config.get('image_colormap', 'gray')\n",
    "                })\n",
    "        \n",
    "        # Handle different config structures for data plotting\n",
    "        if 'data_col' in type_config:\n",
    "            # Single column data type\n",
    "            data_col = type_config['data_col']\n",
    "            if data_col in data[data_type].columns and not data[data_type][data_col].isna().all():\n",
    "                plot_panels.append({\n",
    "                    'type': 'data',\n",
    "                    'data_type': data_type,\n",
    "                    'columns': [data_col],\n",
    "                    'config': type_config\n",
    "                })\n",
    "                \n",
    "        elif 'data_cols' in type_config:\n",
    "            # Multi-column data type\n",
    "            data_cols = type_config['data_cols']\n",
    "            available_cols = [col for col in data_cols if col in data[data_type].columns \n",
    "                            and not data[data_type][col].isna().all()]\n",
    "            \n",
    "            if available_cols:\n",
    "                # Check for subplot grouping control\n",
    "                if type_config.get('group_in_subplot', True):\n",
    "                    # Plot all columns in one subplot\n",
    "                    plot_panels.append({\n",
    "                        'type': 'data',\n",
    "                        'data_type': data_type,\n",
    "                        'columns': available_cols,\n",
    "                        'config': type_config\n",
    "                    })\n",
    "                else:\n",
    "                    # Plot each column separately\n",
    "                    for col in available_cols:\n",
    "                        col_config = type_config.copy()\n",
    "                        col_config['data_cols'] = [col]\n",
    "                        plot_panels.append({\n",
    "                            'type': 'data',\n",
    "                            'data_type': data_type,\n",
    "                            'columns': [col],\n",
    "                            'config': col_config\n",
    "                        })\n",
    "                        \n",
    "        else:\n",
    "            # Nested configuration (like MST) - process in the order defined in config\n",
    "            for item_name in type_config.keys():\n",
    "                item_config = type_config[item_name]\n",
    "                if (isinstance(item_config, dict) and \n",
    "                    'data_col' in item_config):\n",
    "                    data_col = item_config['data_col']\n",
    "                    if data_col in data[data_type].columns and not data[data_type][data_col].isna().all():\n",
    "                        plot_panels.append({\n",
    "                            'type': 'data',\n",
    "                            'data_type': data_type,\n",
    "                            'columns': [data_col],\n",
    "                            'config': item_config,\n",
    "                            'item_name': item_name\n",
    "                        })\n",
    "    \n",
    "    if not plot_panels:\n",
    "        raise ValueError(\"No data available to plot\")\n",
    "    \n",
    "    # Create subplot\n",
    "    n_plots = len(plot_panels)\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(1.2*n_plots, 12))\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    fig.suptitle(title, fontweight='bold', fontsize=14)\n",
    "    \n",
    "    # Plot each panel\n",
    "    for i, panel in enumerate(plot_panels):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Only set y-label for the leftmost subplot\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Depth', fontweight='bold')\n",
    "        \n",
    "        if panel['type'] == 'image':\n",
    "            # Plot image\n",
    "            img = plt.imread(panel['image_path'])\n",
    "            ax.imshow(img, aspect='auto', extent=[0, 1, core_length, 0], cmap=panel['colormap'])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_xlabel(f'{panel[\"data_type\"].upper()}\\nImage', fontweight='bold', fontsize='small')\n",
    "            \n",
    "        elif panel['type'] == 'data':\n",
    "            # Plot data\n",
    "            _plot_data_panel(ax, panel, data, depth_col, core_length)\n",
    "        \n",
    "        # Set common y-axis properties\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_ylim(core_length, 0)\n",
    "        if i > 0:\n",
    "            ax.tick_params(axis='y', labelleft=False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "def _plot_data_panel(ax, panel, data, depth_col, core_length):\n",
    "    \"\"\"Helper function to plot a single data panel.\"\"\"\n",
    "    data_type = panel['data_type']\n",
    "    columns = panel['columns']\n",
    "    config = panel['config']\n",
    "    \n",
    "    df = data[data_type]\n",
    "    depth_values = df[depth_col].astype(np.float32)\n",
    "    \n",
    "    # Get plot styling from config\n",
    "    if 'plot_colors' in config:\n",
    "        plot_colors = config['plot_colors']\n",
    "    elif 'plot_color' in config:\n",
    "        plot_colors = [config['plot_color']]\n",
    "    else:\n",
    "        plot_colors = ['black'] * len(columns)\n",
    "    \n",
    "    if len(plot_colors) < len(columns):\n",
    "        plot_colors.extend(['black'] * (len(columns) - len(plot_colors)))\n",
    "    \n",
    "    # Plot each column\n",
    "    for j, col in enumerate(columns):\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        values = df[col].astype(np.float32)\n",
    "        color = plot_colors[j]\n",
    "        \n",
    "        # Plot main line\n",
    "        ax.plot(values, depth_values, color=color, linewidth=0.7)\n",
    "        \n",
    "        # Add standard deviation if available\n",
    "        std_col = None\n",
    "        if 'std_col' in config:\n",
    "            std_col = config['std_col']\n",
    "        elif 'std_cols' in config and j < len(config['std_cols']):\n",
    "            std_col = config['std_cols'][j]\n",
    "        \n",
    "        if std_col and std_col in df.columns:\n",
    "            std_values = df[std_col].astype(np.float32)\n",
    "            ax.fill_betweenx(depth_values,\n",
    "                           values - std_values,\n",
    "                           values + std_values,\n",
    "                           color=color, alpha=0.2, linewidth=0)\n",
    "        \n",
    "        # Add colormap visualization if configured\n",
    "        show_colormap = config.get('show_colormap', False)\n",
    "        if show_colormap:\n",
    "            colormap_name = config.get('colormap', 'viridis')\n",
    "            _add_colormap_visualization(ax, values, depth_values, colormap_name)\n",
    "        elif 'colormap_cols' in config and col in config['colormap_cols']:\n",
    "            colormap_name = config.get('colormap', 'viridis')\n",
    "            _add_colormap_visualization(ax, values, depth_values, colormap_name)\n",
    "    \n",
    "    # Set axis labels and styling\n",
    "    plot_label = config.get('plot_label', columns[0] if len(columns) == 1 else 'Data')\n",
    "    ax.set_xlabel(plot_label, fontweight='bold', fontsize='small')\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis='x', labelsize='x-small')\n",
    "\n",
    "\n",
    "def _add_colormap_visualization(ax, values, depths, colormap_name):\n",
    "    \"\"\"Helper function to add colormap visualization using PolyCollection.\"\"\"\n",
    "    # Compute normalization range ignoring NaNs\n",
    "    valid_values = values[~np.isnan(values)]\n",
    "    if len(valid_values) == 0:\n",
    "        return\n",
    "        \n",
    "    vmin, vmax = valid_values.min(), valid_values.max()\n",
    "    if np.isclose(vmin, vmax):\n",
    "        return\n",
    "        \n",
    "    norm = plt.Normalize(vmin, vmax)\n",
    "    cmap = plt.cm.get_cmap(colormap_name)\n",
    "    \n",
    "    polys = []\n",
    "    facecolors = []\n",
    "    \n",
    "    for i in range(len(depths) - 1):\n",
    "        if not (np.isnan(values.iloc[i]) or np.isnan(values.iloc[i+1])):\n",
    "            poly = [\n",
    "                (0, depths.iloc[i]),\n",
    "                (values.iloc[i], depths.iloc[i]),\n",
    "                (values.iloc[i+1], depths.iloc[i+1]),\n",
    "                (0, depths.iloc[i+1])\n",
    "            ]\n",
    "            polys.append(poly)\n",
    "            avg_val = (values.iloc[i] + values.iloc[i+1]) / 2\n",
    "            facecolors.append(cmap(norm(avg_val)))\n",
    "    \n",
    "    if polys:\n",
    "        pc = mcoll.PolyCollection(polys, facecolors=facecolors, edgecolors='none', alpha=0.95)\n",
    "        ax.add_collection(pc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions for Machine Learning to fill data gaps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for plotting filled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filled_data(target_log, original_data, filled_data, data_config, ML_type='ML'):\n",
    "    \"\"\"\n",
    "    Plot original and ML-filled data for a given log using configurable parameters.\n",
    "    All plotting parameters are driven by data_config content.\n",
    "    \n",
    "    Args:\n",
    "        target_log (str): Name of the log to plot\n",
    "        original_data (pd.DataFrame): Original data containing the log\n",
    "        filled_data (pd.DataFrame): Data with ML-filled gaps\n",
    "        data_config (dict): Configuration containing all parameters including depth column, plot labels, etc.\n",
    "        ML_type (str): Type of ML method used for title\n",
    "    \"\"\"\n",
    "    # Get parameters from config\n",
    "    depth_col = data_config['depth_column']\n",
    "    core_length = data_config['core_length']\n",
    "    core_name = data_config['core_name']\n",
    "    \n",
    "    # Find the configuration for this target log\n",
    "    target_config = None\n",
    "    target_data_type = None\n",
    "    \n",
    "    # Search through column_configs to find the target log\n",
    "    for data_type, type_config in data_config['column_configs'].items():\n",
    "        if isinstance(type_config, dict):\n",
    "            # Check for single column data types\n",
    "            if 'data_col' in type_config and type_config['data_col'] == target_log:\n",
    "                target_config = type_config\n",
    "                target_data_type = data_type\n",
    "                break\n",
    "            # Check for multi-column data types\n",
    "            elif 'data_cols' in type_config and target_log in type_config['data_cols']:\n",
    "                target_config = type_config\n",
    "                target_data_type = data_type\n",
    "                break\n",
    "            # Check for nested configurations (like MST)\n",
    "            else:\n",
    "                for sub_key, sub_config in type_config.items():\n",
    "                    if isinstance(sub_config, dict) and 'data_col' in sub_config and sub_config['data_col'] == target_log:\n",
    "                        target_config = sub_config\n",
    "                        target_data_type = data_type\n",
    "                        break\n",
    "                if target_config:\n",
    "                    break\n",
    "    \n",
    "    # Get plot label from config or use default\n",
    "    if target_config and 'plot_label' in target_config:\n",
    "        plot_label = target_config['plot_label']\n",
    "    else:\n",
    "        plot_label = f'{target_log}\\nBrightness'\n",
    "    \n",
    "    # Check if there are any gaps\n",
    "    has_gaps = original_data[target_log].isna().any()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(15, 3))\n",
    "    title_suffix = f'Use {ML_type} for Data Gap Filling' if has_gaps else \"(No Data Gap to be filled by ML)\"\n",
    "    fig.suptitle(f'{core_name} {target_log} Values {title_suffix}', fontweight='bold')\n",
    "\n",
    "    # Plot data with ML-predicted gaps only if gaps exist\n",
    "    if has_gaps:\n",
    "        ax.plot(filled_data[depth_col], filled_data[target_log], \n",
    "                color='red', label=f'ML Predicted {target_log}', linewidth=0.7, alpha=0.7)\n",
    "\n",
    "    # Plot original data\n",
    "    ax.plot(original_data[depth_col], original_data[target_log], \n",
    "            color='black', label=f'Original {target_log}', linewidth=0.7)\n",
    "\n",
    "    # Add uncertainty shade if std column exists - get std column name from config\n",
    "    std_col = None\n",
    "    if target_config:\n",
    "        # For single column configs\n",
    "        if 'std_col' in target_config:\n",
    "            std_col = target_config['std_col']\n",
    "        # For multi-column configs, find the corresponding std column\n",
    "        elif 'std_cols' in target_config and 'data_cols' in target_config:\n",
    "            data_cols = target_config['data_cols']\n",
    "            std_cols = target_config['std_cols']\n",
    "            if target_log in data_cols and len(std_cols) > data_cols.index(target_log):\n",
    "                std_col = std_cols[data_cols.index(target_log)]\n",
    "    \n",
    "    if std_col and std_col in original_data.columns:\n",
    "        ax.fill_between(original_data[depth_col],\n",
    "                       original_data[target_log] - original_data[std_col],\n",
    "                       original_data[target_log] + original_data[std_col],\n",
    "                       color='black', alpha=0.2, linewidth=0)\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_ylabel(plot_label, fontweight='bold', fontsize='small')\n",
    "    ax.set_xlabel('Depth')\n",
    "    ax.grid(True)\n",
    "    ax.invert_xaxis()\n",
    "    ax.set_xlim(0, core_length)\n",
    "    ax.tick_params(axis='y', labelsize='x-small')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Machine Learning Data Gap filling\n",
    "\n",
    "#### Helper Functions for fill_gaps_with_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_feature_data(target_log, All_logs, merge_tolerance, data_config):\n",
    "    \"\"\"Prepare merged feature data for ML training using configurable parameters.\"\"\"\n",
    "    # Get primary depth column from config\n",
    "    depth_col = data_config['depth_column']\n",
    "    \n",
    "    # Get target data from All_logs\n",
    "    target_data = None\n",
    "    for df, cols in All_logs.values():\n",
    "        if target_log in cols:\n",
    "            target_data = df.copy()\n",
    "            break\n",
    "    \n",
    "    if target_data is None:\n",
    "        raise ValueError(f\"Target log '{target_log}' not found in any dataset\")\n",
    "\n",
    "    # Convert depth column to float32 in target data\n",
    "    target_data[depth_col] = target_data[depth_col].astype('float32')\n",
    "    \n",
    "    # Prepare training data by merging all available logs\n",
    "    merged_data = target_data[[depth_col, target_log]].copy()\n",
    "    features = []\n",
    "    \n",
    "    # Merge feature dataframes one by one, using their own depth column\n",
    "    for df_name, (df, cols) in All_logs.items():\n",
    "        if target_log not in cols:  # Skip the target dataframe\n",
    "            df = df.copy()\n",
    "            df[depth_col] = df[depth_col].astype('float32')\n",
    "            # Rename depth column temporarily to avoid conflicts during merging\n",
    "            temp_depth_col = f'{depth_col}_{df_name}'\n",
    "            df = df.rename(columns={depth_col: temp_depth_col})\n",
    "            # Convert all numeric columns to float32\n",
    "            for col in cols:\n",
    "                if col != depth_col and df[col].dtype.kind in 'biufc':\n",
    "                    df[col] = df[col].astype('float32')\n",
    "            # Rename feature columns for merging\n",
    "            df_renamed = df.rename(columns={col: f'{df_name}_{col}' for col in cols if col != depth_col})\n",
    "            df_renamed = df_renamed.sort_values(temp_depth_col)\n",
    "            \n",
    "            # Perform merge_asof with tolerance for data alignment\n",
    "            merged_data = pd.merge_asof(\n",
    "                merged_data.sort_values(depth_col),\n",
    "                df_renamed,\n",
    "                left_on=depth_col,\n",
    "                right_on=temp_depth_col,\n",
    "                direction='nearest',\n",
    "                tolerance=merge_tolerance\n",
    "            )\n",
    "            \n",
    "            # Check for unmatched rows due to the tolerance constraint\n",
    "            unmatched = merged_data[temp_depth_col].isna().sum()\n",
    "            if unmatched > 0:\n",
    "                warnings.warn(f\"{unmatched} rows did not have a matching depth within tolerance for log '{df_name}'.\")\n",
    "            \n",
    "            # Add renamed feature columns to features list\n",
    "            features.extend([f'{df_name}_{col}' for col in cols if col != depth_col])\n",
    "            # Drop the temporary depth column used for merging\n",
    "            merged_data = merged_data.drop(columns=[temp_depth_col])\n",
    "    \n",
    "    # Add depth column as a feature\n",
    "    features.append(depth_col)\n",
    "    \n",
    "    return target_data, merged_data, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_weights(X, data_config):\n",
    "    \"\"\"Apply feature weights using configurable parameters from data_config.\"\"\"\n",
    "    X_weighted = X.copy()\n",
    "    column_configs = data_config['column_configs']\n",
    "    \n",
    "    # Process each data type in column_configs\n",
    "    for data_type, type_config in column_configs.items():\n",
    "        if isinstance(type_config, dict):\n",
    "            # Handle multi-column data types with feature_weights (like RGB)\n",
    "            if 'data_cols' in type_config and 'feature_weights' in type_config:\n",
    "                data_cols = type_config['data_cols']\n",
    "                weights = type_config['feature_weights']\n",
    "                \n",
    "                # Apply weights to each column\n",
    "                for col, weight in zip(data_cols, weights):\n",
    "                    matching_cols = [x_col for x_col in X_weighted.columns if col in x_col]\n",
    "                    for x_col in matching_cols:\n",
    "                        X_weighted[x_col] = (X_weighted[x_col] * weight).astype('float32')\n",
    "            \n",
    "            # Handle single column data types with feature_weight\n",
    "            elif 'data_col' in type_config and 'feature_weight' in type_config:\n",
    "                data_col = type_config['data_col']\n",
    "                weight = type_config['feature_weight']\n",
    "                \n",
    "                # Find matching columns in X that contain this data column name\n",
    "                matching_cols = [x_col for x_col in X_weighted.columns if data_col in x_col]\n",
    "                for x_col in matching_cols:\n",
    "                    X_weighted[x_col] = (X_weighted[x_col] * weight).astype('float32')\n",
    "            \n",
    "            # Handle nested configurations (like MST with multiple sub-types)\n",
    "            else:\n",
    "                for sub_type, sub_config in type_config.items():\n",
    "                    if isinstance(sub_config, dict) and 'data_col' in sub_config and 'feature_weight' in sub_config:\n",
    "                        data_col = sub_config['data_col']\n",
    "                        weight = sub_config['feature_weight']\n",
    "                        \n",
    "                        # Find matching columns in X that contain this data column name\n",
    "                        matching_cols = [x_col for x_col in X_weighted.columns if data_col in x_col]\n",
    "                        for x_col in matching_cols:\n",
    "                            X_weighted[x_col] = (X_weighted[x_col] * weight).astype('float32')\n",
    "    \n",
    "    return X_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_gap_predictions(df, gap_mask, ml_preds, target_log, data_config):\n",
    "    \"\"\"\n",
    "    Adjust ML predictions for gap rows in 'df' so that for each contiguous gap\n",
    "    segment (with both left and right boundaries available) the predictions are\n",
    "    blended with the linear interpolation between the boundary values.\n",
    "    \"\"\"\n",
    "    # Get primary depth column from config\n",
    "    depth_col = data_config['depth_column']\n",
    "    \n",
    "    # Get the integer positions (row numbers) of missing values\n",
    "    gap_positions = np.where(gap_mask.values)[0]\n",
    "    # Create a Series for easier handling; index = positions in df\n",
    "    preds_series = pd.Series(ml_preds, index=gap_positions)\n",
    "    \n",
    "    # Identify contiguous segments in the gap positions\n",
    "    segments = np.split(gap_positions, np.where(np.diff(gap_positions) != 1)[0] + 1)\n",
    "    \n",
    "    adjusted = preds_series.copy()\n",
    "    for seg in segments:\n",
    "        # seg is an array of row positions (in df) for a contiguous gap segment.\n",
    "        start_pos = seg[0]\n",
    "        end_pos = seg[-1]\n",
    "        \n",
    "        # Enforce trend constraints only if both boundaries exist.\n",
    "        if start_pos == 0 or end_pos == len(df) - 1:\n",
    "            continue  # Skip segments at the very beginning or end.\n",
    "        \n",
    "        # Retrieve boundary (observed) values and depths using configurable depth column\n",
    "        left_value = df.iloc[start_pos - 1][target_log]\n",
    "        right_value = df.iloc[end_pos + 1][target_log]\n",
    "        # Skip if boundaries are missing (should not happen if gap_mask is correct)\n",
    "        if pd.isna(left_value) or pd.isna(right_value):\n",
    "            continue\n",
    "        left_depth = df.iloc[start_pos - 1][depth_col]\n",
    "        right_depth = df.iloc[end_pos + 1][depth_col]\n",
    "        \n",
    "        # For each gap row in the segment, blend the ML prediction with linear interpolation\n",
    "        for pos in seg:\n",
    "            current_depth = df.iloc[pos][depth_col]\n",
    "            # Normalize the depth position (x in [0, 1])\n",
    "            if right_depth == left_depth:\n",
    "                x = 0.5\n",
    "            else:\n",
    "                x = (current_depth - left_depth) / (right_depth - left_depth)\n",
    "            # Compute the linear interpolation value at this depth\n",
    "            interp_val = left_value + (right_value - left_value) * x\n",
    "            # Define a weight that is 0 at the boundaries and 1 at the middle.\n",
    "            # Here we use: weight = 1 - 2*|x - 0.5|\n",
    "            weight = 1 - 2 * abs(x - 0.5)\n",
    "            weight = max(0, min(weight, 1))  # Ensure weight is between 0 and 1\n",
    "            # Blend: final = interpolation + weight*(ML_prediction - interpolation)\n",
    "            adjusted[pos] = interp_val + weight * (preds_series[pos] - interp_val)\n",
    "    \n",
    "    return adjusted.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    \"\"\"Helper function for parallel model training.\"\"\"\n",
    "    def train_wrapper(X_train, y_train, X_pred):\n",
    "        model.fit(X_train, y_train)\n",
    "        return model.predict(X_pred)\n",
    "    return train_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_random_forest(X_train, y_train, X_pred):\n",
    "    \"\"\"Apply Random Forest method.\"\"\"\n",
    "    # Handle outliers using IQR method\n",
    "    quantile_cutoff = 0.025\n",
    "    Q1 = y_train.quantile(quantile_cutoff)\n",
    "    Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "    X_train = X_train[outlier_mask]\n",
    "    y_train = y_train[outlier_mask]\n",
    "\n",
    "    def train_model(model):\n",
    "        model.fit(X_train, y_train)\n",
    "        return model.predict(X_pred)\n",
    "\n",
    "    # Initialize two ensemble models\n",
    "    models = [\n",
    "        RandomForestRegressor(n_estimators=1000,\n",
    "                              max_depth=30,\n",
    "                              min_samples_split=5,\n",
    "                              min_samples_leaf=5,\n",
    "                              max_features='sqrt',\n",
    "                              bootstrap=True,\n",
    "                              random_state=42,\n",
    "                              n_jobs=-1),\n",
    "        HistGradientBoostingRegressor(max_iter=800,\n",
    "                                      learning_rate=0.05,\n",
    "                                      max_depth=5,\n",
    "                                      min_samples_leaf=50,\n",
    "                                      l2_regularization=1.0,\n",
    "                                      random_state=42,\n",
    "                                      verbose=0)\n",
    "    ]\n",
    "\n",
    "    # Train models in parallel\n",
    "    predictions = Parallel(n_jobs=-1)(delayed(train_model)(model) for model in models)\n",
    "\n",
    "    # Ensemble predictions by averaging\n",
    "    ensemble_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "    return ensemble_predictions\n",
    "\n",
    "\n",
    "def _apply_random_forest_with_trend_constraints(X_train, y_train, X_pred, merged_data, gap_mask, target_log, data_config):\n",
    "    \"\"\"Apply Random Forest with trend constraints method.\"\"\"\n",
    "    # Handle outliers using IQR method\n",
    "    quantile_cutoff = 0.15\n",
    "    Q1 = y_train.quantile(quantile_cutoff)\n",
    "    Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "    X_train = X_train[outlier_mask]\n",
    "    y_train = y_train[outlier_mask]\n",
    "    \n",
    "    def train_model(model):\n",
    "        model.fit(X_train, y_train)\n",
    "        return model.predict(X_pred)\n",
    "    \n",
    "    # Initialize two ensemble models\n",
    "    models = [\n",
    "        RandomForestRegressor(n_estimators=1000,\n",
    "                              max_depth=30,\n",
    "                              min_samples_split=5,\n",
    "                              min_samples_leaf=5,\n",
    "                              max_features='sqrt',\n",
    "                              bootstrap=True,\n",
    "                              random_state=42,\n",
    "                              n_jobs=-1),\n",
    "        HistGradientBoostingRegressor(max_iter=800,\n",
    "                                      learning_rate=0.05,\n",
    "                                      max_depth=5,\n",
    "                                      min_samples_leaf=50,\n",
    "                                      l2_regularization=1.0,\n",
    "                                      random_state=42,\n",
    "                                      verbose=-1)\n",
    "    ]\n",
    "    \n",
    "    # Train models in parallel and average their predictions\n",
    "    predictions = Parallel(n_jobs=-1)(delayed(train_model)(model) for model in models)\n",
    "    ensemble_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Apply trend constraints using the helper function from original\n",
    "    adjusted_predictions = adjust_gap_predictions(merged_data, gap_mask, ensemble_predictions, target_log, data_config)\n",
    "    \n",
    "    return adjusted_predictions\n",
    "\n",
    "\n",
    "def _apply_xgboost(X_train, y_train, X_pred, data_config):\n",
    "    \"\"\"Apply XGBoost method with configurable feature weights.\"\"\"\n",
    "    # Apply feature weights BEFORE processing\n",
    "    X_train_weighted = apply_feature_weights(X_train, data_config)\n",
    "    X_pred_weighted = apply_feature_weights(X_pred, data_config)\n",
    "    \n",
    "    # Handle outliers using IQR method\n",
    "    quantile_cutoff = 0.025\n",
    "    Q1 = y_train.quantile(quantile_cutoff)\n",
    "    Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "    X_train_weighted = X_train_weighted[outlier_mask]\n",
    "    y_train = y_train[outlier_mask]\n",
    "\n",
    "    # Create feature pipeline\n",
    "    feature_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=True)),\n",
    "        ('selector', SelectKBest(score_func=f_regression, k='all'))\n",
    "    ])\n",
    "\n",
    "    # Process features\n",
    "    X_train_processed = feature_pipeline.fit_transform(X_train_weighted, y_train)\n",
    "    X_pred_processed = feature_pipeline.transform(X_pred_weighted)\n",
    "\n",
    "    # Convert processed arrays to float32\n",
    "    X_train_processed = X_train_processed.astype('float32')\n",
    "    X_pred_processed = X_pred_processed.astype('float32')\n",
    "    y_train = y_train.astype('float32')\n",
    "\n",
    "    # Initialize and train XGBoost model\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.003,\n",
    "        max_depth=10,\n",
    "        min_child_weight=5,\n",
    "        subsample=0.75,\n",
    "        colsample_bytree=0.75,\n",
    "        gamma=0.2,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=3.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_processed, y_train)\n",
    "    predictions = model.predict(X_pred_processed).astype('float32')\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def _apply_xgboost_lightgbm(X_train, y_train, X_pred, data_config):\n",
    "    \"\"\"Apply XGBoost + LightGBM ensemble method with configurable feature weights.\"\"\"\n",
    "    # Apply feature weights BEFORE processing\n",
    "    X_train_weighted = apply_feature_weights(X_train, data_config)\n",
    "    X_pred_weighted = apply_feature_weights(X_pred, data_config)\n",
    "    \n",
    "    # Create feature pipeline\n",
    "    feature_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=True))\n",
    "    ])\n",
    "\n",
    "    # Process features without selector first to get actual feature count\n",
    "    X_train_processed = feature_pipeline.fit_transform(X_train_weighted, y_train)\n",
    "    \n",
    "    # Now add selector with correct feature count\n",
    "    max_features = min(50, X_train.shape[0]//10, X_train_processed.shape[1])\n",
    "    selector = SelectKBest(score_func=f_regression, k=max_features)\n",
    "    X_train_processed = selector.fit_transform(X_train_processed, y_train)\n",
    "    X_pred_processed = feature_pipeline.transform(X_pred_weighted)\n",
    "    X_pred_processed = selector.transform(X_pred_processed)\n",
    "\n",
    "    # Convert processed arrays to float32\n",
    "    X_train_processed = X_train_processed.astype('float32')\n",
    "    X_pred_processed = X_pred_processed.astype('float32')\n",
    "    y_train = y_train.astype('float32')\n",
    "\n",
    "    # Initialize models\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=3000,\n",
    "        learning_rate=0.003,\n",
    "        max_depth=10,\n",
    "        min_child_weight=5,\n",
    "        subsample=0.75,\n",
    "        colsample_bytree=0.75,\n",
    "        gamma=0.2,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=3.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=3000,\n",
    "        learning_rate=0.003,\n",
    "        max_depth=6,\n",
    "        num_leaves=20,\n",
    "        min_child_samples=50,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=3.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        force_col_wise=True,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    # Train both models with warnings suppressed\n",
    "    import warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        xgb_model.fit(X_train_processed, y_train)\n",
    "        lgb_model.fit(X_train_processed, y_train, feature_name='auto')\n",
    "\n",
    "    # Make predictions with both models\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        xgb_predictions = xgb_model.predict(X_pred_processed).astype('float32')\n",
    "        lgb_predictions = lgb_model.predict(X_pred_processed).astype('float32')\n",
    "\n",
    "    # Ensemble predictions (simple average)\n",
    "    predictions = (xgb_predictions + lgb_predictions) / 2\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps_with_ml(target_log, All_logs, data_config, output_csv=True, \n",
    "                      merge_tolerance=3.0, ml_method='xgblgbm'):\n",
    "    \"\"\"\n",
    "    Fill gaps in target data using specified ML method.\n",
    "    All parameters and file paths are driven by data_config content.\n",
    "    \n",
    "    Args:\n",
    "        target_log (str): Name of the target column to fill gaps in.\n",
    "        All_logs (dict): Dictionary of dataframes containing feature data and target data.\n",
    "        data_config (dict): Configuration containing all parameters including file paths, core info, etc.\n",
    "        output_csv (bool): Whether to output filled data to CSV file.\n",
    "        merge_tolerance (float): Maximum allowed difference in depth for merging rows.\n",
    "        ml_method (str): ML method to use - 'rf', 'rftc', 'xgb', 'xgblgbm' (default)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (target_data_filled, gap_mask)\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if target_log is None or All_logs is None or data_config is None:\n",
    "        raise ValueError(\"target_log, All_logs, and data_config must be provided\")\n",
    "    \n",
    "    if ml_method not in ['rf', 'rftc', 'xgb', 'xgblgbm']:\n",
    "        raise ValueError(\"ml_method must be one of: 'rf', 'rftc', 'xgb', 'xgblgbm'\")\n",
    "    \n",
    "    # Get parameters from config\n",
    "    mother_dir = data_config['mother_dir']\n",
    "    filled_output_folder = data_config['filled_output_folder']\n",
    "    core_name = data_config['core_name']\n",
    "    \n",
    "    # Prepare feature data\n",
    "    target_data, merged_data, features = prepare_feature_data(target_log, All_logs, merge_tolerance, data_config)\n",
    "    \n",
    "    # Create a copy of the original data to hold the interpolated results\n",
    "    target_data_filled = target_data.copy()\n",
    "\n",
    "    # Identify gaps in target data\n",
    "    gap_mask = target_data[target_log].isna()\n",
    "    \n",
    "    # If no gaps exist, save to CSV if requested and return original data\n",
    "    if not gap_mask.any():\n",
    "        if output_csv:\n",
    "            # Generate output filename based on target_log and data_config\n",
    "            output_filename = f'{core_name}_{target_log.split(\"_\")[0]}_MLfilled.csv'\n",
    "            output_path = mother_dir + filled_output_folder + output_filename\n",
    "            target_data_filled.to_csv(output_path, index=False)\n",
    "        return target_data_filled, gap_mask\n",
    "\n",
    "    # Prepare features and target for ML\n",
    "    X = merged_data[features].copy()\n",
    "    y = merged_data[target_log].copy()\n",
    "\n",
    "    # Convert all features to float32\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype.kind in 'biufc':\n",
    "            X[col] = X[col].astype('float32')\n",
    "    y = y.astype('float32')\n",
    "\n",
    "    # Split into training (non-gap) and prediction (gap) sets\n",
    "    X_train = X[~gap_mask]\n",
    "    y_train = y[~gap_mask]\n",
    "    X_pred = X[gap_mask]\n",
    "\n",
    "    # Apply specific ML method\n",
    "    if ml_method == 'rf':\n",
    "        predictions = _apply_random_forest(X_train, y_train, X_pred)\n",
    "    elif ml_method == 'rftc':\n",
    "        predictions = _apply_random_forest_with_trend_constraints(X_train, y_train, X_pred, merged_data, gap_mask, target_log, data_config)\n",
    "    elif ml_method == 'xgb':\n",
    "        predictions = _apply_xgboost(X_train, y_train, X_pred, data_config)\n",
    "    elif ml_method == 'xgblgbm':\n",
    "        predictions = _apply_xgboost_lightgbm(X_train, y_train, X_pred, data_config)\n",
    "\n",
    "    # Fill gaps with predictions\n",
    "    target_data_filled.loc[gap_mask, target_log] = predictions\n",
    "    \n",
    "    # Save to CSV if requested\n",
    "    if output_csv:\n",
    "        # Generate output filename based on target_log and data_config\n",
    "        output_filename = f'{core_name}_{target_log.split(\"_\")[0]}_MLfilled.csv'\n",
    "        output_path = mother_dir + filled_output_folder + output_filename\n",
    "        target_data_filled.to_csv(output_path, index=False)\n",
    "\n",
    "    return target_data_filled, gap_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to process and fill logs with chosen ML methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_fill_logs(data_config, ml_method='xgblgbm'):\n",
    "    \"\"\"\n",
    "    Process and fill gaps in log data using ML methods with fully configurable parameters.\n",
    "    All data processing and file handling are driven by data_config content.\n",
    "    \"\"\"\n",
    "    # Get configurable parameters\n",
    "    depth_col = data_config['depth_column']\n",
    "    mother_dir = data_config['mother_dir']\n",
    "    core_name = data_config['core_name']\n",
    "    clean_output_folder = data_config['clean_output_folder']\n",
    "    filled_output_folder = data_config['filled_output_folder']\n",
    "    \n",
    "    os.makedirs(mother_dir + filled_output_folder, exist_ok=True)\n",
    "    \n",
    "    clean_paths = data_config.get('clean_file_paths', {})\n",
    "    filled_paths = data_config.get('filled_file_paths', {})\n",
    "    available_columns = data_config.get('column_configs', {})\n",
    "    valid_data_types = set(clean_paths.keys()) & set(available_columns.keys())\n",
    "    \n",
    "    # Load data with correct path construction\n",
    "    data_dict = {}\n",
    "    for data_type in valid_data_types:\n",
    "        full_path = mother_dir + clean_output_folder + clean_paths[data_type]\n",
    "        if os.path.exists(full_path):\n",
    "            data = pd.read_csv(full_path)\n",
    "            if not data.empty:\n",
    "                data_dict[data_type] = data\n",
    "\n",
    "    if not data_dict:\n",
    "        print(\"No valid data files found for processing\")\n",
    "        return\n",
    "\n",
    "    # Create feature data dictionary using configurable column names\n",
    "    feature_data = {}\n",
    "    \n",
    "    for data_type in valid_data_types:\n",
    "        if data_type in data_dict:\n",
    "            type_config = available_columns[data_type]\n",
    "            \n",
    "            # Handle single column data types\n",
    "            if 'data_col' in type_config:\n",
    "                data_col = type_config['data_col']\n",
    "                if data_col in data_dict[data_type].columns:\n",
    "                    feature_data[data_type] = (data_dict[data_type], [depth_col, data_col])\n",
    "            \n",
    "            # Handle multi-column data types\n",
    "            elif 'data_cols' in type_config:\n",
    "                valid_cols = [depth_col] + [col for col in type_config['data_cols'] \n",
    "                                           if col in data_dict[data_type].columns]\n",
    "                if len(valid_cols) > 1:\n",
    "                    feature_data[data_type] = (data_dict[data_type], valid_cols)\n",
    "            \n",
    "            # Handle nested configurations (like MST)\n",
    "            else:\n",
    "                data_cols = [depth_col]\n",
    "                for sub_key, sub_config in type_config.items():\n",
    "                    if isinstance(sub_config, dict) and 'data_col' in sub_config:\n",
    "                        data_col = sub_config['data_col']\n",
    "                        if data_col in data_dict[data_type].columns:\n",
    "                            data_cols.append(data_col)\n",
    "                if len(data_cols) > 1:\n",
    "                    feature_data[data_type] = (data_dict[data_type], data_cols)\n",
    "\n",
    "    if not feature_data:\n",
    "        print(\"No valid feature data found for ML processing\")\n",
    "        return\n",
    "\n",
    "    # ML method names for plotting\n",
    "    ml_names = {\n",
    "        'rf': 'Random Forest', \n",
    "        'rftc': 'Random Forest with Trend Constraints',\n",
    "        'xgb': 'XGBoost', \n",
    "        'xgblgbm': 'XGBoost + LightGBM'\n",
    "    }\n",
    "\n",
    "    # Collect target logs dynamically from column configurations\n",
    "    target_logs = []\n",
    "    \n",
    "    for data_type in valid_data_types:\n",
    "        if data_type in feature_data:\n",
    "            type_config = available_columns[data_type]\n",
    "            \n",
    "            # Handle multi-column data types (like RGB)\n",
    "            if 'data_cols' in type_config:\n",
    "                for col in type_config['data_cols']:\n",
    "                    if col in data_dict[data_type].columns:\n",
    "                        target_logs.append((col, data_type))\n",
    "            \n",
    "            # Handle single column data types\n",
    "            elif 'data_col' in type_config:\n",
    "                col = type_config['data_col']\n",
    "                if col in data_dict[data_type].columns:\n",
    "                    target_logs.append((col, data_type))\n",
    "            \n",
    "            # Handle nested configurations (like MST)\n",
    "            else:\n",
    "                for sub_key, sub_config in type_config.items():\n",
    "                    if isinstance(sub_config, dict) and 'data_col' in sub_config:\n",
    "                        col = sub_config['data_col']\n",
    "                        if col in data_dict[data_type].columns:\n",
    "                            target_logs.append((col, data_type))\n",
    "\n",
    "    # Process each target log\n",
    "    data_type_results = {}  # Store results by data type for consolidation\n",
    "    \n",
    "    for target_log, data_type in target_logs:\n",
    "        print(f\"Processing {target_log}...\")\n",
    "        \n",
    "        # Get source data\n",
    "        data = data_dict[data_type]\n",
    "        plot_name = target_log\n",
    "        \n",
    "        # Create filtered feature data based on configuration\n",
    "        type_config = available_columns[data_type]\n",
    "        \n",
    "        # For multi-column data types, create filtered features\n",
    "        if 'data_cols' in type_config:\n",
    "            # Use all other feature data types except the current one\n",
    "            filtered_features = {k: v for k, v in feature_data.items() if k != data_type}\n",
    "            filtered_features[data_type] = (data, [depth_col, target_log])\n",
    "            \n",
    "            # For RGB-like data, add specific features (like density) if configured\n",
    "            if data_type in ['rgb']:  # Can be extended for other multi-column types\n",
    "                # Add density from MST if available (configurable special case)\n",
    "                if 'mst' in feature_data:\n",
    "                    df, cols = feature_data['mst']\n",
    "                    # Find density column from MST configuration\n",
    "                    mst_config = available_columns.get('mst', {})\n",
    "                    density_config = None\n",
    "                    for sub_key, sub_config in mst_config.items():\n",
    "                        if isinstance(sub_config, dict) and 'data_col' in sub_config:\n",
    "                            if 'density' in sub_key.lower() or 'den_' in sub_config['data_col'].lower():\n",
    "                                density_config = sub_config\n",
    "                                break\n",
    "                    \n",
    "                    if density_config:\n",
    "                        density_col = density_config['data_col']\n",
    "                        if density_col in cols and density_col in df.columns:\n",
    "                            filtered_features['mst'] = (df, [depth_col, density_col])\n",
    "                    \n",
    "            filled_data, gap_mask = fill_gaps_with_ml(\n",
    "                target_log=target_log,\n",
    "                All_logs=filtered_features,\n",
    "                data_config=data_config,\n",
    "                output_csv=True,\n",
    "                ml_method=ml_method\n",
    "            )\n",
    "            plot_filled_data(plot_name, data, filled_data, data_config, ML_type=ml_names[ml_method])\n",
    "        else:\n",
    "            # For single column or nested data types, don't create individual files\n",
    "            filled_data, gap_mask = fill_gaps_with_ml(\n",
    "                target_log=target_log,\n",
    "                All_logs=feature_data,\n",
    "                data_config=data_config,\n",
    "                output_csv=False,\n",
    "                ml_method=ml_method\n",
    "            )\n",
    "            \n",
    "            # Store the filled results for this column by data type\n",
    "            if data_type not in data_type_results:\n",
    "                data_type_results[data_type] = {}\n",
    "            data_type_results[data_type][target_log] = filled_data[target_log]\n",
    "            \n",
    "            # Plot filled data for each column\n",
    "            plot_filled_data(plot_name, data, filled_data, data_config, ML_type=ml_names[ml_method])\n",
    "\n",
    "    # Create consolidated files for each data type using configured paths\n",
    "    for data_type, filled_columns in data_type_results.items():\n",
    "        if data_type in data_dict and data_type in filled_paths:\n",
    "            data_copy = data_dict[data_type].copy()\n",
    "            updated_columns = []\n",
    "            \n",
    "            for col, filled_values in filled_columns.items():\n",
    "                data_copy[col] = filled_values\n",
    "                updated_columns.append(col)\n",
    "            \n",
    "            # Save consolidated file using filled_file_paths from config\n",
    "            output_filename = filled_paths[data_type]\n",
    "            output_path = mother_dir + filled_output_folder + output_filename\n",
    "            data_copy.to_csv(output_path, index=False)\n",
    "            print(f\"Saved [{', '.join(updated_columns)}] to {output_filename}\")\n",
    "\n",
    "    # Consolidate multi-column data (like RGB) - remove individual files\n",
    "    multi_column_types = [dt for dt, config in available_columns.items() \n",
    "                         if 'data_cols' in config and dt in data_dict]\n",
    "    \n",
    "    for data_type in multi_column_types:\n",
    "        if data_type in filled_paths:\n",
    "            data_copy = data_dict[data_type].copy()\n",
    "            type_config = available_columns[data_type]\n",
    "            data_columns = type_config['data_cols']\n",
    "            updated_columns = []\n",
    "            \n",
    "            for col in data_columns:\n",
    "                if col in data_copy.columns:\n",
    "                    # Check for individual files and merge them\n",
    "                    individual_file = mother_dir + filled_output_folder + f'{core_name}_{col}_MLfilled.csv'\n",
    "                    if os.path.exists(individual_file):\n",
    "                        filled_data = pd.read_csv(individual_file)\n",
    "                        if col in filled_data.columns:\n",
    "                            data_copy[col] = filled_data[col]\n",
    "                            updated_columns.append(col)\n",
    "            \n",
    "            if updated_columns:\n",
    "                # Save consolidated file using filled_file_paths from config\n",
    "                output_filename = filled_paths[data_type]\n",
    "                output_path = mother_dir + filled_output_folder + output_filename\n",
    "                data_copy.to_csv(output_path, index=False)\n",
    "                print(f\"Saved [{', '.join(updated_columns)}] to {output_filename}\")\n",
    "                \n",
    "                # Remove individual files\n",
    "                for col in data_columns:\n",
    "                    individual_file = mother_dir + filled_output_folder + f'{core_name}_{col}_MLfilled.csv'\n",
    "                    if os.path.exists(individual_file):\n",
    "                        os.remove(individual_file)\n",
    "\n",
    "    print(\"ML-based gap filling completed for all target logs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### **Define data structure**\n",
    "\n",
    "#### Define core name and core length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_name = \"M9907-11PC\"  # Core name\n",
    "total_length_cm = 439     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-12PC\"  # Core name\n",
    "# total_length_cm = 488     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-14TC\"  # Core name\n",
    "# total_length_cm = 199     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-22PC\"  # Core name\n",
    "# total_length_cm = 501     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-22TC\"  # Core name\n",
    "# total_length_cm = 173     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-23PC\"  # Core name\n",
    "# total_length_cm = 783     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-25PC\"  # Core name\n",
    "# total_length_cm = 797     # Core length in cm\n",
    "\n",
    "# core_name = \"RR0207-56PC\"  # Core name\n",
    "# total_length_cm = 794     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-30PC\"  # Core name\n",
    "# total_length_cm = 781     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-31PC\"  # Core name\n",
    "# total_length_cm = 767     # Core length in cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define file path, data configuration, and outliner cut-off thresholds for ML data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data configuration for ML data imputation\n",
    "# This replaces all hardcoded column names and parameters in the functions\n",
    "\n",
    "# First get subfolder paths that were previously calculated outside the function\n",
    "def get_data_subfolders(data_config):\n",
    "    \"\"\"\n",
    "    Determine MST and HRMS subfolder paths based on core name.\n",
    "    This function should be called outside of the main functions to provide subfolder paths.\n",
    "    \"\"\"\n",
    "    core_name = data_config['core_name']\n",
    "    \n",
    "    if core_name.startswith('M99'):\n",
    "        mst_subfolder = \"OSU orignal dataset/R-V_Melville99/Calibrated_MST/\"\n",
    "        hrms_subfolder = \"OSU orignal dataset/R-V_Melville99/M9907_point_mag/\"\n",
    "    elif core_name.startswith('RR02'):\n",
    "        mst_subfolder = \"OSU orignal dataset/R-V_Revelle02/Calibrated_MST/\"\n",
    "        hrms_subfolder = \"OSU orignal dataset/R-V_Revelle02/RR0207_point_mag/\"\n",
    "    else:\n",
    "        mst_subfolder = \"OSU orignal dataset/R-V_Melville99/Calibrated_MST/\"\n",
    "        hrms_subfolder = \"OSU orignal dataset/R-V_Melville99/M9907_point_mag/\"\n",
    "    \n",
    "    return mst_subfolder, hrms_subfolder\n",
    "\n",
    "# Calculate subfolder paths once\n",
    "mst_subfolder, hrms_subfolder = get_data_subfolders({'core_name': core_name})\n",
    "\n",
    "data_config = {\n",
    "    # Existing configuration (unchanged)\n",
    "    'mother_dir': '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/',\n",
    "    'core_name': core_name,\n",
    "    'core_length': total_length_cm,\n",
    "    'data_folder': f'_compiled_logs/{core_name}/',\n",
    "    'clean_output_folder': f'_compiled_logs/{core_name}/ML_clean/',\n",
    "    'filled_output_folder': f'_compiled_logs/{core_name}/ML_filled/',\n",
    "    \n",
    "    # NEW: Input file paths for raw data (replaces hardcoded paths and subfolder dependency)\n",
    "    'input_file_paths': {\n",
    "        'ct': f'_compiled_logs/{core_name}/{core_name}_CT.csv',\n",
    "        'rgb': f'_compiled_logs/{core_name}/{core_name}_RGB.csv',\n",
    "        'hrms': f'{hrms_subfolder}{core_name}_point_mag.csv',\n",
    "        'mst': f'{mst_subfolder}{core_name}_MST.csv'\n",
    "    },\n",
    "    \n",
    "    # Existing file paths (unchanged)\n",
    "    'clean_file_paths': {\n",
    "        'ct': f'{core_name}_CT_clean.csv',\n",
    "        'rgb': f'{core_name}_RGB_clean.csv',\n",
    "        'hrms': f'{core_name}_hiresMS_clean.csv',\n",
    "        'mst': f'{core_name}_MST_clean.csv'\n",
    "    },\n",
    "    \n",
    "    'filled_file_paths': {\n",
    "        'ct': f'{core_name}_CT_MLfilled.csv',\n",
    "        'rgb': f'{core_name}_RGB_MLfilled.csv',\n",
    "        'hrms': f'{core_name}_hiresMS_MLfilled.csv',\n",
    "        'mst': f'{core_name}_MST_MLfilled.csv'\n",
    "    },\n",
    "    \n",
    "    # Primary depth column name used throughout all functions\n",
    "    'depth_column': 'SB_DEPTH_cm',\n",
    "    \n",
    "    # Enhanced column configs with all information consolidated including plotting controls\n",
    "    'column_configs': {\n",
    "        'ct': {\n",
    "            'data_col': 'CT', \n",
    "            'std_col': 'CT_std', \n",
    "            'depth_col': 'SB_DEPTH_cm',\n",
    "            'plot_label': 'CT\\nBrightness',\n",
    "            'plot_colors': ['black'],\n",
    "            'show_colormap': True,\n",
    "            'colormap': 'jet',\n",
    "            'image_path': f'_compiled_logs/{core_name}/{core_name}_CT.tiff'\n",
    "        },\n",
    "        'rgb': {\n",
    "            'data_cols': ['R', 'G', 'B', 'Lumin'],\n",
    "            'std_cols': ['R_std', 'G_std', 'B_std', 'Lumin_std'],\n",
    "            'depth_col': 'SB_DEPTH_cm',\n",
    "            'feature_weights': [0.3, 0.3, 0.3, 0.3],  # corresponds to ['R', 'G', 'B', 'Lumin']\n",
    "            'rgb_threshold': [35, 220, 2],  # [min_val, max_val, buffer_size] for extreme RGB values\n",
    "            'group_in_subplot': True,  # Plot all RGB channels together\n",
    "            'plot_label': 'RGB\\nChannels',\n",
    "            'plot_colors': ['red', 'green', 'blue', 'black'],  # Colors for R, G, B, Lumin\n",
    "            'colormap_cols': ['Lumin'],  # Which columns should show colormap visualization\n",
    "            'colormap': 'inferno',  # Colormap for Lumin\n",
    "            'image_path': f'_compiled_logs/{core_name}/{core_name}_RGB.tiff'\n",
    "        },\n",
    "        'hrms': {\n",
    "            'data_col': 'hiresMS', \n",
    "            'depth_col': 'SB_DEPTH_cm',\n",
    "            'plot_label': 'High-Res\\nMagnetic\\nSusceptibility\\n(SI)',\n",
    "            'plot_color': 'darkgreen',\n",
    "            'feature_weight': 3.0,\n",
    "            'threshold': ['<=', 19, 1]\n",
    "        },\n",
    "        'mst': {\n",
    "            'ms': {\n",
    "                'data_col': 'MS', \n",
    "                'depth_col': 'SB_DEPTH_cm',\n",
    "                'plot_label': 'Magnetic\\nSusceptibility\\n(SI)',\n",
    "                'plot_color': 'lightgreen',\n",
    "                'feature_weight': 0.05,\n",
    "                'threshold': ['>', 180, 1]\n",
    "            },\n",
    "            'density': {\n",
    "                'data_col': 'Den_gm/cc', \n",
    "                'depth_col': 'SB_DEPTH_cm',\n",
    "                'plot_label': 'Density\\n(g/cc)',\n",
    "                'plot_color': 'orange',\n",
    "                'feature_weight': 0.5,\n",
    "                'threshold': ['<', 1.14, 1]\n",
    "            },\n",
    "            'pwvel': {\n",
    "                'data_col': 'PWVel_m/s', \n",
    "                'depth_col': 'SB_DEPTH_cm',\n",
    "                'plot_label': 'P-wave\\nVelocity\\n(m/s)',\n",
    "                'plot_color': 'purple',\n",
    "                'feature_weight': 0.01,\n",
    "                'threshold': ['>=', 1076, 1]\n",
    "            },\n",
    "            'pwamp': {\n",
    "                'data_col': 'PWAmp', \n",
    "                'depth_col': 'SB_DEPTH_cm',\n",
    "                'plot_label': 'P-wave\\nAmplitude',\n",
    "                'plot_color': 'purple',\n",
    "                'feature_weight': 0.01,\n",
    "                'threshold': ['>=', 30, 1]\n",
    "            },\n",
    "            'elecres': {\n",
    "                'data_col': 'ElecRes_ohmm', \n",
    "                'depth_col': 'SB_DEPTH_cm',\n",
    "                'plot_label': 'Electrical\\nResistivity\\n(ohm-m)',\n",
    "                'plot_color': 'brown',\n",
    "                'threshold': ['<', 0, 1]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run data cleaning function - now completely driven by data_config\n",
    "print(\"Starting data cleaning...\")\n",
    "preprocess_core_data(data_config)\n",
    "\n",
    "# Plot processed logs using new function signature\n",
    "fig, axes = plot_core_logs(\n",
    "    data_config,                           # Data configuration containing all parameters\n",
    "    file_type='clean',                     # Type of data files to plot ('clean' or 'filled')\n",
    "    title=f'{core_name} Cleaned Logs'      # Title for the plot figure\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-based data gap filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_fill_logs(data_config,              # Data configuration containing all parameters\n",
    "                      ml_method='xgblgbm')      # Available ml_method options: 'rf', 'rftc', 'xgb', 'xgblgbm'\n",
    "                                                # - 'rf': Random Forest ML\n",
    "                                                # - 'rftc': Random Forest ML with trend constraints\n",
    "                                                # - 'xgb': XGBoost ML\n",
    "                                                # - 'xgblgbm': XGBoost + LightGBM ML         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ML-based gap-filled log diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ML-based gap-filled log diagram\n",
    "fig, axes = plot_core_logs(\n",
    "    data_config,                                              # Data configuration containing all parameters\n",
    "    file_type='filled',                                       # Type of data files to plot ('filled' for gap-filled data)\n",
    "    title=f'{core_name} XGBoost + LightGBM ML-Filled Logs'    # Title for the plot figure\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
