{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Log Data Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as mcoll  # For PolyCollection\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from scipy.signal import correlate, find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from PIL import Image\n",
    "\n",
    "# Create interaction features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions for Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for cleaning artifacts and noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_core_data(data_config):\n",
    "    \"\"\"\n",
    "    Preprocess core data by cleaning and scaling depth values using configurable parameters.\n",
    "    \"\"\"\n",
    "    # Get primary depth column from config\n",
    "    depth_col = data_config['depth_column']\n",
    "    \n",
    "    # Validate threshold conditions from column configs\n",
    "    valid_conditions = ['>', '<', '<=', '>=']\n",
    "    \n",
    "    # Check thresholds in MST configs\n",
    "    for log_type, config in data_config['column_configs']['mst'].items():\n",
    "        if 'threshold' in config:\n",
    "            threshold = config['threshold']\n",
    "            if threshold[0] not in valid_conditions:\n",
    "                raise ValueError(f\"Invalid condition '{threshold[0]}' for {log_type}.\")\n",
    "    \n",
    "    # Check threshold in HRMS config\n",
    "    if 'threshold' in data_config['column_configs']['hrms']:\n",
    "        threshold = data_config['column_configs']['hrms']['threshold']\n",
    "        if threshold[0] not in valid_conditions:\n",
    "            raise ValueError(f\"Invalid condition '{threshold[0]}' for hrms.\")\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(data_config['mother_dir'] + data_config['clean_output_folder'], exist_ok=True)\n",
    "\n",
    "    # Process CT data\n",
    "    ct_path = data_config['mother_dir'] + data_config['data_folder'] + f\"{data_config['core_name']}_CT.csv\"\n",
    "    if os.path.exists(ct_path):\n",
    "        ct_data = pd.read_csv(ct_path).astype('float64')\n",
    "        if ct_data is not None:\n",
    "            # Scale depth using configurable depth column\n",
    "            ct_depth_scale = data_config['core_length'] / ct_data[depth_col].max()\n",
    "            ct_data[depth_col] = ct_data[depth_col] * ct_depth_scale\n",
    "            # Use direct file path from config\n",
    "            ct_output_path = data_config['mother_dir'] + data_config['clean_output_folder'] + data_config['clean_file_paths']['ct']\n",
    "            ct_data.to_csv(ct_output_path, index=False)\n",
    "\n",
    "    # Process RGB data\n",
    "    rgb_path = data_config['mother_dir'] + data_config['data_folder'] + f\"{data_config['core_name']}_RGB.csv\"\n",
    "    if os.path.exists(rgb_path):\n",
    "        rgb_data = pd.read_csv(rgb_path).astype('float64')\n",
    "        if rgb_data is not None:\n",
    "            # Get RGB column names from config\n",
    "            rgb_config = data_config['column_configs']['rgb']\n",
    "            rgb_columns = rgb_config['data_cols']\n",
    "            \n",
    "            # Remove buffer for extreme RGB values (same logic as original)\n",
    "            buffer_indices_rgb = []\n",
    "            for col in rgb_columns:\n",
    "                if col in rgb_data.columns:\n",
    "                    extreme_rgb = (rgb_data[col] <= 35) | (rgb_data[col] >= 220)\n",
    "                    for i in range(len(rgb_data)):\n",
    "                        if extreme_rgb[i]:\n",
    "                            buffer_indices_rgb.extend(range(max(0, i-2), min(len(rgb_data), i+2+1)))\n",
    "         \n",
    "            if buffer_indices_rgb:\n",
    "                rgb_data.loc[buffer_indices_rgb, rgb_columns + [f'{col}_std' for col in rgb_columns]] = np.nan\n",
    "            \n",
    "            # Scale depth using configurable depth column\n",
    "            rgb_depth_scale = data_config['core_length'] / rgb_data[depth_col].max()\n",
    "            rgb_data[depth_col] = rgb_data[depth_col] * rgb_depth_scale\n",
    "            # Use direct file path from config\n",
    "            rgb_output_path = data_config['mother_dir'] + data_config['clean_output_folder'] + data_config['clean_file_paths']['rgb']\n",
    "            rgb_data.to_csv(rgb_output_path, index=False)\n",
    "\n",
    "    # Determine subfolder paths based on core name (same logic as original)\n",
    "    if data_config['core_name'].startswith('M99'):\n",
    "        mst_subfolder = \"OSU orignal dataset/R-V_Melville99/Calibrated_MST/\"\n",
    "        hrms_subfolder = \"OSU orignal dataset/R-V_Melville99/M9907_point_mag/\"\n",
    "    elif data_config['core_name'].startswith('RR02'):\n",
    "        mst_subfolder = \"OSU orignal dataset/R-V_Revelle02/Calibrated_MST/\"\n",
    "        hrms_subfolder = \"OSU orignal dataset/R-V_Revelle02/RR0207_point_mag/\"\n",
    "    else:\n",
    "        mst_subfolder = \"OSU orignal dataset/R-V_Melville99/Calibrated_MST/\"\n",
    "        hrms_subfolder = \"OSU orignal dataset/R-V_Melville99/M9907_point_mag/\"\n",
    "\n",
    "    # Process MST data\n",
    "    mst_path = data_config['mother_dir'] + mst_subfolder + f\"{data_config['core_name']}_MST.csv\"\n",
    "    if os.path.exists(mst_path):\n",
    "        mst_data = pd.read_csv(mst_path).astype('float64')\n",
    "        \n",
    "        if mst_data is not None:\n",
    "            # Get MST configs\n",
    "            mst_configs = data_config['column_configs']['mst']\n",
    "            \n",
    "            # Map original column names to config keys for threshold lookup\n",
    "            column_to_config_map = {}\n",
    "            for log_type, config in mst_configs.items():\n",
    "                if 'data_col' in config:\n",
    "                    column_to_config_map[config['data_col']] = log_type\n",
    "\n",
    "            # Get density column from config for extreme detection\n",
    "            density_col = mst_configs['density']['data_col']\n",
    "            density_extreme_indices = []\n",
    "            \n",
    "            if density_col in mst_data.columns and 'threshold' in mst_configs['density']:\n",
    "                condition, threshold_value, buffer_size = mst_configs['density']['threshold']\n",
    "                density_extreme = eval(f\"mst_data['{density_col}'] {condition} {threshold_value}\")\n",
    "                for i in range(len(mst_data)):\n",
    "                    if density_extreme[i]:\n",
    "                        density_extreme_indices.extend(range(max(0, i-buffer_size), min(len(mst_data), i+buffer_size+1)))\n",
    "\n",
    "            # Process each MST column using config-based thresholds\n",
    "            for column in mst_data.columns:\n",
    "                if column in column_to_config_map:\n",
    "                    config_key = column_to_config_map[column]\n",
    "                    if 'threshold' in mst_configs[config_key]:\n",
    "                        condition, threshold_value, buffer_size = mst_configs[config_key]['threshold']\n",
    "                        extreme_values = eval(f\"mst_data['{column}'] {condition} {threshold_value}\")\n",
    "                        \n",
    "                        extreme_indices = []\n",
    "                        for i in range(len(mst_data)):\n",
    "                            if extreme_values[i]:\n",
    "                                extreme_indices.extend(range(max(0, i-buffer_size), min(len(mst_data), i+buffer_size+1)))\n",
    "                        \n",
    "                        # Combine with density extreme indices\n",
    "                        all_extreme_indices = list(set(extreme_indices + density_extreme_indices))\n",
    "                        if all_extreme_indices:\n",
    "                            mst_data.loc[all_extreme_indices, column] = np.nan\n",
    "\n",
    "            # Scale depth using configurable depth column\n",
    "            mst_depth_scale = data_config['core_length'] / mst_data[depth_col].max()\n",
    "            mst_data[depth_col] = mst_data[depth_col] * mst_depth_scale\n",
    "            # Use direct file path from config\n",
    "            mst_output_path = data_config['mother_dir'] + data_config['clean_output_folder'] + data_config['clean_file_paths']['mst']\n",
    "            mst_data.to_csv(mst_output_path, index=False)\n",
    "\n",
    "    # Process HRMS data\n",
    "    hrms_path = data_config['mother_dir'] + hrms_subfolder + f\"{data_config['core_name']}_point_mag.csv\"\n",
    "    if os.path.exists(hrms_path):\n",
    "        hrms_data = pd.read_csv(hrms_path).astype('float64')\n",
    "        if hrms_data is not None and len(hrms_data) > 3:\n",
    "            # Get HRMS column name from config\n",
    "            hrms_col = data_config['column_configs']['hrms']['data_col']\n",
    "            \n",
    "            # Apply threshold using config\n",
    "            if 'threshold' in data_config['column_configs']['hrms']:\n",
    "                condition, threshold_value, buffer_size = data_config['column_configs']['hrms']['threshold']\n",
    "                extreme_hrms = eval(f\"hrms_data['{hrms_col}'] {condition} {threshold_value}\")\n",
    "                \n",
    "                extreme_indices = []\n",
    "                for i in range(len(hrms_data)):\n",
    "                    if extreme_hrms[i]:\n",
    "                        extreme_indices.extend(range(max(0, i-buffer_size), min(len(hrms_data), i+buffer_size+1)))\n",
    "                \n",
    "                if extreme_indices:\n",
    "                    hrms_data.loc[extreme_indices, hrms_col] = np.nan\n",
    "\n",
    "            # Scale depth using configurable depth column (same as other data types)\n",
    "            depth_scale_factor = data_config['core_length'] / hrms_data[depth_col].max()\n",
    "            hrms_data[depth_col] = hrms_data[depth_col] * depth_scale_factor\n",
    "            \n",
    "            # Use direct file path from config\n",
    "            hrms_output_path = data_config['mother_dir'] + data_config['clean_output_folder'] + data_config['clean_file_paths']['hrms']\n",
    "            hrms_data.to_csv(hrms_output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for plotting cleanned core images and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_core_logs(data_config, file_type='clean', title=None):\n",
    "    \"\"\"Plot core logs using configurable parameters\"\"\"\n",
    "    # Get primary depth column from config\n",
    "    depth_col = data_config['depth_column']\n",
    "    \n",
    "    # Get file paths based on type\n",
    "    if file_type == 'clean':\n",
    "        data_paths = data_config.get('clean_file_paths', {})\n",
    "        output_folder = data_config['clean_output_folder']\n",
    "    else:\n",
    "        data_paths = data_config.get('filled_file_paths', {})\n",
    "        output_folder = data_config['filled_output_folder']\n",
    "    \n",
    "    # Get available column configs\n",
    "    available_columns = data_config.get('column_configs', {})\n",
    "    \n",
    "    # Only process data types that have both file path and column config\n",
    "    valid_data_types = set(data_paths.keys()) & set(available_columns.keys())\n",
    "    \n",
    "    # Build full file paths\n",
    "    full_paths = {}\n",
    "    for data_type in valid_data_types:\n",
    "        full_paths[data_type] = data_config['mother_dir'] + output_folder + data_paths[data_type]\n",
    "\n",
    "    # Load images\n",
    "    ct_img_path = data_config['mother_dir'] + data_config['ct_image_path']\n",
    "    rgb_img_path = data_config['mother_dir'] + data_config['rgb_image_path']\n",
    "    \n",
    "    ct_img = plt.imread(ct_img_path) if os.path.exists(ct_img_path) else None\n",
    "    rgb_img = plt.imread(rgb_img_path) if os.path.exists(rgb_img_path) else None\n",
    "    \n",
    "    # Load Core Length and Name\n",
    "    core_length = data_config['core_length']\n",
    "    core_name = data_config['core_name']\n",
    "    \n",
    "    if title is None:\n",
    "        file_type_title = 'Cleaned' if file_type == 'clean' else 'ML-Filled'\n",
    "        title = f'{core_name} {file_type_title} Logs'\n",
    "    \n",
    "    # Load available data\n",
    "    data = {}\n",
    "    for key, path in full_paths.items():\n",
    "        if os.path.exists(path):\n",
    "            loaded_data = pd.read_csv(path)\n",
    "            if depth_col in loaded_data.columns:  # Use configurable depth column\n",
    "                data[key] = loaded_data\n",
    "    \n",
    "    if not data:\n",
    "        raise ValueError(\"No valid data files found to plot\")\n",
    "    \n",
    "    # Calculate number of plots based on available data\n",
    "    n_plots = 0\n",
    "    \n",
    "    if ct_img is not None and 'ct' in data:\n",
    "        n_plots += 2\n",
    "    if rgb_img is not None and 'rgb' in data:\n",
    "        n_plots += 2  # RGB Image + RGB Channels (with luminance overlay) = 2 plots\n",
    "        \n",
    "    # MS panel - check using configurable column names\n",
    "    has_ms = False\n",
    "    if 'mst' in data and 'mst' in available_columns:\n",
    "        ms_config = available_columns['mst']['ms']\n",
    "        ms_col = ms_config['data_col']\n",
    "        if ms_col in data['mst'].columns and not data['mst'][ms_col].isna().all():\n",
    "            has_ms = True\n",
    "    if 'hrms' in data and 'hrms' in available_columns:\n",
    "        hrms_config = available_columns['hrms']\n",
    "        hrms_col = hrms_config['data_col']\n",
    "        if not data['hrms'][hrms_col].isna().all():\n",
    "            has_ms = True\n",
    "    if has_ms:\n",
    "        n_plots += 1\n",
    "        \n",
    "    # Other MST logs - count using EXACT same logic as plotting\n",
    "    if 'mst' in data and 'mst' in available_columns:\n",
    "        mst_configs = available_columns['mst']\n",
    "        for log_type, config in mst_configs.items():\n",
    "            if (log_type != 'ms' and \n",
    "                'data_col' in config and \n",
    "                config['data_col'] in data['mst'].columns and \n",
    "                not data['mst'][config['data_col']].isna().all()):\n",
    "                n_plots += 1\n",
    "\n",
    "    if n_plots == 0:\n",
    "        raise ValueError(\"No data available to plot\")\n",
    "\n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(1.2*n_plots, 12))\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    fig.suptitle(title, fontweight='bold', fontsize=14)\n",
    "\n",
    "    current_ax = 0\n",
    "\n",
    "    # Plot CT image and data\n",
    "    if ct_img is not None and 'ct' in data:\n",
    "        # CT Image\n",
    "        axes[current_ax].imshow(ct_img, aspect='auto', extent=[0, 1, core_length, 0], cmap='gray')\n",
    "        axes[current_ax].set_xticks([])\n",
    "        axes[current_ax].set_xlabel('Sediment\\nCore\\nCT Scan', fontweight='bold', fontsize='small')\n",
    "        # Only set y-label for the leftmost subplot\n",
    "        if current_ax == 0:\n",
    "            axes[current_ax].set_ylabel('Depth (cm)', fontweight='bold')\n",
    "        current_ax += 1\n",
    "\n",
    "        # CT Data - use configurable column names\n",
    "        ct_config = available_columns['ct']\n",
    "        ct_col = ct_config['data_col']\n",
    "        ct_std_col = ct_config['std_col']\n",
    "        ct_depth = data['ct'][depth_col].astype(np.float64)\n",
    "        \n",
    "        axes[current_ax].plot(data['ct'][ct_col].astype(np.float64), ct_depth, \n",
    "                             color='black', linewidth=0.7)\n",
    "        \n",
    "        # Standard deviation fill\n",
    "        if ct_std_col in data['ct'].columns:\n",
    "            axes[current_ax].fill_betweenx(\n",
    "                ct_depth,\n",
    "                data['ct'][ct_col].astype(np.float64) - data['ct'][ct_std_col].astype(np.float64),\n",
    "                data['ct'][ct_col].astype(np.float64) + data['ct'][ct_std_col].astype(np.float64),\n",
    "                color='black', alpha=0.2, linewidth=0\n",
    "            )\n",
    "        \n",
    "        # Color-coded CT values using PolyCollection\n",
    "        ct_values = data['ct'][ct_col].astype(np.float64).values\n",
    "        depths = ct_depth.values\n",
    "        norm = plt.Normalize(300, 1600)\n",
    "        cmap = plt.cm.jet\n",
    "        \n",
    "        ct_polys = []\n",
    "        ct_facecolors = []\n",
    "        for i in range(len(depths) - 1):\n",
    "            # Ignore segments with NaN values\n",
    "            if not (np.isnan(ct_values[i]) or np.isnan(ct_values[i+1])):\n",
    "                poly = [\n",
    "                    (0, depths[i]),\n",
    "                    (ct_values[i], depths[i]),\n",
    "                    (ct_values[i+1], depths[i+1]),\n",
    "                    (0, depths[i+1])\n",
    "                ]\n",
    "                ct_polys.append(poly)\n",
    "                # Use the average value for smoother color transition\n",
    "                avg_val = (ct_values[i] + ct_values[i+1]) / 2\n",
    "                ct_facecolors.append(cmap(norm(avg_val)))\n",
    "                \n",
    "        if ct_polys:\n",
    "            pc_ct = mcoll.PolyCollection(ct_polys, facecolors=ct_facecolors, edgecolors='none', alpha=0.95)\n",
    "            axes[current_ax].add_collection(pc_ct)\n",
    "        \n",
    "        axes[current_ax].set_xlabel('CT\\nBrightness', fontweight='bold', fontsize='small')\n",
    "        axes[current_ax].grid(True)\n",
    "        axes[current_ax].set_xlim(300, None)\n",
    "        axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "        current_ax += 1\n",
    "\n",
    "    # Plot RGB image and data\n",
    "    if rgb_img is not None and 'rgb' in data:\n",
    "        # RGB Image\n",
    "        axes[current_ax].imshow(rgb_img, aspect='auto', extent=[0, 0.5, core_length, 0])\n",
    "        axes[current_ax].set_xticks([])\n",
    "        axes[current_ax].set_xlabel('Sediment\\nCore\\nPhoto', fontweight='bold', fontsize='small')\n",
    "        current_ax += 1\n",
    "        \n",
    "        # RGB data (R, G, B channels)\n",
    "        rgb_config = available_columns['rgb']\n",
    "        rgb_cols = rgb_config['data_cols']\n",
    "        rgb_stds = rgb_config['std_cols']\n",
    "        rgb_depth = data['rgb'][depth_col].astype(np.float64)\n",
    "        colors = ['red', 'green', 'blue']\n",
    "        \n",
    "        for col, std, color in zip(rgb_cols[:3], rgb_stds[:3], colors):\n",
    "            if col in data['rgb'].columns:\n",
    "                axes[current_ax].plot(data['rgb'][col].astype(np.float64), rgb_depth,\n",
    "                                     color=color, linewidth=0.7)\n",
    "                if std in data['rgb'].columns:\n",
    "                    axes[current_ax].fill_betweenx(\n",
    "                        rgb_depth,\n",
    "                        data['rgb'][col].astype(np.float64) - data['rgb'][std].astype(np.float64),\n",
    "                        data['rgb'][col].astype(np.float64) + data['rgb'][std].astype(np.float64),\n",
    "                        color=color, alpha=0.2, linewidth=0\n",
    "                    )\n",
    "        \n",
    "        # Luminance plot using PolyCollection with Inferno colormap (ON SAME SUBPLOT)\n",
    "        if 'Lumin' in data['rgb'].columns:\n",
    "            lumin_values = data['rgb']['Lumin'].astype(np.float64).values\n",
    "            lumin_depths = rgb_depth.values\n",
    "            \n",
    "            # Compute normalization range ignoring NaNs\n",
    "            valid_lumin = lumin_values[~np.isnan(lumin_values)]\n",
    "            if len(valid_lumin) > 0:\n",
    "                vmin, vmax = valid_lumin.min(), valid_lumin.max()\n",
    "                if not np.isclose(vmin, vmax):\n",
    "                    lumin_norm = plt.Normalize(vmin, vmax)\n",
    "                    cmap_inferno = plt.cm.inferno\n",
    "                    \n",
    "                    lumin_polys = []\n",
    "                    lumin_facecolors = []\n",
    "                    for i in range(len(lumin_depths) - 1):\n",
    "                        if not (np.isnan(lumin_values[i]) or np.isnan(lumin_values[i+1])):\n",
    "                            poly = [\n",
    "                                (0, lumin_depths[i]),\n",
    "                                (lumin_values[i], lumin_depths[i]),\n",
    "                                (lumin_values[i+1], lumin_depths[i+1]),\n",
    "                                (0, lumin_depths[i+1])\n",
    "                            ]\n",
    "                            lumin_polys.append(poly)\n",
    "                            avg_lumin = (lumin_values[i] + lumin_values[i+1]) / 2\n",
    "                            lumin_facecolors.append(cmap_inferno(lumin_norm(avg_lumin)))\n",
    "                    \n",
    "                    if lumin_polys:\n",
    "                        pc_lumin = mcoll.PolyCollection(lumin_polys, facecolors=lumin_facecolors, edgecolors='none', alpha=0.95)\n",
    "                        axes[current_ax].add_collection(pc_lumin)\n",
    "        \n",
    "        axes[current_ax].set_xlabel('RGB\\nChannels', fontweight='bold', fontsize='small')\n",
    "        axes[current_ax].grid(True)\n",
    "        axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "        current_ax += 1\n",
    "\n",
    "    # Plot MS data using configurable column names\n",
    "    if has_ms:\n",
    "        # Plot MST MS data if available\n",
    "        if 'mst' in data and 'mst' in available_columns:\n",
    "            ms_config = available_columns['mst']['ms']\n",
    "            ms_col = ms_config['data_col']\n",
    "            if ms_col in data['mst'].columns and not data['mst'][ms_col].isna().all():\n",
    "                axes[current_ax].plot(data['mst'][ms_col].astype(np.float64), \n",
    "                                     data['mst'][depth_col].astype(np.float64), \n",
    "                                     color='green', linewidth=0.7)\n",
    "\n",
    "        # Plot HRMS data if available\n",
    "        if 'hrms' in data and 'hrms' in available_columns:\n",
    "            hrms_config = available_columns['hrms']\n",
    "            hrms_col = hrms_config['data_col']\n",
    "            if not data['hrms'][hrms_col].isna().all():\n",
    "                axes[current_ax].plot(data['hrms'][hrms_col].astype(np.float64), \n",
    "                                     data['hrms'][depth_col].astype(np.float64), \n",
    "                                     color='red', linewidth=0.7)\n",
    "\n",
    "        axes[current_ax].set_xlabel('Magnetic\\nSusceptibility\\n(Î¼SI)', fontweight='bold', fontsize='small')\n",
    "        axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "        axes[current_ax].grid(True)\n",
    "        current_ax += 1\n",
    "\n",
    "    # Plot other MST logs if available - use configurable labels and colors\n",
    "    if 'mst' in data and 'mst' in available_columns:\n",
    "        mst_configs = available_columns['mst']\n",
    "        \n",
    "        for log_type, config in mst_configs.items():\n",
    "            if log_type != 'ms' and config['data_col'] in data['mst'].columns and not data['mst'][config['data_col']].isna().all():\n",
    "                data_col = config['data_col']\n",
    "                plot_label = config.get('plot_label', data_col)  # Use configured label or fallback to column name\n",
    "                plot_color = config.get('plot_color', 'black')   # Use configured color or fallback to black\n",
    "                \n",
    "                axes[current_ax].plot(\n",
    "                    data['mst'][data_col].astype(np.float64), \n",
    "                    data['mst'][depth_col].astype(np.float64), \n",
    "                    color=plot_color, \n",
    "                    linewidth=0.7\n",
    "                )\n",
    "                axes[current_ax].set_xlabel(plot_label, fontweight='bold', fontsize='small')\n",
    "                axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "                axes[current_ax].grid(True)\n",
    "                if log_type == 'density':\n",
    "                    axes[current_ax].set_xlim(1, 2)\n",
    "                current_ax += 1\n",
    "    \n",
    "    # Set common y-axis properties\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_ylim(core_length, 0)\n",
    "        # Hide y-axis tick labels for all columns except the first\n",
    "        if i > 0:\n",
    "            ax.tick_params(axis='y', labelleft=False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions for Machine Learning to fill data gaps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for plotting filled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filled_data(target_log, original_data, filled_data, core_length, core_name, data_config, ML_type='ML'):\n",
    "    \"\"\"\n",
    "    Plot original and ML-filled data for a given log using configurable parameters.\n",
    "    \n",
    "    Args:\n",
    "        target_log (str): Name of the log to plot\n",
    "        original_data (pd.DataFrame): Original data containing the log\n",
    "        filled_data (pd.DataFrame): Data with ML-filled gaps\n",
    "        core_length (int): Length of the core in cm\n",
    "        core_name (str): Name of the core for plot title\n",
    "        data_config (dict): Configuration containing depth column and other parameters\n",
    "        ML_type (str): Type of ML method used for title\n",
    "    \"\"\"\n",
    "    # Get primary depth column from config\n",
    "    depth_col = data_config['depth_column']\n",
    "    \n",
    "    # Check if there are any gaps\n",
    "    has_gaps = original_data[target_log].isna().any()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(15, 3))\n",
    "    title_suffix = f'Use {ML_type} for Data Gap Filling' if has_gaps else \"(No Data Gap to be filled by ML)\"\n",
    "    fig.suptitle(f'{core_name} {target_log} Values {title_suffix}', fontweight='bold')\n",
    "\n",
    "    # Plot data with ML-predicted gaps only if gaps exist\n",
    "    if has_gaps:\n",
    "        ax.plot(filled_data[depth_col], filled_data[target_log], \n",
    "                color='red', label=f'ML Predicted {target_log}', linewidth=0.7, alpha=0.7)\n",
    "\n",
    "    # Plot original data\n",
    "    ax.plot(original_data[depth_col], original_data[target_log], \n",
    "            color='black', label=f'Original {target_log}', linewidth=0.7)\n",
    "\n",
    "    # Add uncertainty shade if std column exists\n",
    "    std_col = f'{target_log}_std'\n",
    "    if std_col in original_data.columns:\n",
    "        ax.fill_between(original_data[depth_col],\n",
    "                       original_data[target_log] - original_data[std_col],\n",
    "                       original_data[target_log] + original_data[std_col],\n",
    "                       color='black', alpha=0.2, linewidth=0)\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_ylabel(f'{target_log}\\nBrightness', fontweight='bold', fontsize='small')\n",
    "    ax.set_xlabel('Depth (cm)')\n",
    "    ax.grid(True)\n",
    "    ax.invert_xaxis()\n",
    "    ax.set_xlim(0, core_length)\n",
    "    ax.tick_params(axis='y', labelsize='x-small')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Machine Learning Data Gap filling\n",
    "\n",
    "#### Helper Functions for fill_gaps_with_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_feature_data(target_log, All_logs, merge_tolerance, data_config):\n",
    "    \"\"\"Prepare merged feature data for ML training using configurable parameters.\"\"\"\n",
    "    # Get primary depth column from config\n",
    "    depth_col = data_config['depth_column']\n",
    "    \n",
    "    # Get target data from All_logs\n",
    "    target_data = None\n",
    "    for df, cols in All_logs.values():\n",
    "        if target_log in cols:\n",
    "            target_data = df.copy()\n",
    "            break\n",
    "    \n",
    "    if target_data is None:\n",
    "        raise ValueError(f\"Target log '{target_log}' not found in any dataset\")\n",
    "\n",
    "    # Convert depth column to float64 in target data\n",
    "    target_data[depth_col] = target_data[depth_col].astype('float64')\n",
    "    \n",
    "    # Prepare training data by merging all available logs\n",
    "    merged_data = target_data[[depth_col, target_log]].copy()\n",
    "    features = []\n",
    "    \n",
    "    # Merge feature dataframes one by one, using their own depth column\n",
    "    for df_name, (df, cols) in All_logs.items():\n",
    "        if target_log not in cols:  # Skip the target dataframe\n",
    "            df = df.copy()\n",
    "            df[depth_col] = df[depth_col].astype('float64')\n",
    "            # Rename depth column temporarily to avoid conflicts during merging\n",
    "            temp_depth_col = f'{depth_col}_{df_name}'\n",
    "            df = df.rename(columns={depth_col: temp_depth_col})\n",
    "            # Convert all numeric columns to float64\n",
    "            for col in cols:\n",
    "                if col != depth_col and df[col].dtype.kind in 'biufc':\n",
    "                    df[col] = df[col].astype('float64')\n",
    "            # Rename feature columns for merging\n",
    "            df_renamed = df.rename(columns={col: f'{df_name}_{col}' for col in cols if col != depth_col})\n",
    "            df_renamed = df_renamed.sort_values(temp_depth_col)\n",
    "            \n",
    "            # Perform merge_asof with tolerance for data alignment\n",
    "            merged_data = pd.merge_asof(\n",
    "                merged_data.sort_values(depth_col),\n",
    "                df_renamed,\n",
    "                left_on=depth_col,\n",
    "                right_on=temp_depth_col,\n",
    "                direction='nearest',\n",
    "                tolerance=merge_tolerance\n",
    "            )\n",
    "            \n",
    "            # Check for unmatched rows due to the tolerance constraint\n",
    "            unmatched = merged_data[temp_depth_col].isna().sum()\n",
    "            if unmatched > 0:\n",
    "                warnings.warn(f\"{unmatched} rows did not have a matching depth within tolerance for log '{df_name}'.\")\n",
    "            \n",
    "            # Add renamed feature columns to features list\n",
    "            features.extend([f'{df_name}_{col}' for col in cols if col != depth_col])\n",
    "            # Drop the temporary depth column used for merging\n",
    "            merged_data = merged_data.drop(columns=[temp_depth_col])\n",
    "    \n",
    "    # Add depth column as a feature\n",
    "    features.append(depth_col)\n",
    "    \n",
    "    return target_data, merged_data, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_weights(X, data_config):\n",
    "    \"\"\"Apply feature weights using configurable parameters.\"\"\"\n",
    "    X_weighted = X.copy()\n",
    "    column_configs = data_config['column_configs']\n",
    "    \n",
    "    # Apply RGB weights (list corresponding to data_cols)\n",
    "    if 'rgb' in column_configs and 'feature_weights' in column_configs['rgb']:\n",
    "        rgb_config = column_configs['rgb']\n",
    "        rgb_cols = rgb_config['data_cols']\n",
    "        rgb_weights = rgb_config['feature_weights']\n",
    "        \n",
    "        # Apply weights to each RGB column\n",
    "        for col, weight in zip(rgb_cols, rgb_weights):\n",
    "            matching_cols = [x_col for x_col in X_weighted.columns if col in x_col]\n",
    "            for x_col in matching_cols:\n",
    "                X_weighted[x_col] = (X_weighted[x_col] * weight).astype('float32')\n",
    "    \n",
    "    # Apply MST weights (individual feature_weight for each log type)\n",
    "    if 'mst' in column_configs:\n",
    "        mst_configs = column_configs['mst']\n",
    "        for log_type, config in mst_configs.items():\n",
    "            if 'feature_weight' in config:\n",
    "                data_col = config['data_col']\n",
    "                weight = config['feature_weight']\n",
    "                \n",
    "                # Find matching columns in X that contain this data column name\n",
    "                matching_cols = [x_col for x_col in X_weighted.columns if data_col in x_col]\n",
    "                for x_col in matching_cols:\n",
    "                    X_weighted[x_col] = (X_weighted[x_col] * weight).astype('float32')\n",
    "    \n",
    "    # Apply HRMS weight (single feature_weight)\n",
    "    if 'hrms' in column_configs and 'feature_weight' in column_configs['hrms']:\n",
    "        hrms_config = column_configs['hrms']\n",
    "        hrms_col = hrms_config['data_col']\n",
    "        hrms_weight = hrms_config['feature_weight']\n",
    "        \n",
    "        # Find matching columns in X that contain the HRMS column name\n",
    "        matching_cols = [x_col for x_col in X_weighted.columns if hrms_col in x_col]\n",
    "        for x_col in matching_cols:\n",
    "            X_weighted[x_col] = (X_weighted[x_col] * hrms_weight).astype('float32')\n",
    "    \n",
    "    return X_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_gap_predictions(df, gap_mask, ml_preds, target_log, data_config):\n",
    "    \"\"\"\n",
    "    Adjust ML predictions for gap rows in 'df' so that for each contiguous gap\n",
    "    segment (with both left and right boundaries available) the predictions are\n",
    "    blended with the linear interpolation between the boundary values.\n",
    "    \"\"\"\n",
    "    # Get primary depth column from config\n",
    "    depth_col = data_config['depth_column']\n",
    "    \n",
    "    # Get the integer positions (row numbers) of missing values\n",
    "    gap_positions = np.where(gap_mask.values)[0]\n",
    "    # Create a Series for easier handling; index = positions in df\n",
    "    preds_series = pd.Series(ml_preds, index=gap_positions)\n",
    "    \n",
    "    # Identify contiguous segments in the gap positions\n",
    "    segments = np.split(gap_positions, np.where(np.diff(gap_positions) != 1)[0] + 1)\n",
    "    \n",
    "    adjusted = preds_series.copy()\n",
    "    for seg in segments:\n",
    "        # seg is an array of row positions (in df) for a contiguous gap segment.\n",
    "        start_pos = seg[0]\n",
    "        end_pos = seg[-1]\n",
    "        \n",
    "        # Enforce trend constraints only if both boundaries exist.\n",
    "        if start_pos == 0 or end_pos == len(df) - 1:\n",
    "            continue  # Skip segments at the very beginning or end.\n",
    "        \n",
    "        # Retrieve boundary (observed) values and depths using configurable depth column\n",
    "        left_value = df.iloc[start_pos - 1][target_log]\n",
    "        right_value = df.iloc[end_pos + 1][target_log]\n",
    "        # Skip if boundaries are missing (should not happen if gap_mask is correct)\n",
    "        if pd.isna(left_value) or pd.isna(right_value):\n",
    "            continue\n",
    "        left_depth = df.iloc[start_pos - 1][depth_col]\n",
    "        right_depth = df.iloc[end_pos + 1][depth_col]\n",
    "        \n",
    "        # For each gap row in the segment, blend the ML prediction with linear interpolation\n",
    "        for pos in seg:\n",
    "            current_depth = df.iloc[pos][depth_col]\n",
    "            # Normalize the depth position (x in [0, 1])\n",
    "            if right_depth == left_depth:\n",
    "                x = 0.5\n",
    "            else:\n",
    "                x = (current_depth - left_depth) / (right_depth - left_depth)\n",
    "            # Compute the linear interpolation value at this depth\n",
    "            interp_val = left_value + (right_value - left_value) * x\n",
    "            # Define a weight that is 0 at the boundaries and 1 at the middle.\n",
    "            # Here we use: weight = 1 - 2*|x - 0.5|\n",
    "            weight = 1 - 2 * abs(x - 0.5)\n",
    "            weight = max(0, min(weight, 1))  # Ensure weight is between 0 and 1\n",
    "            # Blend: final = interpolation + weight*(ML_prediction - interpolation)\n",
    "            adjusted[pos] = interp_val + weight * (preds_series[pos] - interp_val)\n",
    "    \n",
    "    return adjusted.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    \"\"\"Helper function for parallel model training.\"\"\"\n",
    "    def train_wrapper(X_train, y_train, X_pred):\n",
    "        model.fit(X_train, y_train)\n",
    "        return model.predict(X_pred)\n",
    "    return train_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps_with_ml(target_log, All_logs, output_csv=True, output_dir=None, core_name=None, \n",
    "                      merge_tolerance=3.0, ml_method='xgblgbm', data_config=None):\n",
    "    \"\"\"\n",
    "    Fill gaps in target data using specified ML method.\n",
    "    \n",
    "    Args:\n",
    "        target_log (str): Name of the target column to fill gaps in.\n",
    "        All_logs (dict): Dictionary of dataframes containing feature data and target data.\n",
    "        output_csv (bool): Whether to output filled data to CSV file.\n",
    "        output_dir (str): Directory to save output CSV file.\n",
    "        core_name (str): Name of the core for CSV filename.\n",
    "        merge_tolerance (float): Maximum allowed difference in depth for merging rows.\n",
    "        ml_method (str): ML method to use - 'rf', 'rftc', 'xgb', 'xgblgbm' (default)\n",
    "        data_config (dict): Configuration containing depth column and other parameters.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (target_data_filled, gap_mask)\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if target_log is None or All_logs is None:\n",
    "        raise ValueError(\"Both target_log and All_logs must be provided\")\n",
    "        \n",
    "    if output_csv and (output_dir is None or core_name is None):\n",
    "        raise ValueError(\"output_dir and core_name must be provided when output_csv is True\")\n",
    "    \n",
    "    if ml_method not in ['rf', 'rftc', 'xgb', 'xgblgbm']:\n",
    "        raise ValueError(\"ml_method must be one of: 'rf', 'rftc', 'xgb', 'xgblgbm'\")\n",
    "    \n",
    "    # Prepare feature data\n",
    "    target_data, merged_data, features = prepare_feature_data(target_log, All_logs, merge_tolerance, data_config)\n",
    "    \n",
    "    # Create a copy of the original data to hold the interpolated results\n",
    "    target_data_filled = target_data.copy()\n",
    "\n",
    "    # Identify gaps in target data\n",
    "    gap_mask = target_data[target_log].isna()\n",
    "    \n",
    "    # If no gaps exist, save to CSV if requested and return original data\n",
    "    if not gap_mask.any():\n",
    "        if output_csv:\n",
    "            output_path = os.path.join(output_dir, f'{core_name}_{target_log.split(\"_\")[0]}_MLfilled.csv')\n",
    "            target_data_filled.to_csv(output_path, index=False)\n",
    "        return target_data_filled, gap_mask\n",
    "\n",
    "    # Prepare features and target for ML\n",
    "    X = merged_data[features].copy()\n",
    "    y = merged_data[target_log].copy()\n",
    "\n",
    "    # Convert all features to float64\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype.kind in 'biufc':\n",
    "            X[col] = X[col].astype('float64')\n",
    "    y = y.astype('float64')\n",
    "\n",
    "    # Split into training (non-gap) and prediction (gap) sets\n",
    "    X_train = X[~gap_mask]\n",
    "    y_train = y[~gap_mask]\n",
    "    X_pred = X[gap_mask]\n",
    "\n",
    "    # Apply specific ML method\n",
    "    if ml_method == 'rf':\n",
    "        predictions = _apply_random_forest(X_train, y_train, X_pred)\n",
    "    elif ml_method == 'rftc':\n",
    "        predictions = _apply_random_forest_with_trend_constraints(X_train, y_train, X_pred, merged_data, gap_mask, target_log, data_config)\n",
    "    elif ml_method == 'xgb':\n",
    "        predictions = _apply_xgboost(X_train, y_train, X_pred, data_config)\n",
    "    elif ml_method == 'xgblgbm':\n",
    "        predictions = _apply_xgboost_lightgbm(X_train, y_train, X_pred, data_config)\n",
    "\n",
    "    # Fill gaps with predictions\n",
    "    target_data_filled.loc[gap_mask, target_log] = predictions\n",
    "    \n",
    "    # Save to CSV if requested\n",
    "    if output_csv:\n",
    "        output_path = os.path.join(output_dir, f\"{core_name}_{target_log.split('_')[0]}_MLfilled.csv\")\n",
    "        target_data_filled.to_csv(output_path, index=False)\n",
    "\n",
    "    return target_data_filled, gap_mask\n",
    "\n",
    "\n",
    "def _apply_random_forest(X_train, y_train, X_pred):\n",
    "    \"\"\"Apply Random Forest method.\"\"\"\n",
    "    # Handle outliers using IQR method\n",
    "    quantile_cutoff = 0.025\n",
    "    Q1 = y_train.quantile(quantile_cutoff)\n",
    "    Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "    X_train = X_train[outlier_mask]\n",
    "    y_train = y_train[outlier_mask]\n",
    "\n",
    "    def train_model(model):\n",
    "        model.fit(X_train, y_train)\n",
    "        return model.predict(X_pred)\n",
    "\n",
    "    # Initialize two ensemble models\n",
    "    models = [\n",
    "        RandomForestRegressor(n_estimators=1000,\n",
    "                              max_depth=30,\n",
    "                              min_samples_split=5,\n",
    "                              min_samples_leaf=5,\n",
    "                              max_features='sqrt',\n",
    "                              bootstrap=True,\n",
    "                              random_state=42,\n",
    "                              n_jobs=-1),\n",
    "        HistGradientBoostingRegressor(max_iter=800,\n",
    "                                      learning_rate=0.05,\n",
    "                                      max_depth=5,\n",
    "                                      min_samples_leaf=50,\n",
    "                                      l2_regularization=1.0,\n",
    "                                      random_state=42,\n",
    "                                      verbose=0)\n",
    "    ]\n",
    "\n",
    "    # Train models in parallel\n",
    "    predictions = Parallel(n_jobs=-1)(delayed(train_model)(model) for model in models)\n",
    "\n",
    "    # Ensemble predictions by averaging\n",
    "    ensemble_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "    return ensemble_predictions\n",
    "\n",
    "\n",
    "def _apply_random_forest_with_trend_constraints(X_train, y_train, X_pred, merged_data, gap_mask, target_log):\n",
    "    \"\"\"Apply Random Forest with trend constraints method.\"\"\"\n",
    "    # Handle outliers using IQR method\n",
    "    quantile_cutoff = 0.15\n",
    "    Q1 = y_train.quantile(quantile_cutoff)\n",
    "    Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "    X_train = X_train[outlier_mask]\n",
    "    y_train = y_train[outlier_mask]\n",
    "    \n",
    "    def train_model(model):\n",
    "        model.fit(X_train, y_train)\n",
    "        return model.predict(X_pred)\n",
    "    \n",
    "    # Initialize two ensemble models\n",
    "    models = [\n",
    "        RandomForestRegressor(n_estimators=1000,\n",
    "                              max_depth=30,\n",
    "                              min_samples_split=5,\n",
    "                              min_samples_leaf=5,\n",
    "                              max_features='sqrt',\n",
    "                              bootstrap=True,\n",
    "                              random_state=42,\n",
    "                              n_jobs=-1),\n",
    "        HistGradientBoostingRegressor(max_iter=800,\n",
    "                                      learning_rate=0.05,\n",
    "                                      max_depth=5,\n",
    "                                      min_samples_leaf=50,\n",
    "                                      l2_regularization=1.0,\n",
    "                                      random_state=42,\n",
    "                                      verbose=-1)\n",
    "    ]\n",
    "    \n",
    "    # Train models in parallel and average their predictions\n",
    "    predictions = Parallel(n_jobs=-1)(delayed(train_model)(model) for model in models)\n",
    "    ensemble_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Apply trend constraints using the helper function from original\n",
    "    adjusted_predictions = _adjust_gap_predictions(merged_data, gap_mask, ensemble_predictions, target_log, data_config)\n",
    "    \n",
    "    return adjusted_predictions\n",
    "\n",
    "\n",
    "def _adjust_gap_predictions(df, gap_mask, ml_preds, target_log, data_config):\n",
    "    \"\"\"\n",
    "    Adjust ML predictions for gap rows in 'df' so that for each contiguous gap\n",
    "    segment (with both left and right boundaries available) the predictions are\n",
    "    blended with the linear interpolation between the boundary values.\n",
    "    \"\"\"\n",
    "    # Get primary depth column from config\n",
    "    depth_col = data_config['depth_column']\n",
    "    \n",
    "    # Get the integer positions (row numbers) of missing values\n",
    "    gap_positions = np.where(gap_mask.values)[0]\n",
    "    # Create a Series for easier handling; index = positions in df\n",
    "    preds_series = pd.Series(ml_preds, index=gap_positions)\n",
    "    \n",
    "    # Identify contiguous segments in the gap positions\n",
    "    segments = np.split(gap_positions, np.where(np.diff(gap_positions) != 1)[0] + 1)\n",
    "    \n",
    "    adjusted = preds_series.copy()\n",
    "    for seg in segments:\n",
    "        # seg is an array of row positions (in df) for a contiguous gap segment.\n",
    "        start_pos = seg[0]\n",
    "        end_pos = seg[-1]\n",
    "        \n",
    "        # Enforce trend constraints only if both boundaries exist.\n",
    "        if start_pos == 0 or end_pos == len(df) - 1:\n",
    "            continue  # Skip segments at the very beginning or end.\n",
    "        \n",
    "        # Retrieve boundary (observed) values and depths using configurable depth column\n",
    "        left_value = df.iloc[start_pos - 1][target_log]\n",
    "        right_value = df.iloc[end_pos + 1][target_log]\n",
    "        # Skip if boundaries are missing (should not happen if gap_mask is correct)\n",
    "        if pd.isna(left_value) or pd.isna(right_value):\n",
    "            continue\n",
    "        left_depth = df.iloc[start_pos - 1][depth_col]\n",
    "        right_depth = df.iloc[end_pos + 1][depth_col]\n",
    "        \n",
    "        # For each gap row in the segment, blend the ML prediction with linear interpolation\n",
    "        for pos in seg:\n",
    "            current_depth = df.iloc[pos][depth_col]\n",
    "            # Normalize the depth position (x in [0, 1])\n",
    "            if right_depth == left_depth:\n",
    "                x = 0.5\n",
    "            else:\n",
    "                x = (current_depth - left_depth) / (right_depth - left_depth)\n",
    "            # Compute the linear interpolation value at this depth\n",
    "            interp_val = left_value + (right_value - left_value) * x\n",
    "            # Define a weight that is 0 at the boundaries and 1 at the middle.\n",
    "            # Here we use: weight = 1 - 2*|x - 0.5|\n",
    "            weight = 1 - 2 * abs(x - 0.5)\n",
    "            weight = max(0, min(weight, 1))  # Ensure weight is between 0 and 1\n",
    "            # Blend: final = interpolation + weight*(ML_prediction - interpolation)\n",
    "            adjusted[pos] = interp_val + weight * (preds_series[pos] - interp_val)\n",
    "    \n",
    "    return adjusted.values\n",
    "\n",
    "\n",
    "def _apply_xgboost(X_train, y_train, X_pred, data_config):\n",
    "    \"\"\"Apply XGBoost method with configurable feature weights.\"\"\"\n",
    "    # Apply feature weights BEFORE processing\n",
    "    X_train_weighted = apply_feature_weights(X_train, data_config)\n",
    "    X_pred_weighted = apply_feature_weights(X_pred, data_config)\n",
    "    \n",
    "    # Handle outliers using IQR method\n",
    "    quantile_cutoff = 0.025\n",
    "    Q1 = y_train.quantile(quantile_cutoff)\n",
    "    Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "    X_train_weighted = X_train_weighted[outlier_mask]\n",
    "    y_train = y_train[outlier_mask]\n",
    "\n",
    "    # Create feature pipeline\n",
    "    feature_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=True)),\n",
    "        ('selector', SelectKBest(score_func=f_regression, k='all'))\n",
    "    ])\n",
    "\n",
    "    # Process features\n",
    "    X_train_processed = feature_pipeline.fit_transform(X_train_weighted, y_train)\n",
    "    X_pred_processed = feature_pipeline.transform(X_pred_weighted)\n",
    "\n",
    "    # Convert processed arrays to float32\n",
    "    X_train_processed = X_train_processed.astype('float32')\n",
    "    X_pred_processed = X_pred_processed.astype('float32')\n",
    "    y_train = y_train.astype('float32')\n",
    "\n",
    "    # Initialize and train XGBoost model\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.003,\n",
    "        max_depth=10,\n",
    "        min_child_weight=5,\n",
    "        subsample=0.75,\n",
    "        colsample_bytree=0.75,\n",
    "        gamma=0.2,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=3.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_processed, y_train)\n",
    "    predictions = model.predict(X_pred_processed).astype('float32')\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def _apply_xgboost_lightgbm(X_train, y_train, X_pred, data_config):\n",
    "    \"\"\"Apply XGBoost + LightGBM ensemble method with configurable feature weights.\"\"\"\n",
    "    # Apply feature weights BEFORE processing\n",
    "    X_train_weighted = apply_feature_weights(X_train, data_config)\n",
    "    X_pred_weighted = apply_feature_weights(X_pred, data_config)\n",
    "    \n",
    "    # Create feature pipeline\n",
    "    feature_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=True))\n",
    "    ])\n",
    "\n",
    "    # Process features without selector first to get actual feature count\n",
    "    X_train_processed = feature_pipeline.fit_transform(X_train_weighted, y_train)\n",
    "    \n",
    "    # Now add selector with correct feature count\n",
    "    max_features = min(50, X_train.shape[0]//10, X_train_processed.shape[1])\n",
    "    selector = SelectKBest(score_func=f_regression, k=max_features)\n",
    "    X_train_processed = selector.fit_transform(X_train_processed, y_train)\n",
    "    X_pred_processed = feature_pipeline.transform(X_pred_weighted)\n",
    "    X_pred_processed = selector.transform(X_pred_processed)\n",
    "\n",
    "    # Convert processed arrays to float32\n",
    "    X_train_processed = X_train_processed.astype('float32')\n",
    "    X_pred_processed = X_pred_processed.astype('float32')\n",
    "    y_train = y_train.astype('float32')\n",
    "\n",
    "    # Initialize models\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=3000,\n",
    "        learning_rate=0.003,\n",
    "        max_depth=10,\n",
    "        min_child_weight=5,\n",
    "        subsample=0.75,\n",
    "        colsample_bytree=0.75,\n",
    "        gamma=0.2,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=3.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=3000,\n",
    "        learning_rate=0.003,\n",
    "        max_depth=6,\n",
    "        num_leaves=20,\n",
    "        min_child_samples=50,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=3.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        force_col_wise=True,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    # Train both models with warnings suppressed\n",
    "    import warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        xgb_model.fit(X_train_processed, y_train)\n",
    "        lgb_model.fit(X_train_processed, y_train, feature_name='auto')\n",
    "\n",
    "    # Make predictions with both models\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        xgb_predictions = xgb_model.predict(X_pred_processed).astype('float32')\n",
    "        lgb_predictions = lgb_model.predict(X_pred_processed).astype('float32')\n",
    "\n",
    "    # Ensemble predictions (simple average)\n",
    "    predictions = (xgb_predictions + lgb_predictions) / 2\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to process and fill logs with chosen ML methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_fill_logs(data_config, ml_method='xgblgbm'):\n",
    "    \"\"\"\n",
    "    Process and fill gaps in log data using ML methods with configurable parameters.\n",
    "    \"\"\"\n",
    "    # Get configurable parameters\n",
    "    depth_col = data_config['depth_column']\n",
    "    mother_dir = data_config['mother_dir']\n",
    "    core_name = data_config['core_name']\n",
    "    core_length = data_config['core_length']\n",
    "    clean_output_folder = data_config['clean_output_folder']\n",
    "    filled_output_folder = data_config['filled_output_folder']\n",
    "    \n",
    "    os.makedirs(mother_dir + filled_output_folder, exist_ok=True)\n",
    "    \n",
    "    clean_paths = data_config.get('clean_file_paths', {})\n",
    "    available_columns = data_config.get('column_configs', {})\n",
    "    valid_data_types = set(clean_paths.keys()) & set(available_columns.keys())\n",
    "    \n",
    "    # Load data with correct path construction\n",
    "    data_dict = {}\n",
    "    for data_type in valid_data_types:\n",
    "        full_path = mother_dir + clean_output_folder + clean_paths[data_type]\n",
    "        if os.path.exists(full_path):\n",
    "            data = pd.read_csv(full_path)\n",
    "            if not data.empty:\n",
    "                data_dict[data_type] = data\n",
    "\n",
    "    if not data_dict:\n",
    "        print(\"No valid data files found for processing\")\n",
    "        return\n",
    "\n",
    "    # Create feature data dictionary using configurable column names\n",
    "    feature_data = {}\n",
    "    \n",
    "    if 'ct' in data_dict and 'ct' in available_columns:\n",
    "        ct_col = available_columns['ct']['data_col']\n",
    "        if ct_col in data_dict['ct'].columns:\n",
    "            feature_data['ct'] = (data_dict['ct'], [depth_col, ct_col])\n",
    "    \n",
    "    if 'rgb' in data_dict and 'rgb' in available_columns:\n",
    "        valid_rgb_cols = [depth_col] + [col for col in available_columns['rgb']['data_cols'] \n",
    "                                       if col in data_dict['rgb'].columns]\n",
    "        if len(valid_rgb_cols) > 1:\n",
    "            feature_data['rgb'] = (data_dict['rgb'], valid_rgb_cols)\n",
    "    \n",
    "    if 'mst' in data_dict and 'mst' in available_columns:\n",
    "        mst_cols = [depth_col]\n",
    "        for log_type, config in available_columns['mst'].items():\n",
    "            if config['data_col'] in data_dict['mst'].columns:\n",
    "                mst_cols.append(config['data_col'])\n",
    "        if len(mst_cols) > 1:\n",
    "            feature_data['mst'] = (data_dict['mst'], mst_cols)\n",
    "    \n",
    "    if 'hrms' in data_dict and 'hrms' in available_columns:\n",
    "        hrms_col = available_columns['hrms']['data_col']\n",
    "        if hrms_col in data_dict['hrms'].columns:\n",
    "            feature_data['hrms'] = (data_dict['hrms'], [depth_col, hrms_col])\n",
    "\n",
    "    if not feature_data:\n",
    "        print(\"No valid feature data found for ML processing\")\n",
    "        return\n",
    "\n",
    "    # ML method names for plotting\n",
    "    ml_names = {\n",
    "        'rf': 'Random Forest', \n",
    "        'rftc': 'Random Forest with Trend Constraints',\n",
    "        'xgb': 'XGBoost', \n",
    "        'xgblgbm': 'XGBoost + LightGBM'\n",
    "    }\n",
    "\n",
    "    # Process each target log\n",
    "    target_logs = []\n",
    "    \n",
    "    # Add RGB targets\n",
    "    if 'rgb' in feature_data:\n",
    "        rgb_cols = available_columns['rgb']['data_cols']\n",
    "        for col in rgb_cols:\n",
    "            if col in data_dict['rgb'].columns:\n",
    "                target_logs.append((col, 'rgb'))\n",
    "    \n",
    "    # Add CT target\n",
    "    if 'ct' in feature_data:\n",
    "        ct_col = available_columns['ct']['data_col']\n",
    "        target_logs.append((ct_col, 'ct'))\n",
    "    \n",
    "    # Add MST targets\n",
    "    if 'mst' in feature_data:\n",
    "        mst_configs = available_columns['mst']\n",
    "        for log_type, config in mst_configs.items():\n",
    "            if 'data_col' in config:\n",
    "                mst_col = config['data_col']\n",
    "                if mst_col in data_dict['mst'].columns:\n",
    "                    target_logs.append((mst_col, 'mst'))\n",
    "    \n",
    "    # Add HRMS target\n",
    "    if 'hrms' in feature_data:\n",
    "        hrms_col = available_columns['hrms']['data_col']\n",
    "        target_logs.append((hrms_col, 'hrms'))\n",
    "\n",
    "    # Process each target log\n",
    "    mst_filled_results = {}  # Store MST results without saving individual files\n",
    "    ct_processed = False  # Track if CT was processed\n",
    "    \n",
    "    for target_log, data_type in target_logs:\n",
    "        print(f\"Processing {target_log}...\")\n",
    "        \n",
    "        # Get source data\n",
    "        data = data_dict[data_type]\n",
    "        plot_name = target_log\n",
    "        \n",
    "        # Create filtered feature data based on target log type\n",
    "        if target_log in available_columns['rgb']['data_cols']:\n",
    "            # For RGB targets, use only density from MST if available\n",
    "            filtered_features = {k: v for k, v in feature_data.items() if k != 'rgb'}\n",
    "            filtered_features['rgb'] = (data, [depth_col, target_log])\n",
    "            \n",
    "            # Add only density from MST if available\n",
    "            if 'mst' in feature_data:\n",
    "                df, cols = feature_data['mst']\n",
    "                density_col = available_columns['mst']['density']['data_col']\n",
    "                if density_col in cols and density_col in df.columns:\n",
    "                    filtered_features['mst'] = (df, [depth_col, density_col])\n",
    "                    \n",
    "            filled_data, gap_mask = fill_gaps_with_ml(\n",
    "                target_log=target_log,\n",
    "                All_logs=filtered_features,\n",
    "                output_csv=True,\n",
    "                output_dir=mother_dir + filled_output_folder,\n",
    "                core_name=core_name,\n",
    "                ml_method=ml_method,\n",
    "                data_config=data_config\n",
    "            )\n",
    "            plot_filled_data(plot_name, data, filled_data, core_length, core_name, data_config, ML_type=ml_names[ml_method])\n",
    "        elif data_type == 'mst':\n",
    "            # For MST targets, don't create individual files - store results for consolidation\n",
    "            filled_data, gap_mask = fill_gaps_with_ml(\n",
    "                target_log=target_log,\n",
    "                All_logs=feature_data,\n",
    "                output_csv=False,  # Don't create individual files for MST\n",
    "                output_dir=None,\n",
    "                core_name=None,\n",
    "                ml_method=ml_method,\n",
    "                data_config=data_config\n",
    "            )\n",
    "            # Store the filled results for this MST column\n",
    "            mst_filled_results[target_log] = filled_data[target_log]\n",
    "            # Plot filled data for each MST column\n",
    "            plot_filled_data(plot_name, data, filled_data, core_length, core_name, data_config, ML_type=ml_names[ml_method])\n",
    "        else:\n",
    "            filled_data, gap_mask = fill_gaps_with_ml(\n",
    "                target_log=target_log,\n",
    "                All_logs=feature_data,\n",
    "                output_csv=True,\n",
    "                output_dir=mother_dir + filled_output_folder,\n",
    "                core_name=core_name,\n",
    "                ml_method=ml_method,\n",
    "                data_config=data_config\n",
    "            )\n",
    "            # Track CT processing but don't print message yet\n",
    "            if data_type == 'ct':\n",
    "                ct_processed = True\n",
    "            plot_filled_data(plot_name, data, filled_data, core_length, core_name, data_config, ML_type=ml_names[ml_method])\n",
    "\n",
    "    # Create consolidated MST file directly from results\n",
    "    if mst_filled_results and 'mst' in data_dict:\n",
    "        mst_data = data_dict['mst'].copy()\n",
    "        filled_columns = []\n",
    "        for col, filled_values in mst_filled_results.items():\n",
    "            mst_data[col] = filled_values\n",
    "            filled_columns.append(col)\n",
    "        mst_data.to_csv(mother_dir + filled_output_folder + f'{core_name}_MST_MLfilled.csv', index=False)\n",
    "        print(f\"Saved [{', '.join(filled_columns)}] to {core_name}_MST_MLfilled.csv\")\n",
    "\n",
    "    # Print CT message if it was processed\n",
    "    if ct_processed:\n",
    "        print(f\"Saved [CT] to {core_name}_CT_MLfilled.csv\")\n",
    "\n",
    "    # Consolidate RGB data\n",
    "    if 'rgb' in data_dict and 'rgb' in available_columns:\n",
    "        rgb_data = data_dict['rgb'].copy()\n",
    "        rgb_columns = available_columns['rgb']['data_cols']\n",
    "        updated_columns = []\n",
    "        \n",
    "        for col in rgb_columns:\n",
    "            if col in rgb_data.columns:\n",
    "                filled_file = mother_dir + filled_output_folder + f'{core_name}_{col}_MLfilled.csv'\n",
    "                if os.path.exists(filled_file):\n",
    "                    filled_data = pd.read_csv(filled_file)\n",
    "                    if col in filled_data.columns:\n",
    "                        rgb_data[col] = filled_data[col]\n",
    "                        updated_columns.append(col)\n",
    "        \n",
    "        if updated_columns:\n",
    "            rgb_data.to_csv(mother_dir + filled_output_folder + f'{core_name}_RGB_MLfilled.csv', index=False)\n",
    "            print(f\"Saved [{', '.join(updated_columns)}] to {core_name}_RGB_MLfilled.csv\")\n",
    "            for col in rgb_columns:\n",
    "                filled_file = mother_dir + filled_output_folder + f'{core_name}_{col}_MLfilled.csv'\n",
    "                if os.path.exists(filled_file):\n",
    "                    os.remove(filled_file)\n",
    "\n",
    "    # Consolidate MST data - already handled above, no individual files to consolidate\n",
    "\n",
    "    # Consolidate HRMS data\n",
    "    if 'hrms' in data_dict and 'hrms' in available_columns:\n",
    "        hrms_data = data_dict['hrms'].copy()\n",
    "        hrms_col = available_columns['hrms']['data_col']\n",
    "        \n",
    "        if hrms_col in hrms_data.columns:\n",
    "            filled_file = mother_dir + filled_output_folder + f'{core_name}_{hrms_col}_MLfilled.csv'\n",
    "            final_file = mother_dir + filled_output_folder + f'{core_name}_hiresMS_MLfilled.csv'\n",
    "            \n",
    "            if os.path.exists(filled_file):\n",
    "                filled_data = pd.read_csv(filled_file)\n",
    "                if hrms_col in filled_data.columns:\n",
    "                    hrms_data[hrms_col] = filled_data[hrms_col]\n",
    "                    hrms_data.to_csv(final_file, index=False)\n",
    "                    print(f\"Saved [{hrms_col}] to {core_name}_hiresMS_MLfilled.csv\")\n",
    "                    # Only delete individual file if it's different from final file\n",
    "                    if filled_file != final_file:\n",
    "                        os.remove(filled_file)\n",
    "\n",
    "    print(\"ML-based gap filling completed for all target logs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### **Define data structure**\n",
    "\n",
    "#### Define core name and core length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_name = \"M9907-11PC\"  # Core name\n",
    "total_length_cm = 439     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-12PC\"  # Core name\n",
    "# total_length_cm = 488     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-14TC\"  # Core name\n",
    "# total_length_cm = 199     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-22PC\"  # Core name\n",
    "# total_length_cm = 501     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-22TC\"  # Core name\n",
    "# total_length_cm = 173     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-23PC\"  # Core name\n",
    "# total_length_cm = 783     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-25PC\"  # Core name\n",
    "# total_length_cm = 797     # Core length in cm\n",
    "\n",
    "# core_name = \"RR0207-56PC\"  # Core name\n",
    "# total_length_cm = 794     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-30PC\"  # Core name\n",
    "# total_length_cm = 781     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-31PC\"  # Core name\n",
    "# total_length_cm = 767     # Core length in cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define file path, data configuration, and outliner cut-off thresholds for ML data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data configuration for ML data imputation\n",
    "# This replaces all hardcoded column names and parameters in the functions\n",
    "\n",
    "data_config = {\n",
    "    # Existing configuration (unchanged)\n",
    "    'mother_dir': '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/',\n",
    "    'core_name': core_name,\n",
    "    'core_length': total_length_cm,\n",
    "    'data_folder': f'_compiled_logs/{core_name}/',\n",
    "    'clean_output_folder': f'_compiled_logs/{core_name}/ML_clean/',\n",
    "    'filled_output_folder': f'_compiled_logs/{core_name}/ML_filled/',\n",
    "    \n",
    "    # Existing file paths (unchanged)\n",
    "    'clean_file_paths': {\n",
    "        'ct': f'{core_name}_CT_clean.csv',\n",
    "        'rgb': f'{core_name}_RGB_clean.csv',\n",
    "        'mst': f'{core_name}_MST_clean.csv',\n",
    "        'hrms': f'{core_name}_hiresMS_clean.csv'\n",
    "    },\n",
    "    \n",
    "    'filled_file_paths': {\n",
    "        'ct': f'{core_name}_CT_MLfilled.csv',\n",
    "        'rgb': f'{core_name}_RGB_MLfilled.csv',\n",
    "        'mst': f'{core_name}_MST_MLfilled.csv',\n",
    "        'hrms': f'{core_name}_hiresMS_MLfilled.csv'\n",
    "    },\n",
    "    \n",
    "    'ct_image_path': f'_compiled_logs/{core_name}/{core_name}_CT.tiff',\n",
    "    'rgb_image_path': f'_compiled_logs/{core_name}/{core_name}_RGB.tiff',\n",
    "    \n",
    "    # Primary depth column name used throughout all functions\n",
    "    'depth_column': 'SB_DEPTH_cm',\n",
    "    \n",
    "    # Enhanced column configs with all information consolidated\n",
    "    'column_configs': {\n",
    "        'ct': {\n",
    "            'data_col': 'CT', \n",
    "            'std_col': 'CT_std', \n",
    "            'depth_col': 'SB_DEPTH_cm'\n",
    "        },\n",
    "        'rgb': {\n",
    "            'data_cols': ['R', 'G', 'B', 'Lumin'],\n",
    "            'std_cols': ['R_std', 'G_std', 'B_std', 'Lumin_std'],\n",
    "            'depth_col': 'SB_DEPTH_cm',\n",
    "            'feature_weights': [0.3, 0.3, 0.3, 0.3]  # corresponds to ['R', 'G', 'B', 'Lumin']\n",
    "        },\n",
    "        'mst': {\n",
    "            'density': {\n",
    "                'data_col': 'Den_gm/cc', \n",
    "                'depth_col': 'SB_DEPTH_cm',\n",
    "                'plot_label': 'Density\\n(g/cc)',\n",
    "                'plot_color': 'orange',\n",
    "                'feature_weight': 0.5,\n",
    "                'threshold': ['<', 1.14, 1]\n",
    "            },\n",
    "            'pwvel': {\n",
    "                'data_col': 'PWVel_m/s', \n",
    "                'depth_col': 'SB_DEPTH_cm',\n",
    "                'plot_label': 'P-wave\\nVelocity\\n(m/s)',\n",
    "                'plot_color': 'purple',\n",
    "                'feature_weight': 0.01,\n",
    "                'threshold': ['>=', 1076, 1]\n",
    "            },\n",
    "            'pwamp': {\n",
    "                'data_col': 'PWAmp', \n",
    "                'depth_col': 'SB_DEPTH_cm',\n",
    "                'plot_label': 'P-wave\\nAmplitude',\n",
    "                'plot_color': 'purple',\n",
    "                'feature_weight': 0.01,\n",
    "                'threshold': ['>=', 30, 1]\n",
    "            },\n",
    "            'elecres': {\n",
    "                'data_col': 'ElecRes_ohmm', \n",
    "                'depth_col': 'SB_DEPTH_cm',\n",
    "                'plot_label': 'Electrical\\nResistivity\\n(ohm-m)',\n",
    "                'plot_color': 'brown',\n",
    "                'threshold': ['<', 0, 1]\n",
    "            },\n",
    "            'ms': {\n",
    "                'data_col': 'MS', \n",
    "                'depth_col': 'SB_DEPTH_cm',\n",
    "                'feature_weight': 0.05,\n",
    "                'threshold': ['>', 180, 1]\n",
    "            }\n",
    "        },\n",
    "        'hrms': {\n",
    "            'data_col': 'hiresMS', \n",
    "            'depth_col': 'SB_DEPTH_cm',\n",
    "            'feature_weight': 3.0,\n",
    "            'threshold': ['<=', 19, 1]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run data cleaning function\n",
    "print(\"Starting data cleaning...\")\n",
    "preprocess_core_data(data_config)\n",
    "\n",
    "# Plot processed logs using new function signature\n",
    "fig, axes = plot_core_logs(\n",
    "    data_config,                           # Data configuration containing all parameters\n",
    "    file_type='clean',                     # Type of data files to plot ('clean' or 'filled')\n",
    "    title=f'{core_name} Cleaned Logs'      # Title for the plot figure\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-based data gap filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_fill_logs(data_config,              # Data configuration containing all parameters\n",
    "                      ml_method='xgblgbm')      # Available ml_method options: 'rf', 'rftc', 'xgb', 'xgblgbm'\n",
    "                                                # - 'rf': Random Forest ML\n",
    "                                                # - 'rftc': Random Forest ML with trend constraints\n",
    "                                                # - 'xgb': XGBoost ML\n",
    "                                                # - 'xgblgbm': XGBoost + LightGBM ML         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ML-based gap-filled log diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ML-based gap-filled log diagram\n",
    "fig, axes = plot_core_logs(\n",
    "    data_config,                                              # Data configuration containing all parameters\n",
    "    file_type='filled',                                       # Type of data files to plot ('filled' for gap-filled data)\n",
    "    title=f'{core_name} XGBoost + LightGBM ML-Filled Logs'    # Title for the plot figure\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
