{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Log Data Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as mcoll  # For PolyCollection\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from scipy.signal import correlate, find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Create interaction features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions for Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for cleaning artifacts and noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_peak_valley_shift(signal_a, signal_b, common_depth):\n",
    "    \"\"\"\n",
    "    Computes a candidate shift based on matching the 2 largest peaks.\n",
    "    \n",
    "    For peaks:\n",
    "      - Finds all local maxima in each signal\n",
    "      - Sorts them by amplitude (largest first), selects the top 2 peaks\n",
    "      - Sorts these peaks by depth and computes the average depth difference\n",
    "    \n",
    "    Args:\n",
    "        signal_a (ndarray): First signal (e.g., HRMS segment)\n",
    "        signal_b (ndarray): Second signal (e.g., density, MS, or CT)\n",
    "        common_depth (ndarray): Depth grid onto which the signals are interpolated\n",
    "        \n",
    "    Returns:\n",
    "        float: The candidate shift (in depth units) based on peak matching\n",
    "    \"\"\"\n",
    "    # Find peaks in each signal\n",
    "    peaks_a, _ = find_peaks(signal_a)\n",
    "    peaks_b, _ = find_peaks(signal_b)\n",
    "    \n",
    "    # If no peaks found, use global maximum\n",
    "    if len(peaks_a) == 0:\n",
    "        peaks_a = np.array([np.argmax(signal_a)])\n",
    "    if len(peaks_b) == 0:\n",
    "        peaks_b = np.array([np.argmax(signal_b)])\n",
    "    \n",
    "    # Sort peaks by amplitude (largest first) and take top 2\n",
    "    sorted_peaks_a = sorted(peaks_a, key=lambda i: signal_a[i], reverse=True)[:2]\n",
    "    sorted_peaks_b = sorted(peaks_b, key=lambda i: signal_b[i], reverse=True)[:2]\n",
    "    \n",
    "    # Sort selected peaks by depth\n",
    "    top_peaks_a = sorted(sorted_peaks_a)\n",
    "    top_peaks_b = sorted(sorted_peaks_b)\n",
    "    \n",
    "    # Calculate peak shifts\n",
    "    n_peaks = min(len(top_peaks_a), len(top_peaks_b))\n",
    "    if n_peaks > 0:\n",
    "        peak_shifts = [common_depth[top_peaks_b[i]] - common_depth[top_peaks_a[i]] \n",
    "                      for i in range(n_peaks)]\n",
    "        candidate_shift = np.mean(peak_shifts)\n",
    "    else:\n",
    "        candidate_shift = 0.0\n",
    "        \n",
    "    return candidate_shift\n",
    "\n",
    "def compute_candidate_shift(signal_a, signal_b, common_depth, \n",
    "                            w_corr=0.2, w_peak=0.7):\n",
    "    \"\"\"\n",
    "    Compute a candidate depth shift between two signals based on:\n",
    "      - Cross-correlation, and\n",
    "      - Matching top 2 peaks\n",
    "    \n",
    "    The final candidate shift is a weighted combination of these two methods.\n",
    "    \n",
    "    Args:\n",
    "        signal_a (ndarray): First signal (e.g., HRMS segment).\n",
    "        signal_b (ndarray): Second signal (e.g., density, MS, or CT).\n",
    "        common_depth (ndarray): Depth grid onto which both signals are interpolated.\n",
    "        w_corr (float): Weight for the cross-correlation candidate.\n",
    "        w_peak (float): Weight for the peak/valley candidate.\n",
    "        \n",
    "    Returns:\n",
    "        float: The weighted candidate shift (in depth units).\n",
    "    \"\"\"\n",
    "    # --- Candidate 1: Cross-correlation shift ---\n",
    "    # Apply smoothing using a Gaussian filter\n",
    "    window = 3  # Window size for smoothing\n",
    "    a_smoothed = gaussian_filter1d(signal_a, sigma=window)\n",
    "    b_smoothed = gaussian_filter1d(signal_b, sigma=window)\n",
    "    \n",
    "    # Detrend the smoothed signals\n",
    "    a_detrended = a_smoothed - np.mean(a_smoothed)\n",
    "    b_detrended = b_smoothed - np.mean(b_smoothed)\n",
    "    \n",
    "    # Calculate cross-correlation\n",
    "    corr = correlate(a_detrended, b_detrended, mode='full')\n",
    "    lags = np.arange(-len(common_depth) + 1, len(common_depth))\n",
    "    best_lag = lags[np.argmax(corr)]\n",
    "    \n",
    "    # Handle case where common_depth is too short\n",
    "    try:\n",
    "        depth_step = common_depth[1] - common_depth[0]\n",
    "        cross_corr_shift = best_lag * depth_step\n",
    "    except IndexError:\n",
    "        cross_corr_shift = 0.0\n",
    "        w_corr = 0.0  # Zero out the weight for cross-correlation\n",
    "\n",
    "    # --- Candidate 2: Peak & Valley shift ---\n",
    "    candidate_peak_valley = compute_peak_valley_shift(signal_a, signal_b, common_depth)\n",
    "\n",
    "    # Return the weighted combination.\n",
    "    return w_corr * cross_corr_shift + w_peak * candidate_peak_valley\n",
    "\n",
    "def preprocess_core_data(data_config, shift_limit_multiplier=3.0):\n",
    "    \"\"\"\n",
    "    Preprocess core data by cleaning and scaling depth values using direct file paths.\n",
    "    \"\"\"\n",
    "    # Validate threshold conditions\n",
    "    valid_conditions = ['>', '<', '<=', '>=']\n",
    "    for param, threshold in data_config.get('thresholds', {}).items():\n",
    "        if threshold[0] not in valid_conditions:\n",
    "            raise ValueError(f\"Invalid condition '{threshold[0]}' for {param}. Must be one of: {valid_conditions}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(data_config['mother_dir'] + data_config['clean_output_folder'], exist_ok=True)\n",
    "    \n",
    "    # Initialize data variables\n",
    "    ct_data = None\n",
    "    rgb_data = None \n",
    "    mst_data = None\n",
    "    hrms_data = None\n",
    "\n",
    "    # Try to read each data file if it exists\n",
    "    ct_path = data_config['mother_dir'] + data_config['data_folder'] + f\"{data_config['core_name']}_CT.csv\"\n",
    "    if os.path.exists(ct_path):\n",
    "        ct_data = pd.read_csv(ct_path).astype('float64')\n",
    "        \n",
    "        # Process CT data\n",
    "        if ct_data is not None and 'ct' in data_config['thresholds']:\n",
    "            condition, threshold_value, buffer_size = data_config['thresholds']['ct']\n",
    "            invalid_ct = eval(f\"ct_data['CT'] {condition} {threshold_value}\")\n",
    "            buffer_indices_ct = []\n",
    "            for i in range(len(ct_data)):\n",
    "                if invalid_ct[i]:\n",
    "                    buffer_indices_ct.extend(range(max(0, i-buffer_size), min(len(ct_data), i+buffer_size+1)))\n",
    "            ct_data.loc[buffer_indices_ct, ['CT', 'CT_std']] = np.nan\n",
    "        \n",
    "        if ct_data is not None:\n",
    "            ct_depth_scale = data_config['core_length'] / ct_data['SB_DEPTH_cm'].max()\n",
    "            ct_data['SB_DEPTH_cm'] = ct_data['SB_DEPTH_cm'] * ct_depth_scale\n",
    "            # Use direct file path\n",
    "            ct_output_path = data_config['mother_dir'] + data_config['clean_file_paths']['ct']\n",
    "            ct_data.to_csv(ct_output_path, index=False)\n",
    "\n",
    "    rgb_path = data_config['mother_dir'] + data_config['data_folder'] + f\"{data_config['core_name']}_RGB.csv\"\n",
    "    if os.path.exists(rgb_path):\n",
    "        rgb_data = pd.read_csv(rgb_path).astype('float64')\n",
    "        \n",
    "        # Process RGB data\n",
    "        if rgb_data is not None:\n",
    "            rgb_columns = ['R', 'G', 'B', 'Lumin']\n",
    "            buffer_indices_rgb = []\n",
    "            \n",
    "            for col in rgb_columns:\n",
    "                if col.lower() in data_config['thresholds']:\n",
    "                    condition, threshold_value, buffer_size = data_config['thresholds'][col.lower()]\n",
    "                    invalid_values = eval(f\"rgb_data['{col}'] {condition} {threshold_value}\")\n",
    "                    for i in range(len(rgb_data)):\n",
    "                        if invalid_values[i]:\n",
    "                            buffer_indices_rgb.extend(range(max(0, i-buffer_size), min(len(rgb_data), i+buffer_size+1)))\n",
    "            \n",
    "            if buffer_indices_rgb:\n",
    "                rgb_data.loc[buffer_indices_rgb, rgb_columns + [f'{col}_std' for col in rgb_columns]] = np.nan\n",
    "            \n",
    "            rgb_depth_scale = data_config['core_length'] / rgb_data['SB_DEPTH_cm'].max()\n",
    "            rgb_data['SB_DEPTH_cm'] = rgb_data['SB_DEPTH_cm'] * rgb_depth_scale\n",
    "            # Use direct file path\n",
    "            rgb_output_path = data_config['mother_dir'] + data_config['clean_file_paths']['rgb']\n",
    "            rgb_data.to_csv(rgb_output_path, index=False)\n",
    "\n",
    "    # Determine subfolder paths based on core name\n",
    "    if data_config['core_name'].startswith('M99'):\n",
    "        mst_subfolder = \"OSU orignal dataset/R-V_Melville99/Calibrated_MST/\"\n",
    "        hrms_subfolder = \"OSU orignal dataset/R-V_Melville99/M9907_point_mag/\"\n",
    "    elif data_config['core_name'].startswith('RR02'):\n",
    "        mst_subfolder = \"OSU orignal dataset/R-V_Revelle02/Calibrated_MST/\"\n",
    "        hrms_subfolder = \"OSU orignal dataset/R-V_Revelle02/RR0207_point_mag/\"\n",
    "    else:\n",
    "        mst_subfolder = \"OSU orignal dataset/R-V_Melville99/Calibrated_MST/\"\n",
    "        hrms_subfolder = \"OSU orignal dataset/R-V_Melville99/M9907_point_mag/\"\n",
    "\n",
    "    mst_path = data_config['mother_dir'] + mst_subfolder + f\"{data_config['core_name']}_MST.csv\"\n",
    "    if os.path.exists(mst_path):\n",
    "        mst_data = pd.read_csv(mst_path).astype('float64')\n",
    "        \n",
    "        # Process MST data\n",
    "        if mst_data is not None:\n",
    "            mst_columns = {\n",
    "                'MS': 'ms',\n",
    "                'PWVel_m/s': 'pwvel',\n",
    "                'PWAmp': 'pwamp',\n",
    "                'Den_gm/cc': 'den', \n",
    "                'ElecRes_ohmm': 'elecres'\n",
    "            }\n",
    "\n",
    "            density_extreme_indices = []\n",
    "            if 'Den_gm/cc' in mst_data.columns and 'den' in data_config['thresholds']:\n",
    "                condition, threshold_value, buffer_size = data_config['thresholds']['den']\n",
    "                density_extreme = eval(f\"mst_data['Den_gm/cc'] {condition} {threshold_value}\")\n",
    "                for i in range(len(mst_data)):\n",
    "                    if density_extreme[i]:\n",
    "                        density_extreme_indices.extend(range(max(0, i-buffer_size), min(len(mst_data), i+buffer_size+1)))\n",
    "\n",
    "            for column, param_key in mst_columns.items():\n",
    "                if column in mst_data.columns and param_key in data_config['thresholds']:\n",
    "                    condition, threshold_value, buffer_size = data_config['thresholds'][param_key]\n",
    "                    extreme_values = eval(f\"mst_data[column] {condition} {threshold_value}\")\n",
    "                    \n",
    "                    buffer_indices = []\n",
    "                    for i in range(len(mst_data)):\n",
    "                        if extreme_values[i]:\n",
    "                            buffer_indices.extend(range(max(0, i-buffer_size), min(len(mst_data), i+buffer_size+1)))\n",
    "                    \n",
    "                    if column == 'MS':\n",
    "                        buffer_indices.extend(density_extreme_indices)\n",
    "                    \n",
    "                    mst_data.loc[buffer_indices, column] = np.nan\n",
    "\n",
    "            if not mst_data.drop('SB_DEPTH_cm', axis=1).isna().all().all():\n",
    "                mst_depth_scale = data_config['core_length'] / mst_data['SB_DEPTH_cm'].max()\n",
    "                mst_data['SB_DEPTH_cm'] = mst_data['SB_DEPTH_cm'] * mst_depth_scale\n",
    "                # Use direct file path\n",
    "                mst_output_path = data_config['mother_dir'] + data_config['clean_file_paths']['mst']\n",
    "                mst_data.to_csv(mst_output_path, index=False)\n",
    "\n",
    "    hrms_path = data_config['mother_dir'] + hrms_subfolder + f\"{data_config['core_name']}_ptMS.csv\"\n",
    "    if os.path.exists(hrms_path):\n",
    "        hrms_data = pd.read_csv(hrms_path).astype('float64')\n",
    "        \n",
    "        # Process Hi-res MS data only if both HRMS data and at least one reference curve exist\n",
    "        if hrms_data is not None and (ct_data is not None or (mst_data is not None and 'Den_gm/cc' in mst_data.columns)):\n",
    "            # [Rest of HRMS processing logic remains the same]\n",
    "            hrms_depth = hrms_data['SB_DEPTH_cm'].values\n",
    "            \n",
    "            # Resample density data if available\n",
    "            density_resampled = None\n",
    "            if mst_data is not None and 'Den_gm/cc' in mst_data.columns:\n",
    "                density_resampled = np.interp(hrms_depth, \n",
    "                                            mst_data['SB_DEPTH_cm'].values,\n",
    "                                            mst_data['Den_gm/cc'].values)\n",
    "            else:\n",
    "                density_resampled = np.full_like(hrms_depth, np.nan)\n",
    "                \n",
    "            # Resample CT data if available\n",
    "            ct_resampled = None\n",
    "            if ct_data is not None:\n",
    "                ct_resampled = np.interp(hrms_depth,\n",
    "                                       ct_data['SB_DEPTH_cm'].values,\n",
    "                                       ct_data['CT'].values)\n",
    "            else:\n",
    "                ct_resampled = np.full_like(hrms_depth, np.nan)\n",
    "\n",
    "            # Apply thresholds to HRMS data\n",
    "            if 'hiresms' in data_config['thresholds']:\n",
    "                condition, threshold_value, buffer_size = data_config['thresholds']['hiresms']\n",
    "                extreme_values = eval(f\"hrms_data['hiresMS'] {condition} {threshold_value}\")\n",
    "                buffer_indices = []\n",
    "                for i in range(len(hrms_data)):\n",
    "                    if extreme_values[i]:\n",
    "                        buffer_indices.extend(range(max(0, i - buffer_size), min(len(hrms_data), i + buffer_size + 1)))\n",
    "                hrms_data.loc[buffer_indices, 'hiresMS'] = np.nan\n",
    "\n",
    "            # Identify continuous segments in HRMS data\n",
    "            valid_indices = hrms_data.index[hrms_data['hiresMS'].notna()].tolist()\n",
    "            segments = []\n",
    "            if valid_indices:\n",
    "                current_segment = [valid_indices[0]]\n",
    "                for idx in valid_indices[1:]:\n",
    "                    if idx == current_segment[-1] + 1:\n",
    "                        current_segment.append(idx)\n",
    "                    else:\n",
    "                        segments.append(current_segment)\n",
    "                        current_segment = [idx]\n",
    "                segments.append(current_segment)\n",
    "\n",
    "            # Process each HRMS segment\n",
    "            for seg in segments:\n",
    "                seg_depth = hrms_data.loc[seg, 'SB_DEPTH_cm']\n",
    "                seg_values = hrms_data.loc[seg, 'hiresMS']\n",
    "                if seg_values.empty:\n",
    "                    continue\n",
    "\n",
    "                # Get resampled data for this segment\n",
    "                seg_density = density_resampled[seg] if density_resampled is not None else None\n",
    "                seg_ct = ct_resampled[seg] if ct_resampled is not None else None\n",
    "\n",
    "                # Check if at least one reference curve has valid data\n",
    "                if (seg_density is None or np.all(np.isnan(seg_density))) and (seg_ct is None or np.all(np.isnan(seg_ct))):\n",
    "                    print(f\"Warning: No valid reference data for segment at depth {seg_depth.iloc[0]:.2f}\")\n",
    "                    continue\n",
    "\n",
    "                # ---- Candidate Shift from Density Curve ----\n",
    "                candidate_shift_density = 0.0\n",
    "                corr_density = 0.0\n",
    "                if seg_density is not None and not np.all(np.isnan(seg_density)):\n",
    "                    candidate_shift_density = compute_candidate_shift(seg_values.values, \n",
    "                                                                   seg_density,\n",
    "                                                                   seg_depth.values)\n",
    "                    corr_density = np.abs(np.corrcoef(seg_values.values, seg_density)[0,1])\n",
    "                    if np.isnan(corr_density): corr_density = 0.0\n",
    "\n",
    "                # ---- Candidate Shift from CT Curve ----\n",
    "                candidate_shift_ct = 0.0\n",
    "                corr_ct = 0.0\n",
    "                if seg_ct is not None and not np.all(np.isnan(seg_ct)):\n",
    "                    candidate_shift_ct = compute_candidate_shift(seg_values.values,\n",
    "                                                              seg_ct, \n",
    "                                                              seg_depth.values)\n",
    "                    corr_ct = np.abs(np.corrcoef(seg_values.values, seg_ct)[0,1])\n",
    "                    if np.isnan(corr_ct): corr_ct = 0.0\n",
    "\n",
    "                # Determine the maximum allowed shift based on neighboring gaps\n",
    "                if seg[0] > 0:\n",
    "                    gap_before = hrms_data.at[seg[0], 'SB_DEPTH_cm'] - hrms_data.at[seg[0]-1, 'SB_DEPTH_cm']\n",
    "                else:\n",
    "                    gap_before = np.inf\n",
    "                if seg[-1] < len(hrms_data) - 1:\n",
    "                    gap_after = hrms_data.at[seg[-1]+1, 'SB_DEPTH_cm'] - hrms_data.at[seg[-1], 'SB_DEPTH_cm']\n",
    "                else:\n",
    "                    gap_after = np.inf\n",
    "\n",
    "                # Calculate gap-based shift limit\n",
    "                gap_based_shift = min(gap_before, gap_after) * shift_limit_multiplier\n",
    "\n",
    "                # ---- Consensus Shift ----\n",
    "                # Set correlation to 0 if candidate shift exceeds gap-based shift or is negative\n",
    "                if abs(candidate_shift_density) > gap_based_shift or corr_density < 0:\n",
    "                    corr_density = 0.0\n",
    "                if abs(candidate_shift_ct) > gap_based_shift or corr_ct < 0:\n",
    "                    corr_ct = 0.0\n",
    "\n",
    "                # Calculate weights based on correlations\n",
    "                total_corr = corr_density + corr_ct\n",
    "                if total_corr > 0:\n",
    "                    w_density = corr_density / total_corr\n",
    "                    w_ct = corr_ct / total_corr\n",
    "                else:\n",
    "                    w_density, w_ct = 0.4, 0.6\n",
    "\n",
    "                consensus_shift = (w_density * candidate_shift_density + \n",
    "                                 w_ct * candidate_shift_ct)\n",
    "\n",
    "                # Apply the consensus shift by moving hiresMS values to new positions\n",
    "                if abs(consensus_shift) <= gap_based_shift:\n",
    "                    # Calculate target indices based on depth shift\n",
    "                    target_indices = []\n",
    "                    for idx in seg:\n",
    "                        current_depth = hrms_data.at[idx, 'SB_DEPTH_cm']\n",
    "                        target_depth = current_depth + consensus_shift\n",
    "                        # Find closest depth position\n",
    "                        target_idx = (hrms_data['SB_DEPTH_cm'] - target_depth).abs().idxmin()\n",
    "                        target_indices.append(target_idx)\n",
    "                    \n",
    "                    # Store original values\n",
    "                    original_values = hrms_data.loc[seg, 'hiresMS'].copy()\n",
    "                    \n",
    "                    # Clear original positions\n",
    "                    hrms_data.loc[seg, 'hiresMS'] = np.nan\n",
    "                    \n",
    "                    # Move values to new positions\n",
    "                    for orig_val, target_idx in zip(original_values, target_indices):\n",
    "                        hrms_data.at[target_idx, 'hiresMS'] = orig_val\n",
    "                else:\n",
    "                    print(f\"Warning: Computed shift ({consensus_shift:.2f}) exceeds gap-based shift limit ({gap_based_shift:.2f}) \"\n",
    "                          f\"for segment at depths {seg_depth.iloc[0]:.2f}-{seg_depth.iloc[-1]:.2f}\")\n",
    "\n",
    "            # Rescale the depth after shifting\n",
    "            depth_scale_factor = data_config['core_length'] / hrms_data['SB_DEPTH_cm'].max()\n",
    "            hrms_data['SB_DEPTH_cm'] = hrms_data['SB_DEPTH_cm'] * depth_scale_factor\n",
    "            hrms_output_path = data_config['mother_dir'] + data_config['clean_file_paths']['hrms']\n",
    "            hrms_data.to_csv(hrms_output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for plotting cleanned core images and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_core_logs(data_config, file_type='clean', title=None):\n",
    "    \"\"\"Plot core logs using direct file paths\"\"\"\n",
    "    # Get file paths based on type\n",
    "    if file_type == 'clean':\n",
    "        data_paths = data_config.get('clean_file_paths', {})\n",
    "    else:\n",
    "        data_paths = data_config.get('filled_file_paths', {})\n",
    "    \n",
    "    # Get available column configs\n",
    "    available_columns = data_config.get('column_configs', {})\n",
    "    \n",
    "    # Only process data types that have both file path and column config\n",
    "    valid_data_types = set(data_paths.keys()) & set(available_columns.keys())\n",
    "    \n",
    "    # Build full file paths\n",
    "    full_paths = {}\n",
    "    for data_type in valid_data_types:\n",
    "        if file_type == 'clean':\n",
    "            full_paths[data_type] = data_config['mother_dir'] + data_config['clean_output_folder'] + data_paths[data_type]\n",
    "        else:\n",
    "            full_paths[data_type] = data_config['mother_dir'] + data_config['filled_output_folder'] + data_paths[data_type]\n",
    "    \n",
    "\n",
    "    # Load images\n",
    "    ct_img_path = data_config['mother_dir'] + data_config['ct_image_path']\n",
    "    rgb_img_path = data_config['mother_dir'] + data_config['rgb_image_path']\n",
    "    \n",
    "    ct_img = plt.imread(ct_img_path) if os.path.exists(ct_img_path) else None\n",
    "    rgb_img = plt.imread(rgb_img_path) if os.path.exists(rgb_img_path) else None\n",
    "    \n",
    "    # Load Core Length and Name\n",
    "    core_length = data_config['core_length']\n",
    "    core_name = data_config['core_name']\n",
    "    \n",
    "    if title is None:\n",
    "        file_type_title = 'Cleaned' if file_type == 'clean' else 'ML-Filled'\n",
    "        title = f'{core_name} {file_type_title} Logs'\n",
    "    \n",
    "    # Load available data\n",
    "    data = {}\n",
    "    for key, path in full_paths.items():\n",
    "        if os.path.exists(path):\n",
    "            loaded_data = pd.read_csv(path)\n",
    "            if 'SB_DEPTH_cm' in loaded_data.columns:\n",
    "                data[key] = loaded_data\n",
    "    \n",
    "    if not data:\n",
    "        raise ValueError(\"No valid data files found to plot\")\n",
    "    \n",
    "    # [Rest of plotting logic remains the same as before]\n",
    "    # Calculate number of plots based on available data\n",
    "    n_plots = 0\n",
    "    \n",
    "    if ct_img is not None and 'ct' in data:\n",
    "        n_plots += 2\n",
    "    if rgb_img is not None and 'rgb' in data:\n",
    "        n_plots += 2\n",
    "        \n",
    "    # MS panel\n",
    "    has_ms = False\n",
    "    if 'mst' in data and 'mst' in available_columns:\n",
    "        ms_col = available_columns['mst']['ms']['data_col']\n",
    "        if ms_col in data['mst'].columns and not data['mst'][ms_col].isna().all():\n",
    "            has_ms = True\n",
    "    if 'hrms' in data and 'hrms' in available_columns:\n",
    "        hrms_col = available_columns['hrms']['data_col']\n",
    "        if not data['hrms'][hrms_col].isna().all():\n",
    "            has_ms = True\n",
    "    if has_ms:\n",
    "        n_plots += 1\n",
    "        \n",
    "    # Other MST logs\n",
    "    if 'mst' in data and 'mst' in available_columns:\n",
    "        for log_type, config in available_columns['mst'].items():\n",
    "            if log_type != 'ms' and config['data_col'] in data['mst'].columns and not data['mst'][config['data_col']].isna().all():\n",
    "                n_plots += 1\n",
    "\n",
    "    if n_plots == 0:\n",
    "        raise ValueError(\"No valid data to plot\")\n",
    "    \n",
    "    # Create figure and axes\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(10, 16), sharey=True)\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    fig.suptitle(title, fontweight='bold')\n",
    "    \n",
    "    current_ax = 0\n",
    "    \n",
    "    # Plot CT if available\n",
    "    if ct_img is not None and 'ct' in data:\n",
    "        # CT image\n",
    "        axes[current_ax].imshow(ct_img, aspect='auto', extent=[0, 0.5, core_length, 0])\n",
    "        axes[current_ax].set_ylabel('Depth (cm)')\n",
    "        axes[current_ax].set_xticks([])\n",
    "        axes[current_ax].set_xlabel('Sediment\\nCore\\nCT Scan', fontweight='bold', fontsize='small')\n",
    "        current_ax += 1\n",
    "        \n",
    "        # CT data\n",
    "        ct_config = available_columns['ct']\n",
    "        ct_col = ct_config['data_col']\n",
    "        ct_std = ct_config['std_col']\n",
    "        ct_depth = data['ct'][ct_config['depth_col']].astype(np.float64)\n",
    "        \n",
    "        axes[current_ax].plot(data['ct'][ct_col].astype(np.float64), ct_depth, color='black', linewidth=0.7)\n",
    "        \n",
    "        if ct_std in data['ct'].columns:\n",
    "            axes[current_ax].fill_betweenx(\n",
    "                ct_depth,\n",
    "                data['ct'][ct_col].astype(np.float64) - data['ct'][ct_std].astype(np.float64),\n",
    "                data['ct'][ct_col].astype(np.float64) + data['ct'][ct_std].astype(np.float64),\n",
    "                color='black', alpha=0.2, linewidth=0\n",
    "            )\n",
    "        \n",
    "        # Color-coded CT values\n",
    "        ct_values = data['ct'][ct_col].astype(np.float64).values\n",
    "        depths = ct_depth.values\n",
    "        norm = plt.Normalize(300, 1600)\n",
    "        cmap = plt.cm.jet\n",
    "        \n",
    "        ct_polys = []\n",
    "        ct_facecolors = []\n",
    "        for i in range(len(depths) - 1):\n",
    "            if not (np.isnan(ct_values[i]) or np.isnan(ct_values[i+1])):\n",
    "                poly = [(0, depths[i]), (ct_values[i], depths[i]), (ct_values[i+1], depths[i+1]), (0, depths[i+1])]\n",
    "                ct_polys.append(poly)\n",
    "                avg_val = (ct_values[i] + ct_values[i+1]) / 2\n",
    "                ct_facecolors.append(cmap(norm(avg_val)))\n",
    "                \n",
    "        if ct_polys:\n",
    "            pc_ct = mcoll.PolyCollection(ct_polys, facecolors=ct_facecolors, edgecolors='none', alpha=0.95)\n",
    "            axes[current_ax].add_collection(pc_ct)\n",
    "        \n",
    "        axes[current_ax].set_xlabel('CT#\\nBrightness', fontweight='bold', fontsize='small')\n",
    "        axes[current_ax].grid(True)\n",
    "        axes[current_ax].set_xlim(300, None)\n",
    "        axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "        current_ax += 1\n",
    "    \n",
    "    # Plot RGB if available\n",
    "    if rgb_img is not None and 'rgb' in data:\n",
    "        # RGB image\n",
    "        axes[current_ax].imshow(rgb_img, aspect='auto', extent=[0, 0.5, core_length, 0])\n",
    "        axes[current_ax].set_xticks([])\n",
    "        axes[current_ax].set_xlabel('Sediment\\nCore\\nPhoto', fontweight='bold', fontsize='small')\n",
    "        current_ax += 1\n",
    "        \n",
    "        # RGB data\n",
    "        rgb_config = available_columns['rgb']\n",
    "        rgb_cols = rgb_config['data_cols']\n",
    "        rgb_stds = rgb_config['std_cols']\n",
    "        rgb_depth = data['rgb'][rgb_config['depth_col']].astype(np.float64)\n",
    "        colors = ['red', 'green', 'blue']\n",
    "        \n",
    "        for col, std, color in zip(rgb_cols[:3], rgb_stds[:3], colors):\n",
    "            if col in data['rgb'].columns:\n",
    "                axes[current_ax].plot(data['rgb'][col].astype(np.float64), rgb_depth, color=color, linewidth=0.7)\n",
    "                if std in data['rgb'].columns:\n",
    "                    axes[current_ax].fill_betweenx(\n",
    "                        rgb_depth,\n",
    "                        data['rgb'][col].astype(np.float64) - data['rgb'][std].astype(np.float64),\n",
    "                        data['rgb'][col].astype(np.float64) + data['rgb'][std].astype(np.float64),\n",
    "                        color=color, alpha=0.2, linewidth=0\n",
    "                    )\n",
    "        \n",
    "        # Luminance plot\n",
    "        if 'Lumin' in data['rgb'].columns:\n",
    "            lumin_values = data['rgb']['Lumin'].astype(np.float64).values\n",
    "            lumin_depths = rgb_depth.values\n",
    "            \n",
    "            valid_lumin = lumin_values[~np.isnan(lumin_values)]\n",
    "            if len(valid_lumin) > 0:\n",
    "                vmin, vmax = valid_lumin.min(), valid_lumin.max()\n",
    "                if not np.isclose(vmin, vmax):\n",
    "                    lumin_norm = plt.Normalize(vmin, vmax)\n",
    "                    cmap_inferno = plt.cm.inferno\n",
    "                    \n",
    "                    lumin_polys = []\n",
    "                    lumin_facecolors = []\n",
    "                    for i in range(len(lumin_depths) - 1):\n",
    "                        if not (np.isnan(lumin_values[i]) or np.isnan(lumin_values[i+1])):\n",
    "                            poly = [(0, lumin_depths[i]), (lumin_values[i], lumin_depths[i]), \n",
    "                                   (lumin_values[i+1], lumin_depths[i+1]), (0, lumin_depths[i+1])]\n",
    "                            lumin_polys.append(poly)\n",
    "                            avg_val = (lumin_values[i] + lumin_values[i+1]) / 2\n",
    "                            lumin_facecolors.append(cmap_inferno(lumin_norm(avg_val)))\n",
    "                    if lumin_polys:\n",
    "                        pc_lumin = mcoll.PolyCollection(lumin_polys, facecolors=lumin_facecolors, edgecolors='none', alpha=0.95)\n",
    "                        axes[current_ax].add_collection(pc_lumin)\n",
    "        \n",
    "        axes[current_ax].set_xlabel('RGB\\nLuminance', fontweight='bold', fontsize='small')\n",
    "        axes[current_ax].grid(True)\n",
    "        axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "        current_ax += 1\n",
    "    \n",
    "    # Plot MS data if available\n",
    "    if has_ms:\n",
    "        has_mst_ms = False\n",
    "        has_hrms = False\n",
    "        \n",
    "        if 'mst' in data and 'mst' in available_columns:\n",
    "            ms_col = available_columns['mst']['ms']['data_col']\n",
    "            if ms_col in data['mst'].columns and not data['mst'][ms_col].isna().all():\n",
    "                has_mst_ms = True\n",
    "                \n",
    "        if 'hrms' in data and 'hrms' in available_columns:\n",
    "            if not data['hrms'][available_columns['hrms']['data_col']].isna().all():\n",
    "                has_hrms = True\n",
    "        \n",
    "        if has_mst_ms:\n",
    "            axes[current_ax].plot(\n",
    "                data['mst'][ms_col].astype(np.float64), \n",
    "                data['mst'][available_columns['mst']['ms']['depth_col']].astype(np.float64),\n",
    "                color='darkgray', label='Lo-res', linewidth=0.7\n",
    "            )\n",
    "        if has_hrms:\n",
    "            axes[current_ax].plot(\n",
    "                data['hrms'][available_columns['hrms']['data_col']].astype(np.float64), \n",
    "                data['hrms'][available_columns['hrms']['depth_col']].astype(np.float64),\n",
    "                color='black', label='Hi-res', linewidth=0.7\n",
    "            )\n",
    "        axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "        axes[current_ax].set_xlabel('Magnetic\\nSusceptibility', fontweight='bold', fontsize='small')\n",
    "        axes[current_ax].grid(True)\n",
    "        current_ax += 1\n",
    "\n",
    "    # Plot other MST logs if available\n",
    "    if 'mst' in data and 'mst' in available_columns:\n",
    "        mst_labels = {\n",
    "            'density': 'Density\\n(g/cc)',\n",
    "            'pwvel': 'P-wave\\nVelocity\\n(m/s)',\n",
    "            'pwamp': 'P-wave\\nAmplitude',\n",
    "            'elecres': 'Electrical\\nResistivity\\n(ohm-m)'\n",
    "        }\n",
    "        \n",
    "        mst_colors = {\n",
    "            'density': 'orange',\n",
    "            'pwvel': 'purple',\n",
    "            'pwamp': 'purple',\n",
    "            'elecres': 'brown'\n",
    "        }\n",
    "\n",
    "        for log_type, config in available_columns['mst'].items():\n",
    "            if log_type != 'ms' and config['data_col'] in data['mst'].columns and not data['mst'][config['data_col']].isna().all():\n",
    "                axes[current_ax].plot(\n",
    "                    data['mst'][config['data_col']].astype(np.float64), \n",
    "                    data['mst'][config['depth_col']].astype(np.float64), \n",
    "                    color=mst_colors.get(log_type, 'black'), \n",
    "                    linewidth=0.7\n",
    "                )\n",
    "                axes[current_ax].set_xlabel(mst_labels[log_type], fontweight='bold', fontsize='small')\n",
    "                axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "                axes[current_ax].grid(True)\n",
    "                if log_type == 'density':\n",
    "                    axes[current_ax].set_xlim(1, 2)\n",
    "                current_ax += 1\n",
    "    \n",
    "    # Set common y-axis properties\n",
    "    for ax in axes:\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_ylim(core_length, 0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions for Machine Learning to fill data gaps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for plotting filled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filled_data(target_log, original_data, filled_data, core_length, core_name, ML_type = 'ML'):\n",
    "    \"\"\"\n",
    "    Plot original and ML-filled data for a given log.\n",
    "    \n",
    "    Args:\n",
    "        target_log (str): Name of the log to plot\n",
    "        original_data (pd.DataFrame): Original data containing the log\n",
    "        filled_data (pd.DataFrame): Data with ML-filled gaps\n",
    "        core_length (int): Length of the core in cm\n",
    "        core_name (str): Name of the core for plot title\n",
    "    \"\"\"\n",
    "    # Check if there are any gaps\n",
    "    has_gaps = original_data[target_log].isna().any()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(15, 3))\n",
    "    title_suffix = f'Use {ML_type} for Data Gap Filling' if has_gaps else \"(No Data Gap to be filled by ML)\"\n",
    "    fig.suptitle(f'{core_name} {target_log} Values {title_suffix}', fontweight='bold')\n",
    "\n",
    "    # Plot data with ML-predicted gaps only if gaps exist\n",
    "    if has_gaps:\n",
    "        ax.plot(filled_data['SB_DEPTH_cm'], filled_data[target_log], \n",
    "                color='red', label=f'ML Predicted {target_log}', linewidth=0.7, alpha=0.7)\n",
    "\n",
    "    # Plot original data\n",
    "    ax.plot(original_data['SB_DEPTH_cm'], original_data[target_log], \n",
    "            color='black', label=f'Original {target_log}', linewidth=0.7)\n",
    "\n",
    "    # Add uncertainty shade if std column exists\n",
    "    std_col = f'{target_log}_std'\n",
    "    if std_col in original_data.columns:\n",
    "        ax.fill_between(original_data['SB_DEPTH_cm'],\n",
    "                       original_data[target_log] - original_data[std_col],\n",
    "                       original_data[target_log] + original_data[std_col],\n",
    "                       color='black', alpha=0.2, linewidth=0)\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_ylabel(f'{target_log}\\nBrightness', fontweight='bold', fontsize='small')\n",
    "    ax.set_xlabel('Depth (cm)')\n",
    "    ax.grid(True)\n",
    "    ax.invert_xaxis()\n",
    "    ax.set_xlim(0, core_length)\n",
    "    ax.tick_params(axis='y', labelsize='x-small')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Machine Learning Data Gap filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for fill_gaps_with_ml\n",
    "\n",
    "def prepare_feature_data(target_log, All_logs, merge_tolerance):\n",
    "    \"\"\"Prepare merged feature data for ML training.\"\"\"\n",
    "    # Get target data from All_logs\n",
    "    target_data = None\n",
    "    for df, cols in All_logs.values():\n",
    "        if target_log in cols:\n",
    "            target_data = df.copy()\n",
    "            break\n",
    "    \n",
    "    if target_data is None:\n",
    "        raise ValueError(f\"Target log '{target_log}' not found in any dataset\")\n",
    "\n",
    "    # Convert SB_DEPTH_cm to float64 in target data\n",
    "    target_data['SB_DEPTH_cm'] = target_data['SB_DEPTH_cm'].astype('float64')\n",
    "\n",
    "    # Prepare training data by merging all available logs\n",
    "    merged_data = target_data[['SB_DEPTH_cm', target_log]].copy()\n",
    "    features = []\n",
    "    \n",
    "    # Merge feature dataframes one by one, using their own SB_DEPTH_cm column\n",
    "    for df_name, (df, cols) in All_logs.items():\n",
    "        if target_log not in cols:  # Skip the target dataframe\n",
    "            df = df.copy()\n",
    "            df['SB_DEPTH_cm'] = df['SB_DEPTH_cm'].astype('float64')\n",
    "            # Rename SB_DEPTH_cm temporarily to avoid conflicts during merging\n",
    "            df = df.rename(columns={'SB_DEPTH_cm': f'SB_DEPTH_cm_{df_name}'})\n",
    "            # Convert all numeric columns to float64\n",
    "            for col in cols:\n",
    "                if col != 'SB_DEPTH_cm' and df[col].dtype.kind in 'biufc':\n",
    "                    df[col] = df[col].astype('float64')\n",
    "            # Rename feature columns for merging\n",
    "            df_renamed = df.rename(columns={col: f'{df_name}_{col}' for col in cols if col != 'SB_DEPTH_cm'})\n",
    "            df_renamed = df_renamed.sort_values(f'SB_DEPTH_cm_{df_name}')\n",
    "            \n",
    "            # Perform merge_asof with tolerance for data alignment\n",
    "            merged_data = pd.merge_asof(\n",
    "                merged_data.sort_values('SB_DEPTH_cm'),\n",
    "                df_renamed,\n",
    "                left_on='SB_DEPTH_cm',\n",
    "                right_on=f'SB_DEPTH_cm_{df_name}',\n",
    "                direction='nearest',\n",
    "                tolerance=merge_tolerance\n",
    "            )\n",
    "            \n",
    "            # Check for unmatched rows due to the tolerance constraint\n",
    "            unmatched = merged_data[f'SB_DEPTH_cm_{df_name}'].isna().sum()\n",
    "            if unmatched > 0:\n",
    "                warnings.warn(f\"{unmatched} rows did not have a matching depth within tolerance for log '{df_name}'.\")\n",
    "            \n",
    "            # Add renamed feature columns to features list\n",
    "            features.extend([f'{df_name}_{col}' for col in cols if col != 'SB_DEPTH_cm'])\n",
    "            # Drop the temporary depth column used for merging\n",
    "            merged_data = merged_data.drop(columns=[f'SB_DEPTH_cm_{df_name}'])\n",
    "    \n",
    "    # Add SB_DEPTH_cm as a feature\n",
    "    features.append('SB_DEPTH_cm')\n",
    "    \n",
    "    return target_data, merged_data, features\n",
    "\n",
    "def apply_feature_weights(X, method):\n",
    "    \"\"\"Apply feature weights for XGBoost methods.\"\"\"\n",
    "    if method == 'xgb':\n",
    "        feature_weights = {\n",
    "            'RGB': {\n",
    "                'R': 0.1,\n",
    "                'G': 0.1,\n",
    "                'B': 0.1,\n",
    "                'Lumin': 0.32\n",
    "            },\n",
    "            'MS': {\n",
    "                'hiresMS': 3.0,\n",
    "                'MS': 0.5\n",
    "            },\n",
    "            'Physical': {\n",
    "                'PWVel_m/s': 0.05,\n",
    "                'PWAmp': 0.05,\n",
    "                'Den_gm/cc': 3.0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        X_weighted = X.copy()\n",
    "        for group, weights in feature_weights.items():\n",
    "            for feature, weight in weights.items():\n",
    "                if feature in X_weighted.columns:\n",
    "                    X_weighted[feature] = (X_weighted[feature] * weight).astype('float32')\n",
    "        return X_weighted\n",
    "    \n",
    "    elif method == 'xgblgbm':\n",
    "        feature_weights = {\n",
    "            'RGB': {\n",
    "                'R': 0.3,\n",
    "                'G': 0.3,\n",
    "                'B': 0.3,\n",
    "                'Lumin': 0.3\n",
    "            },\n",
    "            'MS': {\n",
    "                'hiresMS': 3.0,\n",
    "                'MS': 0.05\n",
    "            },\n",
    "            'Physical': {\n",
    "                'PWVel_m/s': 0.01,\n",
    "                'PWAmp': 0.01,\n",
    "                'Den_gm/cc': 0.5\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        X_weighted = X.copy()\n",
    "        for group, weights in feature_weights.items():\n",
    "            for feature, weight in weights.items():\n",
    "                matching_cols = [col for col in X_weighted.columns if feature in col]\n",
    "                for col in matching_cols:\n",
    "                    X_weighted[col] = (X_weighted[col] * weight).astype('float32')\n",
    "        return X_weighted\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def adjust_gap_predictions(df, gap_mask, ml_preds, target_log):\n",
    "    \"\"\"\n",
    "    Adjust ML predictions for gap rows in 'df' so that for each contiguous gap\n",
    "    segment (with both left and right boundaries available) the predictions are\n",
    "    blended with the linear interpolation between the boundary values.\n",
    "    \"\"\"\n",
    "    # Get the integer positions (row numbers) of missing values\n",
    "    gap_positions = np.where(gap_mask.values)[0]\n",
    "    # Create a Series for easier handling; index = positions in df\n",
    "    preds_series = pd.Series(ml_preds, index=gap_positions)\n",
    "    \n",
    "    # Identify contiguous segments in the gap positions\n",
    "    segments = np.split(gap_positions, np.where(np.diff(gap_positions) != 1)[0] + 1)\n",
    "    \n",
    "    adjusted = preds_series.copy()\n",
    "    for seg in segments:\n",
    "        # seg is an array of row positions (in df) for a contiguous gap segment.\n",
    "        start_pos = seg[0]\n",
    "        end_pos = seg[-1]\n",
    "        \n",
    "        # Enforce trend constraints only if both boundaries exist.\n",
    "        if start_pos == 0 or end_pos == len(df) - 1:\n",
    "            continue  # Skip segments at the very beginning or end.\n",
    "        \n",
    "        # Retrieve boundary (observed) values and depths\n",
    "        left_value = df.iloc[start_pos - 1][target_log]\n",
    "        right_value = df.iloc[end_pos + 1][target_log]\n",
    "        # Skip if boundaries are missing (should not happen if gap_mask is correct)\n",
    "        if pd.isna(left_value) or pd.isna(right_value):\n",
    "            continue\n",
    "        left_depth = df.iloc[start_pos - 1]['SB_DEPTH_cm']\n",
    "        right_depth = df.iloc[end_pos + 1]['SB_DEPTH_cm']\n",
    "        \n",
    "        # For each gap row in the segment, blend the ML prediction with linear interpolation\n",
    "        for pos in seg:\n",
    "            current_depth = df.iloc[pos]['SB_DEPTH_cm']\n",
    "            # Normalize the depth position (x in [0, 1])\n",
    "            if right_depth == left_depth:\n",
    "                x = 0.5\n",
    "            else:\n",
    "                x = (current_depth - left_depth) / (right_depth - left_depth)\n",
    "            # Compute the linear interpolation value at this depth\n",
    "            interp_val = left_value + (right_value - left_value) * x\n",
    "            # Define a weight that is 0 at the boundaries and 1 at the middle.\n",
    "            # Here we use: weight = 1 - 2*|x - 0.5|\n",
    "            weight = 1 - 2 * abs(x - 0.5)\n",
    "            weight = max(0, min(weight, 1))  # Ensure weight is between 0 and 1\n",
    "            # Blend: final = interpolation + weight*(ML_prediction - interpolation)\n",
    "            adjusted[pos] = interp_val + weight * (preds_series.loc[pos] - interp_val)\n",
    "    return adjusted.values\n",
    "\n",
    "\n",
    "def train_model(model):\n",
    "    \"\"\"Helper function for parallel model training.\"\"\"\n",
    "    def train_wrapper(X_train, y_train, X_pred):\n",
    "        model.fit(X_train, y_train)\n",
    "        return model.predict(X_pred)\n",
    "    return train_wrapper\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps_with_ml(target_log=None, \n",
    "                      All_logs=None, \n",
    "                      output_csv=False, \n",
    "                      output_dir=None, \n",
    "                      core_name=None, \n",
    "                      merge_tolerance=3.0,\n",
    "                      ml_method='xgblgbm'):\n",
    "    \"\"\"\n",
    "    Fill gaps in target data using specified ML method.\n",
    "    \n",
    "    Args:\n",
    "        target_log (str): Name of the target column to fill gaps in.\n",
    "        All_logs (dict): Dictionary of dataframes containing feature data and target data.\n",
    "                         Format: {'df_name': (dataframe, [column_names])}\n",
    "        output_csv (bool): Whether to output filled data to CSV file.\n",
    "        output_dir (str): Directory to save output CSV file.\n",
    "        core_name (str): Name of the core for CSV filename.\n",
    "        merge_tolerance (float): Maximum allowed difference in depth (SB_DEPTH_cm) for merging\n",
    "                                 rows from different logs.\n",
    "        ml_method (str): ML method to use - 'rf', 'rftc', 'xgb', 'xgblgbm' (default)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (target_data_filled, gap_mask)\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if target_log is None or All_logs is None:\n",
    "        raise ValueError(\"Both target_log and All_logs must be provided\")\n",
    "        \n",
    "    if output_csv and (output_dir is None or core_name is None):\n",
    "        raise ValueError(\"output_dir and core_name must be provided when output_csv is True\")\n",
    "    \n",
    "    if ml_method not in ['rf', 'rftc', 'xgb', 'xgblgbm']:\n",
    "        raise ValueError(\"ml_method must be one of: 'rf', 'rftc', 'xgb', 'xgblgbm'\")\n",
    "    \n",
    "    # Prepare feature data\n",
    "    target_data, merged_data, features = prepare_feature_data(target_log, All_logs, merge_tolerance)\n",
    "    \n",
    "    # Create a copy of the original data to hold the interpolated results\n",
    "    target_data_filled = target_data.copy()\n",
    "\n",
    "    # Identify gaps in target data\n",
    "    gap_mask = target_data[target_log].isna()\n",
    "    \n",
    "    # If no gaps exist, save to CSV if requested and return original data\n",
    "    if not gap_mask.any():\n",
    "        if output_csv:\n",
    "            output_path = os.path.join(output_dir, f'{core_name}_{target_log.split(\"_\")[0]}_MLfilled.csv')\n",
    "            target_data_filled.to_csv(output_path, index=False)\n",
    "        return target_data_filled, gap_mask\n",
    "\n",
    "    # Prepare features and target for ML\n",
    "    X = merged_data[features].copy()\n",
    "    y = merged_data[target_log].copy()\n",
    "\n",
    "    # Convert all features to float64\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype.kind in 'biufc':\n",
    "            X[col] = X[col].astype('float64')\n",
    "    y = y.astype('float64')\n",
    "\n",
    "    # Split into training (non-gap) and prediction (gap) sets\n",
    "    X_train = X[~gap_mask]\n",
    "    y_train = y[~gap_mask]\n",
    "    X_pred = X[gap_mask]\n",
    "\n",
    "    # Apply specific ML method\n",
    "    if ml_method == 'rf':\n",
    "        predictions = _apply_random_forest(X_train, y_train, X_pred)\n",
    "    elif ml_method == 'rftc':\n",
    "        predictions = _apply_random_forest_with_trend_constraints(X_train, y_train, X_pred, merged_data, gap_mask, target_log)\n",
    "    elif ml_method == 'xgb':\n",
    "        predictions = _apply_xgboost(X_train, y_train, X_pred)\n",
    "    elif ml_method == 'xgblgbm':\n",
    "        predictions = _apply_xgboost_lightgbm(X_train, y_train, X_pred)\n",
    "\n",
    "    # Fill gaps with predictions\n",
    "    target_data_filled.loc[gap_mask, target_log] = predictions\n",
    "    \n",
    "    # Save to CSV if requested\n",
    "    if output_csv:\n",
    "        output_path = os.path.join(output_dir, f\"{core_name}_{target_log.split('_')[0]}_MLfilled.csv\")\n",
    "        target_data_filled.to_csv(output_path, index=False)\n",
    "\n",
    "    return target_data_filled, gap_mask\n",
    "\n",
    "\n",
    "def _apply_random_forest(X_train, y_train, X_pred):\n",
    "    \"\"\"Apply Random Forest method.\"\"\"\n",
    "    # Handle outliers using IQR method\n",
    "    quantile_cutoff = 0.025\n",
    "    Q1 = y_train.quantile(quantile_cutoff)\n",
    "    Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "    X_train = X_train[outlier_mask]\n",
    "    y_train = y_train[outlier_mask]\n",
    "\n",
    "    def train_model(model):\n",
    "        model.fit(X_train, y_train)\n",
    "        return model.predict(X_pred)\n",
    "\n",
    "    # Initialize two ensemble models\n",
    "    models = [\n",
    "        RandomForestRegressor(n_estimators=1000,\n",
    "                              max_depth=30,\n",
    "                              min_samples_split=5,\n",
    "                              min_samples_leaf=5,\n",
    "                              max_features='sqrt',\n",
    "                              bootstrap=True,\n",
    "                              random_state=42,\n",
    "                              n_jobs=-1),\n",
    "        HistGradientBoostingRegressor(max_iter=800,\n",
    "                                      learning_rate=0.05,\n",
    "                                      max_depth=5,\n",
    "                                      min_samples_leaf=50,\n",
    "                                      l2_regularization=1.0,\n",
    "                                      random_state=42,\n",
    "                                      verbose=0)\n",
    "    ]\n",
    "\n",
    "    # Train models in parallel\n",
    "    predictions = Parallel(n_jobs=-1)(delayed(train_model)(model) for model in models)\n",
    "\n",
    "    # Ensemble predictions by averaging\n",
    "    ensemble_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "    return ensemble_predictions\n",
    "\n",
    "\n",
    "def _apply_random_forest_with_trend_constraints(X_train, y_train, X_pred, merged_data, gap_mask, target_log):\n",
    "    \"\"\"Apply Random Forest with trend constraints method.\"\"\"\n",
    "    # Handle outliers using IQR method\n",
    "    quantile_cutoff = 0.15\n",
    "    Q1 = y_train.quantile(quantile_cutoff)\n",
    "    Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "    X_train = X_train[outlier_mask]\n",
    "    y_train = y_train[outlier_mask]\n",
    "    \n",
    "    def train_model(model):\n",
    "        model.fit(X_train, y_train)\n",
    "        return model.predict(X_pred)\n",
    "    \n",
    "    # Initialize two ensemble models\n",
    "    models = [\n",
    "        RandomForestRegressor(n_estimators=1000,\n",
    "                              max_depth=30,\n",
    "                              min_samples_split=5,\n",
    "                              min_samples_leaf=5,\n",
    "                              max_features='sqrt',\n",
    "                              bootstrap=True,\n",
    "                              random_state=42,\n",
    "                              n_jobs=-1),\n",
    "        HistGradientBoostingRegressor(max_iter=800,\n",
    "                                      learning_rate=0.05,\n",
    "                                      max_depth=5,\n",
    "                                      min_samples_leaf=50,\n",
    "                                      l2_regularization=1.0,\n",
    "                                      random_state=42,\n",
    "                                      verbose=-1)\n",
    "    ]\n",
    "    \n",
    "    # Train models in parallel and average their predictions\n",
    "    predictions = Parallel(n_jobs=-1)(delayed(train_model)(model) for model in models)\n",
    "    ensemble_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Apply trend constraints using the helper function from original\n",
    "    adjusted_predictions = _adjust_gap_predictions(merged_data, gap_mask, ensemble_predictions, target_log)\n",
    "    \n",
    "    return adjusted_predictions\n",
    "\n",
    "\n",
    "def _adjust_gap_predictions(df, gap_mask, ml_preds, target_log):\n",
    "    \"\"\"\n",
    "    Adjust ML predictions for gap rows in 'df' so that for each contiguous gap\n",
    "    segment (with both left and right boundaries available) the predictions are\n",
    "    blended with the linear interpolation between the boundary values.\n",
    "    \"\"\"\n",
    "    # Get the integer positions (row numbers) of missing values\n",
    "    gap_positions = np.where(gap_mask.values)[0]\n",
    "    # Create a Series for easier handling; index = positions in df\n",
    "    preds_series = pd.Series(ml_preds, index=gap_positions)\n",
    "    \n",
    "    # Identify contiguous segments in the gap positions\n",
    "    segments = np.split(gap_positions, np.where(np.diff(gap_positions) != 1)[0] + 1)\n",
    "    \n",
    "    adjusted = preds_series.copy()\n",
    "    for seg in segments:\n",
    "        # seg is an array of row positions (in df) for a contiguous gap segment.\n",
    "        start_pos = seg[0]\n",
    "        end_pos = seg[-1]\n",
    "        \n",
    "        # Enforce trend constraints only if both boundaries exist.\n",
    "        if start_pos == 0 or end_pos == len(df) - 1:\n",
    "            continue  # Skip segments at the very beginning or end.\n",
    "        \n",
    "        # Retrieve boundary (observed) values and depths\n",
    "        left_value = df.iloc[start_pos - 1][target_log]\n",
    "        right_value = df.iloc[end_pos + 1][target_log]\n",
    "        # Skip if boundaries are missing (should not happen if gap_mask is correct)\n",
    "        if pd.isna(left_value) or pd.isna(right_value):\n",
    "            continue\n",
    "        left_depth = df.iloc[start_pos - 1]['SB_DEPTH_cm']\n",
    "        right_depth = df.iloc[end_pos + 1]['SB_DEPTH_cm']\n",
    "        \n",
    "        # For each gap row in the segment, blend the ML prediction with linear interpolation\n",
    "        for pos in seg:\n",
    "            current_depth = df.iloc[pos]['SB_DEPTH_cm']\n",
    "            # Normalize the depth position (x in [0, 1])\n",
    "            if right_depth == left_depth:\n",
    "                x = 0.5\n",
    "            else:\n",
    "                x = (current_depth - left_depth) / (right_depth - left_depth)\n",
    "            # Compute the linear interpolation value at this depth\n",
    "            interp_val = left_value + (right_value - left_value) * x\n",
    "            # Define a weight that is 0 at the boundaries and 1 at the middle.\n",
    "            # Here we use: weight = 1 - 2*|x - 0.5|\n",
    "            weight = 1 - 2 * abs(x - 0.5)\n",
    "            weight = max(0, min(weight, 1))  # Ensure weight is between 0 and 1\n",
    "            # Blend: final = interpolation + weight*(ML_prediction - interpolation)\n",
    "            adjusted[pos] = interp_val + weight * (preds_series.loc[pos] - interp_val)\n",
    "    return adjusted.values\n",
    "\n",
    "\n",
    "def _apply_xgboost(X_train, y_train, X_pred):\n",
    "    \"\"\"Apply XGBoost method.\"\"\"\n",
    "    # Handle outliers using IQR method\n",
    "    quantile_cutoff = 0.025\n",
    "    Q1 = y_train.quantile(quantile_cutoff)\n",
    "    Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "    X_train = X_train[outlier_mask]\n",
    "    y_train = y_train[outlier_mask]\n",
    "\n",
    "    # Create feature pipeline\n",
    "    feature_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=True)),\n",
    "        ('selector', SelectKBest(score_func=f_regression, k='all'))\n",
    "    ])\n",
    "\n",
    "    # Process features\n",
    "    X_train_processed = feature_pipeline.fit_transform(X_train, y_train)\n",
    "    X_pred_processed = feature_pipeline.transform(X_pred)\n",
    "\n",
    "    # Convert processed arrays to float32\n",
    "    X_train_processed = X_train_processed.astype('float32')\n",
    "    X_pred_processed = X_pred_processed.astype('float32')\n",
    "    y_train = y_train.astype('float32')\n",
    "\n",
    "    # Initialize and train XGBoost model\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.003,\n",
    "        max_depth=10,\n",
    "        min_child_weight=5,\n",
    "        subsample=0.75,\n",
    "        colsample_bytree=0.75,\n",
    "        gamma=0.2,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=3.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_processed, y_train)\n",
    "    predictions = model.predict(X_pred_processed).astype('float32')\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def _apply_xgboost_lightgbm(X_train, y_train, X_pred):\n",
    "    \"\"\"Apply XGBoost + LightGBM ensemble method.\"\"\"\n",
    "    # Create feature pipeline\n",
    "    feature_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=True))\n",
    "    ])\n",
    "\n",
    "    # Process features without selector first to get actual feature count\n",
    "    X_train_processed = feature_pipeline.fit_transform(X_train, y_train)\n",
    "    \n",
    "    # Now add selector with correct feature count\n",
    "    max_features = min(50, X_train.shape[0]//10, X_train_processed.shape[1])\n",
    "    selector = SelectKBest(score_func=f_regression, k=max_features)\n",
    "    X_train_processed = selector.fit_transform(X_train_processed, y_train)\n",
    "    X_pred_processed = feature_pipeline.transform(X_pred)\n",
    "    X_pred_processed = selector.transform(X_pred_processed)\n",
    "\n",
    "    # Convert processed arrays to float32\n",
    "    X_train_processed = X_train_processed.astype('float32')\n",
    "    X_pred_processed = X_pred_processed.astype('float32')\n",
    "    y_train = y_train.astype('float32')\n",
    "\n",
    "    # Initialize models\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=3000,\n",
    "        learning_rate=0.003,\n",
    "        max_depth=10,\n",
    "        min_child_weight=5,\n",
    "        subsample=0.75,\n",
    "        colsample_bytree=0.75,\n",
    "        gamma=0.2,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=3.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=3000,\n",
    "        learning_rate=0.003,\n",
    "        max_depth=6,\n",
    "        num_leaves=20,\n",
    "        min_child_samples=50,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=3.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        force_col_wise=True,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    # Train both models with warnings suppressed\n",
    "    import warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        xgb_model.fit(X_train_processed, y_train)\n",
    "        lgb_model.fit(X_train_processed, y_train, feature_name='auto')\n",
    "\n",
    "    # Make predictions with both models\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        xgb_predictions = xgb_model.predict(X_pred_processed).astype('float32')\n",
    "        lgb_predictions = lgb_model.predict(X_pred_processed).astype('float32')\n",
    "\n",
    "    # Ensemble predictions (simple average)\n",
    "    predictions = (xgb_predictions + lgb_predictions) / 2\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to process and fill logs with chosen ML methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_fill_logs(data_config, ml_method='xgblgbm'):\n",
    "    \"\"\"Process and fill gaps using direct file paths\"\"\"\n",
    "    mother_dir = data_config['mother_dir']\n",
    "    core_name = data_config['core_name']\n",
    "    core_length = data_config['core_length']\n",
    "    clean_output_folder = data_config['clean_output_folder']\n",
    "    filled_output_folder = data_config['filled_output_folder']\n",
    "    \n",
    "    os.makedirs(mother_dir + filled_output_folder, exist_ok=True)\n",
    "    \n",
    "    clean_paths = data_config.get('clean_file_paths', {})\n",
    "    available_columns = data_config.get('column_configs', {})\n",
    "    valid_data_types = set(clean_paths.keys()) & set(available_columns.keys())\n",
    "    \n",
    "    # Load data with correct path construction\n",
    "    data_dict = {}\n",
    "    for data_type in valid_data_types:\n",
    "        full_path = mother_dir + clean_output_folder + clean_paths[data_type]\n",
    "        if os.path.exists(full_path):\n",
    "            data = pd.read_csv(full_path)\n",
    "            if not data.empty:\n",
    "                data_dict[data_type] = data\n",
    "\n",
    "    if not data_dict:\n",
    "        print(\"No valid data files found for processing\")\n",
    "        return\n",
    "\n",
    "    # Create feature data dictionary\n",
    "    feature_data = {}\n",
    "    \n",
    "    if 'ct' in data_dict and 'ct' in available_columns:\n",
    "        ct_col = available_columns['ct']['data_col']\n",
    "        if ct_col in data_dict['ct'].columns:\n",
    "            feature_data['ct'] = (data_dict['ct'], ['SB_DEPTH_cm', ct_col])\n",
    "    \n",
    "    if 'rgb' in data_dict and 'rgb' in available_columns:\n",
    "        valid_rgb_cols = ['SB_DEPTH_cm'] + [col for col in available_columns['rgb']['data_cols'] \n",
    "                                           if col in data_dict['rgb'].columns]\n",
    "        if len(valid_rgb_cols) > 1:\n",
    "            feature_data['rgb'] = (data_dict['rgb'], valid_rgb_cols)\n",
    "    \n",
    "    if 'mst' in data_dict and 'mst' in available_columns:\n",
    "        mst_cols = ['SB_DEPTH_cm']\n",
    "        for log_type, config in available_columns['mst'].items():\n",
    "            if config['data_col'] in data_dict['mst'].columns:\n",
    "                mst_cols.append(config['data_col'])\n",
    "        if len(mst_cols) > 1:\n",
    "            feature_data['mst'] = (data_dict['mst'], mst_cols)\n",
    "    \n",
    "    if 'hrms' in data_dict and 'hrms' in available_columns:\n",
    "        hrms_col = available_columns['hrms']['data_col']\n",
    "        if hrms_col in data_dict['hrms'].columns:\n",
    "            feature_data['hrms'] = (data_dict['hrms'], ['SB_DEPTH_cm', hrms_col])\n",
    "\n",
    "    if not feature_data:\n",
    "        print(\"No valid feature data found for ML processing\")\n",
    "        return\n",
    "\n",
    "    # Define logs to process\n",
    "    logs_to_process = []\n",
    "    \n",
    "    if 'ct' in data_dict and 'ct' in available_columns:\n",
    "        ct_col = available_columns['ct']['data_col']\n",
    "        if ct_col in data_dict['ct'].columns:\n",
    "            logs_to_process.append((ct_col, ct_col, data_dict['ct']))\n",
    "    \n",
    "    if 'rgb' in data_dict and 'rgb' in available_columns:\n",
    "        for col in available_columns['rgb']['data_cols']:\n",
    "            if col in data_dict['rgb'].columns and not data_dict['rgb'][col].empty:\n",
    "                logs_to_process.append((col, col, data_dict['rgb']))\n",
    "    \n",
    "    if 'mst' in data_dict and 'mst' in available_columns:\n",
    "        for log_type, config in available_columns['mst'].items():\n",
    "            col = config['data_col']\n",
    "            if col in data_dict['mst'].columns and not data_dict['mst'][col].empty:\n",
    "                logs_to_process.append((col, col, data_dict['mst']))\n",
    "    \n",
    "    if 'hrms' in data_dict and 'hrms' in available_columns:\n",
    "        hrms_col = available_columns['hrms']['data_col']\n",
    "        if hrms_col in data_dict['hrms'].columns:\n",
    "            logs_to_process.append((hrms_col, hrms_col, data_dict['hrms']))\n",
    "\n",
    "    if not logs_to_process:\n",
    "        print(\"No valid logs found for processing\")\n",
    "        return\n",
    "\n",
    "    ml_names = {\n",
    "        'rf': 'Random Forest',\n",
    "        'rftc': 'Random Forest + Trend Constraint', \n",
    "        'xgb': 'XGBoost',\n",
    "        'xgblgbm': 'XGBoost + LightGBM'\n",
    "    }\n",
    "    \n",
    "    if ml_method not in ml_names:\n",
    "        raise ValueError(\"ml_method must be one of: 'rf', 'rftc', 'xgb', 'xgblgbm'\")\n",
    "\n",
    "    # Process each log\n",
    "    for target_log, plot_name, data in logs_to_process:\n",
    "        if target_log in ['R', 'G', 'B', 'Lumin']:\n",
    "            filtered_features = {}\n",
    "            priority_features = ['hrms', 'ct', 'rgb']\n",
    "            \n",
    "            for key in priority_features:\n",
    "                if key in feature_data:\n",
    "                    if key == 'rgb':\n",
    "                        df, cols = feature_data[key]\n",
    "                        valid_cols = ['SB_DEPTH_cm'] + [c for c in ['R','G','B','Lumin'] if c in cols and c in df.columns]\n",
    "                        if len(valid_cols) > 1:\n",
    "                            filtered_features[key] = (df, valid_cols)\n",
    "                    else:\n",
    "                        filtered_features[key] = feature_data[key]\n",
    "            \n",
    "            if 'mst' in feature_data:\n",
    "                df, cols = feature_data['mst']\n",
    "                if 'Den_gm/cc' in cols and 'Den_gm/cc' in df.columns:\n",
    "                    filtered_features['mst'] = (df, ['SB_DEPTH_cm', 'Den_gm/cc'])\n",
    "                    \n",
    "            filled_data, gap_mask = fill_gaps_with_ml(\n",
    "                target_log=target_log,\n",
    "                All_logs=filtered_features,\n",
    "                output_csv=True,\n",
    "                output_dir=mother_dir + filled_output_folder,\n",
    "                core_name=core_name,\n",
    "                ml_method=ml_method\n",
    "            )\n",
    "        else:\n",
    "            filled_data, gap_mask = fill_gaps_with_ml(\n",
    "                target_log=target_log,\n",
    "                All_logs=feature_data,\n",
    "                output_csv=True,\n",
    "                output_dir=mother_dir + filled_output_folder,\n",
    "                core_name=core_name,\n",
    "                ml_method=ml_method\n",
    "            )\n",
    "            \n",
    "        plot_filled_data(plot_name, data, filled_data, core_length, core_name, ML_type=ml_names[ml_method])\n",
    "\n",
    "    # Consolidate RGB data\n",
    "    if 'rgb' in data_dict and 'rgb' in available_columns:\n",
    "        rgb_data = data_dict['rgb'].copy()\n",
    "        rgb_columns = available_columns['rgb']['data_cols']\n",
    "        updated = False\n",
    "        \n",
    "        for col in rgb_columns:\n",
    "            if col in rgb_data.columns:\n",
    "                filled_file = mother_dir + filled_output_folder + f'{core_name}_{col}_MLfilled.csv'\n",
    "                if os.path.exists(filled_file):\n",
    "                    filled_data = pd.read_csv(filled_file)\n",
    "                    if col in filled_data.columns:\n",
    "                        rgb_data[col] = filled_data[col]\n",
    "                        updated = True\n",
    "                        print(f\"Updated {col} column with ML-filled data\")\n",
    "        \n",
    "        if updated:\n",
    "            rgb_data.to_csv(mother_dir + filled_output_folder + f'{core_name}_RGB_MLfilled.csv', index=False)\n",
    "            for col in rgb_columns:\n",
    "                filled_file = mother_dir + filled_output_folder + f'{core_name}_{col}_MLfilled.csv'\n",
    "                if os.path.exists(filled_file):\n",
    "                    os.remove(filled_file)\n",
    "\n",
    "    # Consolidate MST data\n",
    "    if 'mst' in data_dict and 'mst' in available_columns:\n",
    "        mst_data = data_dict['mst'].copy()\n",
    "        updated = False\n",
    "        \n",
    "        for log_type, config in available_columns['mst'].items():\n",
    "            col = config['data_col']\n",
    "            if col in mst_data.columns:\n",
    "                col_name = col.split('_')[0] if '_' in col else col\n",
    "                filled_file = mother_dir + filled_output_folder + f'{core_name}_{col_name}_MLfilled.csv'\n",
    "                if os.path.exists(filled_file):\n",
    "                    filled_data = pd.read_csv(filled_file)\n",
    "                    if col in filled_data.columns:\n",
    "                        mst_data[col] = filled_data[col]\n",
    "                        updated = True\n",
    "                        print(f\"Updated {col} column with ML-filled data\")\n",
    "        \n",
    "        if updated:\n",
    "            mst_data.to_csv(mother_dir + filled_output_folder + f'{core_name}_MST_MLfilled.csv', index=False)\n",
    "            for log_type, config in available_columns['mst'].items():\n",
    "                col = config['data_col']\n",
    "                col_name = col.split('_')[0] if '_' in col else col\n",
    "                filled_file = mother_dir + filled_output_folder + f'{core_name}_{col_name}_MLfilled.csv'\n",
    "                if os.path.exists(filled_file):\n",
    "                    os.remove(filled_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### **Define data structure**\n",
    "\n",
    "#### Define core name and core length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_name = \"M9907-11PC\"  # Core name\n",
    "total_length_cm = 439     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-12PC\"  # Core name\n",
    "# total_length_cm = 488     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-14TC\"  # Core name\n",
    "# total_length_cm = 199     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-22PC\"  # Core name\n",
    "# total_length_cm = 501     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-22TC\"  # Core name\n",
    "# total_length_cm = 173     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-23PC\"  # Core name\n",
    "# total_length_cm = 783     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-25PC\"  # Core name\n",
    "# total_length_cm = 797     # Core length in cm\n",
    "\n",
    "# core_name = \"RR0207-56PC\"  # Core name\n",
    "# total_length_cm = 794     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-30PC\"  # Core name\n",
    "# total_length_cm = 781     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-31PC\"  # Core name\n",
    "# total_length_cm = 767     # Core length in cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define file path, data configuration, and outliner cut-off thresholds for ML data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration for ML data imputation\n",
    "\n",
    "data_config = {\n",
    "    'mother_dir': '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/',\n",
    "    'core_name': core_name,\n",
    "    'core_length': total_length_cm,\n",
    "    'data_folder': f'_compiled_logs/{core_name}/',\n",
    "    'clean_output_folder': f'_compiled_logs/{core_name}/ML_clean/',\n",
    "    'filled_output_folder': f'_compiled_logs/{core_name}/ML_filled/',\n",
    "    \n",
    "    # filenames\n",
    "    'clean_file_paths': {\n",
    "        'ct': f'{core_name}_CT_clean.csv',\n",
    "        'rgb': f'{core_name}_RGB_clean.csv',\n",
    "        'mst': f'{core_name}_MST_clean.csv',\n",
    "        'hrms': f'{core_name}_hiresMS_clean.csv'\n",
    "    },\n",
    "    \n",
    "    'filled_file_paths': {\n",
    "        'ct': f'{core_name}_CT_MLfilled.csv',\n",
    "        'rgb': f'{core_name}_RGB_MLfilled.csv',\n",
    "        'mst': f'{core_name}_MST_MLfilled.csv',\n",
    "        'hrms': f'{core_name}_hiresMS_MLfilled.csv'\n",
    "    },\n",
    "    \n",
    "    'ct_image_path': f'_compiled_logs/{core_name}/{core_name}_CT.tiff',\n",
    "    'rgb_image_path': f'_compiled_logs/{core_name}/{core_name}_RGB.tiff',\n",
    "    \n",
    "    'column_configs': {\n",
    "        'ct': {'data_col': 'CT', 'std_col': 'CT_std', 'depth_col': 'SB_DEPTH_cm'},\n",
    "        'rgb': {\n",
    "            'data_cols': ['R', 'G', 'B', 'Lumin'],\n",
    "            'std_cols': ['R_std', 'G_std', 'B_std', 'Lumin_std'],\n",
    "            'depth_col': 'SB_DEPTH_cm'\n",
    "        },\n",
    "        'mst': {\n",
    "            'density': {'data_col': 'Den_gm/cc', 'depth_col': 'SB_DEPTH_cm'},\n",
    "            'pwvel': {'data_col': 'PWVel_m/s', 'depth_col': 'SB_DEPTH_cm'},\n",
    "            'pwamp': {'data_col': 'PWAmp', 'depth_col': 'SB_DEPTH_cm'},\n",
    "            'elecres': {'data_col': 'ElecRes_ohmm', 'depth_col': 'SB_DEPTH_cm'},\n",
    "            'ms': {'data_col': 'MS', 'depth_col': 'SB_DEPTH_cm'}\n",
    "        },\n",
    "        'hrms': {'data_col': 'hiresMS', 'depth_col': 'SB_DEPTH_cm'}\n",
    "    },\n",
    "\n",
    "    # Thresholds for data cleaning\n",
    "    'thresholds': {\n",
    "        'ms': ['>', 180, 1],\n",
    "        'pwvel': ['>=', 1077, 1], \n",
    "        'pwamp': ['>=', 30, 1],\n",
    "        'den': ['<', 1.14, 1],\n",
    "        'elecres': ['<', 0, 1],\n",
    "        'hiresms': ['<=', 19, 1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run data cleaning function\n",
    "print(\"Starting data cleaning...\")\n",
    "preprocess_core_data(data_config, shift_limit_multiplier=3.0)\n",
    "print(\"Data cleaning completed.\")\n",
    "\n",
    "# Plot processed logs using new function signature\n",
    "fig, axes = plot_core_logs(\n",
    "    data_config,                           # Data configuration containing all parameters\n",
    "    file_type='clean',                     # Type of data files to plot ('clean' or 'filled')\n",
    "    title=f'{core_name} Cleaned Logs'      # Title for the plot figure\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-based data gap filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_fill_logs(data_config,              # Data configuration containing all parameters\n",
    "                      ml_method='xgblgbm')      # Available ml_method options: 'rf', 'rftc', 'xgb', 'xgblgbm'\n",
    "                                                # - 'rf': Random Forest ML\n",
    "                                                # - 'rftc': Random Forest ML with trend constraints\n",
    "                                                # - 'xgb': XGBoost ML\n",
    "                                                # - 'xgblgbm': XGBoost + LightGBM ML         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ML-based gap-filled log diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ML-based gap-filled log diagram\n",
    "fig, axes = plot_core_logs(\n",
    "    data_config,                                              # Data configuration containing all parameters\n",
    "    file_type='filled',                                       # Type of data files to plot ('filled' for gap-filled data)\n",
    "    title=f'{core_name} XGBoost + LightGBM ML-Filled Logs'    # Title for the plot figure\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
