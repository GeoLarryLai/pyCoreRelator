{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Log Data Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as mcoll  # For PolyCollection\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from scipy.signal import correlate, find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Create interaction features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Data Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_file_paths(data_config, file_type='clean'):\n",
    "    \"\"\"Build file paths from data_config patterns\"\"\"\n",
    "    suffix = data_config['clean_file_suffix'] if file_type == 'clean' else data_config['filled_file_suffix']\n",
    "    folder = data_config['clean_output_folder'] if file_type == 'clean' else data_config['filled_output_folder']\n",
    "    \n",
    "    paths = {}\n",
    "    for key, pattern in data_config['file_patterns'].items():\n",
    "        filename = pattern.format(core_name=data_config['core_name'], suffix=suffix)\n",
    "        paths[key] = data_config['mother_dir'] + folder + filename\n",
    "    return paths\n",
    "\n",
    "def load_core_images(data_config):\n",
    "    \"\"\"Load CT and RGB images from data_config patterns\"\"\"\n",
    "    ct_img_path = data_config['mother_dir'] + data_config['image_folder_pattern'].format(\n",
    "        core_name=data_config['core_name']) + data_config['ct_image_pattern'].format(\n",
    "        core_name=data_config['core_name'])\n",
    "    rgb_img_path = data_config['mother_dir'] + data_config['image_folder_pattern'].format(\n",
    "        core_name=data_config['core_name']) + data_config['rgb_image_pattern'].format(\n",
    "        core_name=data_config['core_name'])\n",
    "    \n",
    "    ct_img = plt.imread(ct_img_path)\n",
    "    rgb_img = plt.imread(rgb_img_path)\n",
    "    return ct_img, rgb_img\n",
    "\n",
    "def setup_execution_variables(data_config):\n",
    "    \"\"\"\n",
    "    Extract execution variables from data_config using helper functions.\n",
    "    \n",
    "    Args:\n",
    "        data_config (dict): Configuration dictionary containing all necessary parameters\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (core_name, mother_dir, core_length, clean_output_folder, filled_output_folder, \n",
    "                clean_data_paths, filled_data_paths, ct_img, rgb_img, column_configs)\n",
    "    \"\"\"\n",
    "    # Extract basic variables\n",
    "    core_name = data_config['core_name']\n",
    "    mother_dir = data_config['mother_dir']\n",
    "    core_length = data_config['core_length']\n",
    "    clean_output_folder = data_config['clean_output_folder']\n",
    "    filled_output_folder = data_config['filled_output_folder']\n",
    "    \n",
    "    # Build file paths using helper function\n",
    "    clean_data_paths = build_file_paths(data_config, 'clean')\n",
    "    filled_data_paths = build_file_paths(data_config, 'filled')\n",
    "    \n",
    "    # Load images using helper function\n",
    "    ct_img, rgb_img = load_core_images(data_config)\n",
    "    \n",
    "    # Extract column configurations\n",
    "    column_configs = data_config['column_configs']\n",
    "    \n",
    "    return (core_name, mother_dir, core_length, clean_output_folder, filled_output_folder,\n",
    "            clean_data_paths, filled_data_paths, ct_img, rgb_img, column_configs)\n",
    "\n",
    "def setup_execution_variables(data_config):\n",
    "    \"\"\"\n",
    "    Extract and setup execution variables from data_config.\n",
    "    \n",
    "    Args:\n",
    "        data_config (dict): Dictionary containing configuration parameters\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (core_name, mother_dir, core_length, clean_output_folder, filled_output_folder, \n",
    "                clean_data_paths, filled_data_paths, ct_img, rgb_img, column_configs)\n",
    "    \"\"\"\n",
    "    # Extract variables from data config\n",
    "    core_name = data_config['core_name']\n",
    "    mother_dir = data_config['mother_dir']\n",
    "    core_length = data_config['core_length']\n",
    "    clean_output_folder = data_config['clean_output_folder']\n",
    "    filled_output_folder = data_config['filled_output_folder']\n",
    "\n",
    "    # Build paths using the helper function\n",
    "    clean_data_paths = build_file_paths(data_config, 'clean')\n",
    "    filled_data_paths = build_file_paths(data_config, 'filled')\n",
    "\n",
    "    # Load images using the helper function\n",
    "    ct_img, rgb_img = load_core_images(data_config)\n",
    "\n",
    "    # Get column configurations from config\n",
    "    column_configs = data_config['column_configs']\n",
    "    \n",
    "    return (core_name, mother_dir, core_length, clean_output_folder, filled_output_folder,\n",
    "            clean_data_paths, filled_data_paths, ct_img, rgb_img, column_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions for Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for cleaning artifacts and noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_peak_valley_shift(signal_a, signal_b, common_depth):\n",
    "    \"\"\"\n",
    "    Computes a candidate shift based on matching the 2 largest peaks.\n",
    "    \n",
    "    For peaks:\n",
    "      - Finds all local maxima in each signal\n",
    "      - Sorts them by amplitude (largest first), selects the top 2 peaks\n",
    "      - Sorts these peaks by depth and computes the average depth difference\n",
    "    \n",
    "    Args:\n",
    "        signal_a (ndarray): First signal (e.g., HRMS segment)\n",
    "        signal_b (ndarray): Second signal (e.g., density, MS, or CT)\n",
    "        common_depth (ndarray): Depth grid onto which the signals are interpolated\n",
    "        \n",
    "    Returns:\n",
    "        float: The candidate shift (in depth units) based on peak matching\n",
    "    \"\"\"\n",
    "    # Find peaks in each signal\n",
    "    peaks_a, _ = find_peaks(signal_a)\n",
    "    peaks_b, _ = find_peaks(signal_b)\n",
    "    \n",
    "    # If no peaks found, use global maximum\n",
    "    if len(peaks_a) == 0:\n",
    "        peaks_a = np.array([np.argmax(signal_a)])\n",
    "    if len(peaks_b) == 0:\n",
    "        peaks_b = np.array([np.argmax(signal_b)])\n",
    "    \n",
    "    # Sort peaks by amplitude (largest first) and take top 2\n",
    "    sorted_peaks_a = sorted(peaks_a, key=lambda i: signal_a[i], reverse=True)[:2]\n",
    "    sorted_peaks_b = sorted(peaks_b, key=lambda i: signal_b[i], reverse=True)[:2]\n",
    "    \n",
    "    # Sort selected peaks by depth\n",
    "    top_peaks_a = sorted(sorted_peaks_a)\n",
    "    top_peaks_b = sorted(sorted_peaks_b)\n",
    "    \n",
    "    # Calculate peak shifts\n",
    "    n_peaks = min(len(top_peaks_a), len(top_peaks_b))\n",
    "    if n_peaks > 0:\n",
    "        peak_shifts = [common_depth[top_peaks_b[i]] - common_depth[top_peaks_a[i]] \n",
    "                      for i in range(n_peaks)]\n",
    "        candidate_shift = np.mean(peak_shifts)\n",
    "    else:\n",
    "        candidate_shift = 0.0\n",
    "        \n",
    "    return candidate_shift\n",
    "\n",
    "def compute_candidate_shift(signal_a, signal_b, common_depth, \n",
    "                            w_corr=0.2, w_peak=0.7):\n",
    "    \"\"\"\n",
    "    Compute a candidate depth shift between two signals based on:\n",
    "      - Cross-correlation, and\n",
    "      - Matching top 2 peaks\n",
    "    \n",
    "    The final candidate shift is a weighted combination of these two methods.\n",
    "    \n",
    "    Args:\n",
    "        signal_a (ndarray): First signal (e.g., HRMS segment).\n",
    "        signal_b (ndarray): Second signal (e.g., density, MS, or CT).\n",
    "        common_depth (ndarray): Depth grid onto which both signals are interpolated.\n",
    "        w_corr (float): Weight for the cross-correlation candidate.\n",
    "        w_peak (float): Weight for the peak/valley candidate.\n",
    "        \n",
    "    Returns:\n",
    "        float: The weighted candidate shift (in depth units).\n",
    "    \"\"\"\n",
    "    # --- Candidate 1: Cross-correlation shift ---\n",
    "    # Apply smoothing using a Gaussian filter\n",
    "    window = 3  # Window size for smoothing\n",
    "    a_smoothed = gaussian_filter1d(signal_a, sigma=window)\n",
    "    b_smoothed = gaussian_filter1d(signal_b, sigma=window)\n",
    "    \n",
    "    # Detrend the smoothed signals\n",
    "    a_detrended = a_smoothed - np.mean(a_smoothed)\n",
    "    b_detrended = b_smoothed - np.mean(b_smoothed)\n",
    "    \n",
    "    # Calculate cross-correlation\n",
    "    corr = correlate(a_detrended, b_detrended, mode='full')\n",
    "    lags = np.arange(-len(common_depth) + 1, len(common_depth))\n",
    "    best_lag = lags[np.argmax(corr)]\n",
    "    \n",
    "    # Handle case where common_depth is too short\n",
    "    try:\n",
    "        depth_step = common_depth[1] - common_depth[0]\n",
    "        cross_corr_shift = best_lag * depth_step\n",
    "    except IndexError:\n",
    "        cross_corr_shift = 0.0\n",
    "        w_corr = 0.0  # Zero out the weight for cross-correlation\n",
    "\n",
    "    # --- Candidate 2: Peak & Valley shift ---\n",
    "    candidate_peak_valley = compute_peak_valley_shift(signal_a, signal_b, common_depth)\n",
    "\n",
    "    # Return the weighted combination.\n",
    "    return w_corr * cross_corr_shift + w_peak * candidate_peak_valley\n",
    "\n",
    "def preprocess_core_data(data_config, shift_limit_multiplier=3.0):\n",
    "    \"\"\"\n",
    "    Preprocess core data by cleaning and scaling depth values.\n",
    "    \n",
    "    Args:\n",
    "        data_config (dict): Dictionary containing configuration parameters including:\n",
    "            - mother_dir (str): Source mother directory path\n",
    "            - core_name (str): Name of the core\n",
    "            - core_length (float): Length of the core in cm\n",
    "            - data_folder (str): Path to input data folder\n",
    "            - clean_output_folder (str): Path to output folder for cleaned data\n",
    "            - file_patterns (dict): Dictionary mapping data types to filename patterns\n",
    "            - column_configs (dict): Dictionary containing column configurations for each data type\n",
    "            - thresholds (dict): Dictionary with measurement keys and [condition, value, buffer] values where:\n",
    "                                - Measurement keys can be: 'ct', 'r', 'g', 'b', 'lumin', 'ms', 'pwvel', 'pwamp', 'den', 'elecres', 'hiresms'\n",
    "                                - condition must be one of: '>', '<', '<=', '>='\n",
    "                                - value is the threshold number to compare against\n",
    "                                - buffer is the number of surrounding points to also mark as invalid\n",
    "                                Example: {'ct': ['>', 2000, 5], 'ms': ['>', 150, 1], 'pwvel': ['>', 1085, 3]}\n",
    "    \"\"\"\n",
    "    # Validate threshold conditions\n",
    "    valid_conditions = ['>', '<', '<=', '>=']\n",
    "    for param, threshold in data_config.get('thresholds', {}).items():\n",
    "        if threshold[0] not in valid_conditions:\n",
    "            raise ValueError(f\"Invalid condition '{threshold[0]}' for {param}. Must be one of: {valid_conditions}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(data_config['mother_dir'] + data_config['clean_output_folder'], exist_ok=True)\n",
    "    \n",
    "    # Extract variables from data_config\n",
    "    file_patterns = data_config.get('file_patterns', {})\n",
    "    column_configs = data_config.get('column_configs', {})\n",
    "    thresholds = data_config.get('thresholds', {})\n",
    "    \n",
    "    # Initialize data variables\n",
    "    loaded_data = {}\n",
    "\n",
    "    # Process each data type defined in file_patterns\n",
    "    for data_type, pattern in file_patterns.items():\n",
    "        if data_type not in column_configs:\n",
    "            continue\n",
    "            \n",
    "        # Skip HRMS for now as it needs special handling\n",
    "        if data_type == 'hrms':\n",
    "            continue\n",
    "            \n",
    "        # Build file path\n",
    "        filename = pattern.format(core_name=data_config['core_name'], suffix='.csv')\n",
    "        \n",
    "        # Determine subfolder based on data type and core name\n",
    "        if data_type == 'mst':\n",
    "            if data_config['core_name'].startswith('M99'):\n",
    "                subfolder = \"OSU orignal dataset/R-V_Melville99/Calibrated_MST/\"\n",
    "            elif data_config['core_name'].startswith('RR02'):\n",
    "                subfolder = \"OSU orignal dataset/R-V_Revelle02/Calibrated_MST/\"\n",
    "            else:\n",
    "                subfolder = \"OSU orignal dataset/R-V_Melville99/Calibrated_MST/\"\n",
    "            file_path = data_config['mother_dir'] + subfolder + filename\n",
    "        else:\n",
    "            file_path = data_config['mother_dir'] + data_config.get('data_folder', '') + filename\n",
    "        \n",
    "        # Try to load the file\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                loaded_data[data_type] = pd.read_csv(file_path).astype('float64')\n",
    "                print(f\"Loaded {data_type} data from {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load {data_type} data from {filename}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"Warning: {data_type} file not found: {filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Process the loaded data\n",
    "        data = loaded_data[data_type]\n",
    "        config = column_configs[data_type]\n",
    "        \n",
    "        # Handle different data types\n",
    "        if data_type == 'ct':\n",
    "            # Process CT data\n",
    "            data_col = config['data_col']\n",
    "            std_col = config['std_col']\n",
    "            if 'ct' in thresholds:\n",
    "                condition, threshold_value, buffer_size = thresholds['ct']\n",
    "                invalid_values = eval(f\"data['{data_col}'] {condition} {threshold_value}\")\n",
    "                buffer_indices = []\n",
    "                for i in range(len(data)):\n",
    "                    if invalid_values[i]:\n",
    "                        buffer_indices.extend(range(max(0, i-buffer_size), min(len(data), i+buffer_size+1)))\n",
    "                data.loc[buffer_indices, [data_col, std_col]] = np.nan\n",
    "                \n",
    "        elif data_type == 'rgb':\n",
    "            # Process RGB data\n",
    "            data_columns = config['data_cols']\n",
    "            std_columns = config['std_cols']\n",
    "            \n",
    "            # Apply thresholds for each RGB component\n",
    "            for col in data_columns:\n",
    "                threshold_key = col.lower()  # Convert to lowercase for matching\n",
    "                if threshold_key in thresholds:\n",
    "                    condition, threshold_value, buffer_size = thresholds[threshold_key]\n",
    "                    invalid_values = eval(f\"data['{col}'] {condition} {threshold_value}\")\n",
    "                    buffer_indices = []\n",
    "                    for i in range(len(data)):\n",
    "                        if invalid_values[i]:\n",
    "                            buffer_indices.extend(range(max(0, i-buffer_size), min(len(data), i+buffer_size+1)))\n",
    "                    \n",
    "                    # Apply to both data and std columns for this component\n",
    "                    if col in data_columns:\n",
    "                        col_idx = data_columns.index(col)\n",
    "                        if col_idx < len(std_columns):\n",
    "                            data.loc[buffer_indices, [col, std_columns[col_idx]]] = np.nan\n",
    "                \n",
    "        elif data_type == 'mst':\n",
    "            # Process MST data - handle density extreme indices for MS data first\n",
    "            density_extreme_indices = []\n",
    "            if 'density' in config and 'den' in thresholds:\n",
    "                density_col = config['density']['data_col']\n",
    "                if density_col in data.columns:\n",
    "                    condition, threshold_value, buffer_size = thresholds['den']\n",
    "                    density_extreme = eval(f\"data['{density_col}'] {condition} {threshold_value}\")\n",
    "                    for i in range(len(data)):\n",
    "                        if density_extreme[i]:\n",
    "                            density_extreme_indices.extend(range(max(0, i-buffer_size), min(len(data), i+buffer_size+1)))\n",
    "\n",
    "            # Process each MST measurement\n",
    "            for measurement, measurement_config in config.items():\n",
    "                if isinstance(measurement_config, dict) and 'data_col' in measurement_config:\n",
    "                    column = measurement_config['data_col']\n",
    "                    \n",
    "                    if column in data.columns and measurement in thresholds:\n",
    "                        condition, threshold_value, buffer_size = thresholds[measurement]\n",
    "                        extreme_values = eval(f\"data['{column}'] {condition} {threshold_value}\")\n",
    "                        \n",
    "                        buffer_indices = []\n",
    "                        for i in range(len(data)):\n",
    "                            if extreme_values[i]:\n",
    "                                buffer_indices.extend(range(max(0, i-buffer_size), min(len(data), i+buffer_size+1)))\n",
    "                        \n",
    "                        # Add density extreme indices to MS data\n",
    "                        if measurement == 'ms':\n",
    "                            buffer_indices.extend(density_extreme_indices)\n",
    "                        \n",
    "                        data.loc[buffer_indices, column] = np.nan\n",
    "        \n",
    "        # Scale depth and save processed data\n",
    "        if not data.drop('SB_DEPTH_cm', axis=1).isna().all().all():\n",
    "            depth_scale = data_config['core_length'] / data['SB_DEPTH_cm'].max()\n",
    "            data['SB_DEPTH_cm'] = data['SB_DEPTH_cm'] * depth_scale\n",
    "            output_filename = pattern.format(core_name=data_config['core_name'], suffix=data_config['clean_file_suffix'])\n",
    "            data.to_csv(data_config['mother_dir'] + data_config['clean_output_folder'] + output_filename, index=False)\n",
    "            print(f\"Saved processed {data_type} data to {output_filename}\")\n",
    "\n",
    "    # Handle HRMS data separately due to special processing requirements\n",
    "    if 'hrms' in file_patterns and 'hrms' in column_configs:\n",
    "        # Determine HRMS subfolder\n",
    "        if data_config['core_name'].startswith('M99'):\n",
    "            hrms_subfolder = \"OSU orignal dataset/R-V_Melville99/M9907_point_mag/\"\n",
    "        elif data_config['core_name'].startswith('RR02'):\n",
    "            hrms_subfolder = \"OSU orignal dataset/R-V_Revelle02/RR0207_point_mag/\"\n",
    "        else:\n",
    "            hrms_subfolder = \"OSU orignal dataset/R-V_Melville99/M9907_point_mag/\"\n",
    "\n",
    "        # HRMS files use different naming convention (_ptMS.csv)\n",
    "        hrms_path = data_config['mother_dir'] + hrms_subfolder + f\"{data_config['core_name']}_ptMS.csv\"\n",
    "        \n",
    "        if os.path.exists(hrms_path):\n",
    "            try:\n",
    "                hrms_data = pd.read_csv(hrms_path).astype('float64')\n",
    "                print(f\"Loaded HRMS data from {data_config['core_name']}_ptMS.csv\")\n",
    "                \n",
    "                # Process Hi-res MS data only if both HRMS data and at least one reference curve exist\n",
    "                hrms_config = column_configs['hrms']\n",
    "                hrms_col = hrms_config['data_col']\n",
    "                \n",
    "                # Get reference data columns\n",
    "                density_col = None\n",
    "                ct_col = None\n",
    "                ct_data = loaded_data.get('ct')\n",
    "                mst_data = loaded_data.get('mst')\n",
    "                \n",
    "                if 'mst' in column_configs and mst_data is not None and 'density' in column_configs['mst']:\n",
    "                    density_col = column_configs['mst']['density']['data_col']\n",
    "                if 'ct' in column_configs and ct_data is not None:\n",
    "                    ct_col = column_configs['ct']['data_col']\n",
    "                \n",
    "                if ct_data is not None or (mst_data is not None and density_col and density_col in mst_data.columns):\n",
    "                    hrms_depth = hrms_data['SB_DEPTH_cm'].values\n",
    "                    \n",
    "                    # Resample reference data if available\n",
    "                    density_resampled = None\n",
    "                    if mst_data is not None and density_col and density_col in mst_data.columns:\n",
    "                        density_resampled = np.interp(hrms_depth, \n",
    "                                                    mst_data['SB_DEPTH_cm'].values,\n",
    "                                                    mst_data[density_col].values)\n",
    "                    else:\n",
    "                        density_resampled = np.full_like(hrms_depth, np.nan)\n",
    "                        \n",
    "                    ct_resampled = None\n",
    "                    if ct_data is not None and ct_col:\n",
    "                        ct_resampled = np.interp(hrms_depth,\n",
    "                                               ct_data['SB_DEPTH_cm'].values,\n",
    "                                               ct_data[ct_col].values)\n",
    "                    else:\n",
    "                        ct_resampled = np.full_like(hrms_depth, np.nan)\n",
    "\n",
    "                    # Apply thresholds to HRMS data\n",
    "                    if 'hiresms' in thresholds:\n",
    "                        condition, threshold_value, buffer_size = thresholds['hiresms']\n",
    "                        extreme_values = eval(f\"hrms_data['{hrms_col}'] {condition} {threshold_value}\")\n",
    "                        buffer_indices = []\n",
    "                        for i in range(len(hrms_data)):\n",
    "                            if extreme_values[i]:\n",
    "                                buffer_indices.extend(range(max(0, i - buffer_size), min(len(hrms_data), i + buffer_size + 1)))\n",
    "                        hrms_data.loc[buffer_indices, hrms_col] = np.nan\n",
    "\n",
    "                    # Identify continuous segments in HRMS data\n",
    "                    valid_indices = hrms_data.index[hrms_data[hrms_col].notna()].tolist()\n",
    "                    segments = []\n",
    "                    if valid_indices:\n",
    "                        current_segment = [valid_indices[0]]\n",
    "                        for idx in valid_indices[1:]:\n",
    "                            if idx == current_segment[-1] + 1:\n",
    "                                current_segment.append(idx)\n",
    "                            else:\n",
    "                                segments.append(current_segment)\n",
    "                                current_segment = [idx]\n",
    "                        segments.append(current_segment)\n",
    "\n",
    "                    # Process each HRMS segment\n",
    "                    for seg in segments:\n",
    "                        seg_depth = hrms_data.loc[seg, 'SB_DEPTH_cm']\n",
    "                        seg_values = hrms_data.loc[seg, hrms_col]\n",
    "                        if seg_values.empty:\n",
    "                            continue\n",
    "\n",
    "                        # Get resampled data for this segment\n",
    "                        seg_density = density_resampled[seg] if density_resampled is not None else None\n",
    "                        seg_ct = ct_resampled[seg] if ct_resampled is not None else None\n",
    "\n",
    "                        # Check if at least one reference curve has valid data\n",
    "                        if (seg_density is None or np.all(np.isnan(seg_density))) and (seg_ct is None or np.all(np.isnan(seg_ct))):\n",
    "                            print(f\"Warning: No valid reference data for segment at depth {seg_depth.iloc[0]:.2f}\")\n",
    "                            continue\n",
    "\n",
    "                        # Calculate candidate shifts\n",
    "                        candidate_shift_density = 0.0\n",
    "                        corr_density = 0.0\n",
    "                        if seg_density is not None and not np.all(np.isnan(seg_density)):\n",
    "                            candidate_shift_density = compute_candidate_shift(seg_values.values, \n",
    "                                                                           seg_density,\n",
    "                                                                           seg_depth.values)\n",
    "                            corr_density = np.abs(np.corrcoef(seg_values.values, seg_density)[0,1])\n",
    "                            if np.isnan(corr_density): corr_density = 0.0\n",
    "\n",
    "                        candidate_shift_ct = 0.0\n",
    "                        corr_ct = 0.0\n",
    "                        if seg_ct is not None and not np.all(np.isnan(seg_ct)):\n",
    "                            candidate_shift_ct = compute_candidate_shift(seg_values.values,\n",
    "                                                                      seg_ct, \n",
    "                                                                      seg_depth.values)\n",
    "                            corr_ct = np.abs(np.corrcoef(seg_values.values, seg_ct)[0,1])\n",
    "                            if np.isnan(corr_ct): corr_ct = 0.0\n",
    "\n",
    "                        # Determine the maximum allowed shift based on neighboring gaps\n",
    "                        if seg[0] > 0:\n",
    "                            gap_before = hrms_data.at[seg[0], 'SB_DEPTH_cm'] - hrms_data.at[seg[0]-1, 'SB_DEPTH_cm']\n",
    "                        else:\n",
    "                            gap_before = np.inf\n",
    "                        if seg[-1] < len(hrms_data) - 1:\n",
    "                            gap_after = hrms_data.at[seg[-1]+1, 'SB_DEPTH_cm'] - hrms_data.at[seg[-1], 'SB_DEPTH_cm']\n",
    "                        else:\n",
    "                            gap_after = np.inf\n",
    "\n",
    "                        gap_based_shift = min(gap_before, gap_after) * shift_limit_multiplier\n",
    "\n",
    "                        # Calculate consensus shift\n",
    "                        if abs(candidate_shift_density) > gap_based_shift or corr_density < 0:\n",
    "                            corr_density = 0.0\n",
    "                        if abs(candidate_shift_ct) > gap_based_shift or corr_ct < 0:\n",
    "                            corr_ct = 0.0\n",
    "\n",
    "                        total_corr = corr_density + corr_ct\n",
    "                        if total_corr > 0:\n",
    "                            w_density = corr_density / total_corr\n",
    "                            w_ct = corr_ct / total_corr\n",
    "                        else:\n",
    "                            w_density, w_ct = 0.4, 0.6\n",
    "\n",
    "                        consensus_shift = (w_density * candidate_shift_density + \n",
    "                                         w_ct * candidate_shift_ct)\n",
    "\n",
    "                        # Apply the consensus shift\n",
    "                        if abs(consensus_shift) <= gap_based_shift:\n",
    "                            target_indices = []\n",
    "                            for idx in seg:\n",
    "                                current_depth = hrms_data.at[idx, 'SB_DEPTH_cm']\n",
    "                                target_depth = current_depth + consensus_shift\n",
    "                                target_idx = (hrms_data['SB_DEPTH_cm'] - target_depth).abs().idxmin()\n",
    "                                target_indices.append(target_idx)\n",
    "                            \n",
    "                            original_values = hrms_data.loc[seg, hrms_col].copy()\n",
    "                            hrms_data.loc[seg, hrms_col] = np.nan\n",
    "                            \n",
    "                            for orig_val, target_idx in zip(original_values, target_indices):\n",
    "                                hrms_data.at[target_idx, hrms_col] = orig_val\n",
    "                        else:\n",
    "                            print(f\"Warning: Computed shift ({consensus_shift:.2f}) exceeds gap-based shift limit ({gap_based_shift:.2f}) \"\n",
    "                                  f\"for segment at depths {seg_depth.iloc[0]:.2f}-{seg_depth.iloc[-1]:.2f}\")\n",
    "\n",
    "                    # Rescale the depth after shifting\n",
    "                    depth_scale_factor = data_config['core_length'] / hrms_data['SB_DEPTH_cm'].max()\n",
    "                    hrms_data['SB_DEPTH_cm'] = hrms_data['SB_DEPTH_cm'] * depth_scale_factor\n",
    "                    hrms_output_filename = file_patterns['hrms'].format(core_name=data_config['core_name'], suffix=data_config['clean_file_suffix'])\n",
    "                    hrms_data.to_csv(data_config['mother_dir'] + data_config['clean_output_folder'] + hrms_output_filename, index=False)\n",
    "                    print(f\"Saved processed HRMS data to {hrms_output_filename}\")\n",
    "                else:\n",
    "                    print(\"Warning: No reference data available for HRMS processing\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not process HRMS data: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: HRMS file not found: {data_config['core_name']}_ptMS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for plotting cleanned core images and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_core_logs(data_config, file_type='clean', title=None):\n",
    "    \"\"\"\n",
    "    Plot core logs from multiple data sources using data_config.\n",
    "    \n",
    "    Args:\n",
    "        data_config (dict): Dictionary containing configuration parameters including:\n",
    "            - mother_dir (str): Source mother directory path\n",
    "            - core_name (str): Name of the core\n",
    "            - core_length (float): Length of the core in cm\n",
    "            - file_patterns (dict): Dictionary mapping data types to filename patterns\n",
    "            - column_configs (dict): Dictionary containing column configurations for each data type\n",
    "            - image_folder_pattern (str): Pattern for image folder path\n",
    "            - ct_image_pattern (str): Pattern for CT image filename\n",
    "            - rgb_image_pattern (str): Pattern for RGB image filename\n",
    "        file_type (str): Type of files to plot ('clean' or 'filled')\n",
    "        title (str, optional): Title for the plot. If None, auto-generates based on core_name and file_type\n",
    "    \"\"\"\n",
    "    # Extract variables from data_config\n",
    "    core_length = data_config['core_length']\n",
    "    column_configs = data_config['column_configs']\n",
    "    \n",
    "    # Build file paths using the helper function\n",
    "    data_paths = build_file_paths(data_config, file_type)\n",
    "    \n",
    "    # Load images using the helper function\n",
    "    ct_img, rgb_img = load_core_images(data_config)\n",
    "    \n",
    "    # Auto-generate title if not provided\n",
    "    if title is None:\n",
    "        title_suffix = 'Cleaned' if file_type == 'clean' else 'ML-Filled'\n",
    "        title = f\"{data_config['core_name']} {title_suffix} Logs\"\n",
    "    # Load data\n",
    "    data = {}\n",
    "    for key, path in data_paths.items():\n",
    "        if path:\n",
    "            data[key] = pd.read_csv(path)\n",
    "            if 'SB_DEPTH_cm' not in data[key].columns:\n",
    "                raise ValueError(f\"SB_DEPTH_cm column missing in {key} data\")\n",
    "    \n",
    "    # Calculate number of plots needed\n",
    "    n_plots = 0  # Start with 0\n",
    "    \n",
    "    # Add CT panels if CT image and data exist\n",
    "    if ct_img is not None and 'ct' in data:\n",
    "        n_plots += 2  # CT image and data\n",
    "        \n",
    "    # Add RGB panels if RGB image and data exist  \n",
    "    if rgb_img is not None and 'rgb' in data:\n",
    "        n_plots += 2  # RGB image and data\n",
    "        \n",
    "    # Add MS panel if either MST MS data or hiresMS data exists and has data\n",
    "    if ('mst' in data and column_configs['mst']['ms']['data_col'] in data['mst'].columns and not data['mst'][column_configs['mst']['ms']['data_col']].isna().all()) or ('hrms' in data and not data['hrms'][column_configs['hrms']['data_col']].isna().all()):\n",
    "        n_plots += 1\n",
    "        \n",
    "    # Add panels for other MST logs that exist and have data\n",
    "    if 'mst' in data:\n",
    "        for log_type, config in column_configs['mst'].items():\n",
    "            if log_type != 'ms':  # Skip MS since it's handled separately\n",
    "                if config['data_col'] in data['mst'].columns and not data['mst'][config['data_col']].isna().all():\n",
    "                    n_plots += 1\n",
    "\n",
    "    if n_plots == 0:\n",
    "        raise ValueError(\"No valid data to plot\")\n",
    "    \n",
    "    # Create figure and axes\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(10, 16), sharey=True)\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable\n",
    "    fig.suptitle(title, fontweight='bold')\n",
    "    \n",
    "    current_ax = 0\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Plot CT image and data\n",
    "    # ---------------------------\n",
    "    if ct_img is not None and 'ct' in data:\n",
    "        # Plot CT image\n",
    "        axes[current_ax].imshow(ct_img, aspect='auto', extent=[0, 0.5, core_length, 0])\n",
    "        axes[current_ax].set_ylabel('Depth (cm)')\n",
    "        axes[current_ax].set_xticks([])\n",
    "        axes[current_ax].set_xlabel('Sediment\\nCore\\nCT Scan', fontweight='bold', fontsize='small')\n",
    "        current_ax += 1\n",
    "        \n",
    "        # Plot CT data\n",
    "        ct_col = column_configs['ct']['data_col']\n",
    "        ct_std = column_configs['ct']['std_col']\n",
    "        ct_depth = data['ct'][column_configs['ct']['depth_col']].astype(np.float64)\n",
    "        \n",
    "        axes[current_ax].plot(data['ct'][ct_col].astype(np.float64), ct_depth, \n",
    "                              color='black', linewidth=0.7)\n",
    "        \n",
    "        # Standard deviation fill\n",
    "        axes[current_ax].fill_betweenx(\n",
    "            ct_depth,\n",
    "            data['ct'][ct_col].astype(np.float64) - data['ct'][ct_std].astype(np.float64),\n",
    "            data['ct'][ct_col].astype(np.float64) + data['ct'][ct_std].astype(np.float64),\n",
    "            color='black', alpha=0.2, linewidth=0\n",
    "        )\n",
    "        \n",
    "        # Color-coded CT values using PolyCollection\n",
    "        ct_values = data['ct'][ct_col].astype(np.float64).values\n",
    "        depths = ct_depth.values\n",
    "        norm = plt.Normalize(300, 1600)\n",
    "        cmap = plt.cm.jet\n",
    "        \n",
    "        ct_polys = []\n",
    "        ct_facecolors = []\n",
    "        for i in range(len(depths) - 1):\n",
    "            # Ignore segments with NaN values\n",
    "            if not (np.isnan(ct_values[i]) or np.isnan(ct_values[i+1])):\n",
    "                poly = [\n",
    "                    (0, depths[i]),\n",
    "                    (ct_values[i], depths[i]),\n",
    "                    (ct_values[i+1], depths[i+1]),\n",
    "                    (0, depths[i+1])\n",
    "                ]\n",
    "                ct_polys.append(poly)\n",
    "                # Use the average value for smoother color transition\n",
    "                avg_val = (ct_values[i] + ct_values[i+1]) / 2\n",
    "                ct_facecolors.append(cmap(norm(avg_val)))\n",
    "                \n",
    "        if ct_polys:\n",
    "            pc_ct = mcoll.PolyCollection(ct_polys, facecolors=ct_facecolors, edgecolors='none', alpha=0.95)\n",
    "            axes[current_ax].add_collection(pc_ct)\n",
    "        \n",
    "        axes[current_ax].set_xlabel('CT#\\nBrightness', fontweight='bold', fontsize='small')\n",
    "        axes[current_ax].grid(True)\n",
    "        axes[current_ax].set_xlim(300, None)\n",
    "        axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "        current_ax += 1\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Plot RGB image and data\n",
    "    # ---------------------------\n",
    "    if rgb_img is not None and 'rgb' in data:\n",
    "        # Plot RGB image\n",
    "        axes[current_ax].imshow(rgb_img, aspect='auto', extent=[0, 0.5, core_length, 0])\n",
    "        axes[current_ax].set_xticks([])\n",
    "        axes[current_ax].set_xlabel('Sediment\\nCore\\nPhoto', fontweight='bold', fontsize='small')\n",
    "        current_ax += 1\n",
    "        \n",
    "        # Plot RGB data (R, G, B channels)\n",
    "        rgb_cols = column_configs['rgb']['data_cols']\n",
    "        rgb_stds = column_configs['rgb']['std_cols']\n",
    "        rgb_depth = data['rgb'][column_configs['rgb']['depth_col']].astype(np.float64)\n",
    "        colors = ['red', 'green', 'blue']\n",
    "        \n",
    "        for col, std, color in zip(rgb_cols[:3], rgb_stds[:3], colors):\n",
    "            axes[current_ax].plot(data['rgb'][col].astype(np.float64), rgb_depth,\n",
    "                                  color=color, linewidth=0.7)\n",
    "            axes[current_ax].fill_betweenx(\n",
    "                rgb_depth,\n",
    "                data['rgb'][col].astype(np.float64) - data['rgb'][std].astype(np.float64),\n",
    "                data['rgb'][col].astype(np.float64) + data['rgb'][std].astype(np.float64),\n",
    "                color=color, alpha=0.2, linewidth=0\n",
    "            )\n",
    "        \n",
    "        # Luminance plot using PolyCollection with Inferno colormap\n",
    "        lumin_values = data['rgb']['Lumin'].astype(np.float64).values\n",
    "        lumin_depths = rgb_depth.values\n",
    "        \n",
    "        # Compute normalization range ignoring NaNs\n",
    "        valid_lumin = lumin_values[~np.isnan(lumin_values)]\n",
    "        if len(valid_lumin) == 0:\n",
    "            vmin, vmax = 0, 1\n",
    "        else:\n",
    "            vmin, vmax = valid_lumin.min(), valid_lumin.max()\n",
    "            if np.isclose(vmin, vmax):\n",
    "                vmin, vmax = 0, 1\n",
    "        \n",
    "        lumin_norm = plt.Normalize(vmin, vmax)\n",
    "        cmap_inferno = plt.cm.inferno\n",
    "        \n",
    "        lumin_polys = []\n",
    "        lumin_facecolors = []\n",
    "        for i in range(len(lumin_depths) - 1):\n",
    "            # Only use segments with valid (non-NaN) endpoints\n",
    "            if not (np.isnan(lumin_values[i]) or np.isnan(lumin_values[i+1])):\n",
    "                poly = [\n",
    "                    (0, lumin_depths[i]),\n",
    "                    (lumin_values[i], lumin_depths[i]),\n",
    "                    (lumin_values[i+1], lumin_depths[i+1]),\n",
    "                    (0, lumin_depths[i+1])\n",
    "                ]\n",
    "                lumin_polys.append(poly)\n",
    "                # Use average value for color mapping\n",
    "                avg_val = (lumin_values[i] + lumin_values[i+1]) / 2\n",
    "                lumin_facecolors.append(cmap_inferno(lumin_norm(avg_val)))\n",
    "        if lumin_polys:\n",
    "            pc_lumin = mcoll.PolyCollection(lumin_polys, facecolors=lumin_facecolors, edgecolors='none', alpha=0.95)\n",
    "            axes[current_ax].add_collection(pc_lumin)\n",
    "        \n",
    "        axes[current_ax].set_xlabel('RGB\\nLuminance', fontweight='bold', fontsize='small')\n",
    "        axes[current_ax].grid(True)\n",
    "        axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "        current_ax += 1\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Plot MS and MST data if available\n",
    "    # ---------------------------\n",
    "    if 'mst' in data:\n",
    "        # Plot MS data if available\n",
    "        ms_col = column_configs['mst']['ms']['data_col']\n",
    "        has_mst_ms = 'mst' in data and ms_col in data['mst'].columns and not data['mst'][ms_col].isna().all()\n",
    "        has_hrms = 'hrms' in data and not data['hrms'][column_configs['hrms']['data_col']].isna().all()\n",
    "\n",
    "        if has_mst_ms or has_hrms:\n",
    "            if has_mst_ms:\n",
    "                axes[current_ax].plot(\n",
    "                    data['mst'][ms_col].astype(np.float64), \n",
    "                    data['mst'][column_configs['mst']['ms']['depth_col']].astype(np.float64),\n",
    "                    color='darkgray', label='Lo-res', linewidth=0.7\n",
    "                )\n",
    "            if has_hrms:\n",
    "                axes[current_ax].plot(\n",
    "                    data['hrms'][column_configs['hrms']['data_col']].astype(np.float64), \n",
    "                    data['hrms'][column_configs['hrms']['depth_col']].astype(np.float64),\n",
    "                    color='black', label='Hi-res', linewidth=0.7\n",
    "                )\n",
    "            axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "            axes[current_ax].set_xlabel('Magnetic\\nSusceptibility', fontweight='bold')\n",
    "            axes[current_ax].grid(True)\n",
    "            current_ax += 1\n",
    "\n",
    "        # Plot remaining MST logs that have data\n",
    "        mst_labels = {\n",
    "            'density': 'Density\\n(g/cc)',\n",
    "            'pwvel': 'P-wave\\nVelocity\\n(m/s)',\n",
    "            'pwamp': 'P-wave\\nAmplitude',\n",
    "            'elecres': 'Electrical\\nResistivity\\n(ohm-m)'\n",
    "        }\n",
    "        \n",
    "        mst_colors = {\n",
    "            'density': 'orange',\n",
    "            'pwvel': 'purple',\n",
    "            'pwamp': 'purple',\n",
    "            'elecres': 'brown'\n",
    "        }\n",
    "\n",
    "        for log_type, config in column_configs['mst'].items():\n",
    "            if log_type != 'ms' and config['data_col'] in data['mst'].columns and not data['mst'][config['data_col']].isna().all():\n",
    "                axes[current_ax].plot(\n",
    "                    data['mst'][config['data_col']].astype(np.float64), \n",
    "                    data['mst'][config['depth_col']].astype(np.float64), \n",
    "                    color=mst_colors.get(log_type, 'black'), \n",
    "                    linewidth=0.7\n",
    "                )\n",
    "                axes[current_ax].set_xlabel(mst_labels[log_type], fontweight='bold', fontsize='small')\n",
    "                axes[current_ax].tick_params(axis='x', labelsize='x-small')\n",
    "                axes[current_ax].grid(True)\n",
    "                if log_type == 'density':\n",
    "                    axes[current_ax].set_xlim(1, 2)\n",
    "                current_ax += 1\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Set common y-axis properties\n",
    "    # ---------------------------\n",
    "    for ax in axes:\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_ylim(core_length, 0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions for Machine Learning to fill data gaps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for plotting filled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filled_data(target_log, original_data, filled_data, core_length, core_name, ML_type = 'ML'):\n",
    "    \"\"\"\n",
    "    Plot original and ML-filled data for a given log.\n",
    "    \n",
    "    Args:\n",
    "        target_log (str): Name of the log to plot\n",
    "        original_data (pd.DataFrame): Original data containing the log\n",
    "        filled_data (pd.DataFrame): Data with ML-filled gaps\n",
    "        core_length (int): Length of the core in cm\n",
    "        core_name (str): Name of the core for plot title\n",
    "    \"\"\"\n",
    "    # Check if there are any gaps\n",
    "    has_gaps = original_data[target_log].isna().any()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(15, 3))\n",
    "    title_suffix = f'Use {ML_type} for Data Gap Filling' if has_gaps else \"(No Data Gap to be filled by ML)\"\n",
    "    fig.suptitle(f'{core_name} {target_log} Values {title_suffix}', fontweight='bold')\n",
    "\n",
    "    # Plot data with ML-predicted gaps only if gaps exist\n",
    "    if has_gaps:\n",
    "        ax.plot(filled_data['SB_DEPTH_cm'], filled_data[target_log], \n",
    "                color='red', label=f'ML Predicted {target_log}', linewidth=0.7, alpha=0.7)\n",
    "\n",
    "    # Plot original data\n",
    "    ax.plot(original_data['SB_DEPTH_cm'], original_data[target_log], \n",
    "            color='black', label=f'Original {target_log}', linewidth=0.7)\n",
    "\n",
    "    # Add uncertainty shade if std column exists\n",
    "    std_col = f'{target_log}_std'\n",
    "    if std_col in original_data.columns:\n",
    "        ax.fill_between(original_data['SB_DEPTH_cm'],\n",
    "                       original_data[target_log] - original_data[std_col],\n",
    "                       original_data[target_log] + original_data[std_col],\n",
    "                       color='black', alpha=0.2, linewidth=0)\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_ylabel(f'{target_log}\\nBrightness', fontweight='bold', fontsize='small')\n",
    "    ax.set_xlabel('Depth (cm)')\n",
    "    ax.grid(True)\n",
    "    ax.invert_xaxis()\n",
    "    ax.set_xlim(0, core_length)\n",
    "    ax.tick_params(axis='y', labelsize='x-small')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Machine Learning Data Gap filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for fill_gaps_with_ml\n",
    "\n",
    "def prepare_feature_data(target_log, All_logs, merge_tolerance):\n",
    "    \"\"\"Prepare merged feature data for ML training.\"\"\"\n",
    "    # Get target data from All_logs\n",
    "    target_data = None\n",
    "    for df, cols in All_logs.values():\n",
    "        if target_log in cols:\n",
    "            target_data = df.copy()\n",
    "            break\n",
    "    \n",
    "    if target_data is None:\n",
    "        raise ValueError(f\"Target log '{target_log}' not found in any dataset\")\n",
    "\n",
    "    # Convert SB_DEPTH_cm to float64 in target data\n",
    "    target_data['SB_DEPTH_cm'] = target_data['SB_DEPTH_cm'].astype('float64')\n",
    "\n",
    "    # Prepare training data by merging all available logs\n",
    "    merged_data = target_data[['SB_DEPTH_cm', target_log]].copy()\n",
    "    features = []\n",
    "    \n",
    "    # Merge feature dataframes one by one, using their own SB_DEPTH_cm column\n",
    "    for df_name, (df, cols) in All_logs.items():\n",
    "        if target_log not in cols:  # Skip the target dataframe\n",
    "            df = df.copy()\n",
    "            df['SB_DEPTH_cm'] = df['SB_DEPTH_cm'].astype('float64')\n",
    "            # Rename SB_DEPTH_cm temporarily to avoid conflicts during merging\n",
    "            df = df.rename(columns={'SB_DEPTH_cm': f'SB_DEPTH_cm_{df_name}'})\n",
    "            # Convert all numeric columns to float64\n",
    "            for col in cols:\n",
    "                if col != 'SB_DEPTH_cm' and df[col].dtype.kind in 'biufc':\n",
    "                    df[col] = df[col].astype('float64')\n",
    "            # Rename feature columns for merging\n",
    "            df_renamed = df.rename(columns={col: f'{df_name}_{col}' for col in cols if col != 'SB_DEPTH_cm'})\n",
    "            df_renamed = df_renamed.sort_values(f'SB_DEPTH_cm_{df_name}')\n",
    "            \n",
    "            # Perform merge_asof with tolerance for data alignment\n",
    "            merged_data = pd.merge_asof(\n",
    "                merged_data.sort_values('SB_DEPTH_cm'),\n",
    "                df_renamed,\n",
    "                left_on='SB_DEPTH_cm',\n",
    "                right_on=f'SB_DEPTH_cm_{df_name}',\n",
    "                direction='nearest',\n",
    "                tolerance=merge_tolerance\n",
    "            )\n",
    "            \n",
    "            # Check for unmatched rows due to the tolerance constraint\n",
    "            unmatched = merged_data[f'SB_DEPTH_cm_{df_name}'].isna().sum()\n",
    "            if unmatched > 0:\n",
    "                warnings.warn(f\"{unmatched} rows did not have a matching depth within tolerance for log '{df_name}'.\")\n",
    "            \n",
    "            # Add renamed feature columns to features list\n",
    "            features.extend([f'{df_name}_{col}' for col in cols if col != 'SB_DEPTH_cm'])\n",
    "            # Drop the temporary depth column used for merging\n",
    "            merged_data = merged_data.drop(columns=[f'SB_DEPTH_cm_{df_name}'])\n",
    "    \n",
    "    # Add SB_DEPTH_cm as a feature\n",
    "    features.append('SB_DEPTH_cm')\n",
    "    \n",
    "    return target_data, merged_data, features\n",
    "\n",
    "\n",
    "def apply_feature_weights(X, method):\n",
    "    \"\"\"Apply feature weights for XGBoost methods.\"\"\"\n",
    "    if method == 'xgb':\n",
    "        feature_weights = {\n",
    "            'RGB': {\n",
    "                'R': 0.1,\n",
    "                'G': 0.1,\n",
    "                'B': 0.1,\n",
    "                'Lumin': 0.32\n",
    "            },\n",
    "            'MS': {\n",
    "                'hiresMS': 3.0,\n",
    "                'MS': 0.5\n",
    "            },\n",
    "            'Physical': {\n",
    "                'PWVel_m/s': 0.05,\n",
    "                'PWAmp': 0.05,\n",
    "                'Den_gm/cc': 3.0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        X_weighted = X.copy()\n",
    "        for group, weights in feature_weights.items():\n",
    "            for feature, weight in weights.items():\n",
    "                if feature in X_weighted.columns:\n",
    "                    X_weighted[feature] = (X_weighted[feature] * weight).astype('float32')\n",
    "        return X_weighted\n",
    "    \n",
    "    elif method == 'xgblgbm':\n",
    "        feature_weights = {\n",
    "            'RGB': {\n",
    "                'R': 0.3,\n",
    "                'G': 0.3,\n",
    "                'B': 0.3,\n",
    "                'Lumin': 0.3\n",
    "            },\n",
    "            'MS': {\n",
    "                'hiresMS': 3.0,\n",
    "                'MS': 0.05\n",
    "            },\n",
    "            'Physical': {\n",
    "                'PWVel_m/s': 0.01,\n",
    "                'PWAmp': 0.01,\n",
    "                'Den_gm/cc': 0.5\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        X_weighted = X.copy()\n",
    "        for group, weights in feature_weights.items():\n",
    "            for feature, weight in weights.items():\n",
    "                matching_cols = [col for col in X_weighted.columns if feature in col]\n",
    "                for col in matching_cols:\n",
    "                    X_weighted[col] = (X_weighted[col] * weight).astype('float32')\n",
    "        return X_weighted\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def adjust_gap_predictions(df, gap_mask, ml_preds, target_log):\n",
    "    \"\"\"\n",
    "    Adjust ML predictions for gap rows in 'df' so that for each contiguous gap\n",
    "    segment (with both left and right boundaries available) the predictions are\n",
    "    blended with the linear interpolation between the boundary values.\n",
    "    \"\"\"\n",
    "    # Get the integer positions (row numbers) of missing values\n",
    "    gap_positions = np.where(gap_mask.values)[0]\n",
    "    # Create a Series for easier handling; index = positions in df\n",
    "    preds_series = pd.Series(ml_preds, index=gap_positions)\n",
    "    \n",
    "    # Identify contiguous segments in the gap positions\n",
    "    segments = np.split(gap_positions, np.where(np.diff(gap_positions) != 1)[0] + 1)\n",
    "    \n",
    "    adjusted = preds_series.copy()\n",
    "    for seg in segments:\n",
    "        # seg is an array of row positions (in df) for a contiguous gap segment.\n",
    "        start_pos = seg[0]\n",
    "        end_pos = seg[-1]\n",
    "        \n",
    "        # Enforce trend constraints only if both boundaries exist.\n",
    "        if start_pos == 0 or end_pos == len(df) - 1:\n",
    "            continue  # Skip segments at the very beginning or end.\n",
    "        \n",
    "        # Retrieve boundary (observed) values and depths\n",
    "        left_value = df.iloc[start_pos - 1][target_log]\n",
    "        right_value = df.iloc[end_pos + 1][target_log]\n",
    "        # Skip if boundaries are missing (should not happen if gap_mask is correct)\n",
    "        if pd.isna(left_value) or pd.isna(right_value):\n",
    "            continue\n",
    "        left_depth = df.iloc[start_pos - 1]['SB_DEPTH_cm']\n",
    "        right_depth = df.iloc[end_pos + 1]['SB_DEPTH_cm']\n",
    "        \n",
    "        # For each gap row in the segment, blend the ML prediction with linear interpolation\n",
    "        for pos in seg:\n",
    "            current_depth = df.iloc[pos]['SB_DEPTH_cm']\n",
    "            # Normalize the depth position (x in [0, 1])\n",
    "            if right_depth == left_depth:\n",
    "                x = 0.5\n",
    "            else:\n",
    "                x = (current_depth - left_depth) / (right_depth - left_depth)\n",
    "            # Compute the linear interpolation value at this depth\n",
    "            interp_val = left_value + (right_value - left_value) * x\n",
    "            # Define a weight that is 0 at the boundaries and 1 at the middle.\n",
    "            # Here we use: weight = 1 - 2*|x - 0.5|\n",
    "            weight = 1 - 2 * abs(x - 0.5)\n",
    "            weight = max(0, min(weight, 1))  # Ensure weight is between 0 and 1\n",
    "            # Blend: final = interpolation + weight*(ML_prediction - interpolation)\n",
    "            adjusted[pos] = interp_val + weight * (preds_series.loc[pos] - interp_val)\n",
    "    return adjusted.values\n",
    "\n",
    "\n",
    "def train_model(model):\n",
    "    \"\"\"Helper function for parallel model training.\"\"\"\n",
    "    def train_wrapper(X_train, y_train, X_pred):\n",
    "        model.fit(X_train, y_train)\n",
    "        return model.predict(X_pred)\n",
    "    return train_wrapper\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Consolidated Function\n",
    "def fill_gaps_with_ml(target_log=None, \n",
    "                      All_logs=None, \n",
    "                      output_csv=False, \n",
    "                      output_dir=None, \n",
    "                      core_name=None, \n",
    "                      merge_tolerance=3.0,\n",
    "                      ml_method='xgblgbm'):\n",
    "    \"\"\"\n",
    "    Fill gaps in target data using specified ML method.\n",
    "    \n",
    "    Args:\n",
    "        target_log (str): Name of the target column to fill gaps in.\n",
    "        All_logs (dict): Dictionary of dataframes containing feature data and target data.\n",
    "                         Format: {'df_name': (dataframe, [column_names])}\n",
    "        output_csv (bool): Whether to output filled data to CSV file.\n",
    "        output_dir (str): Directory to save output CSV file.\n",
    "        core_name (str): Name of the core for CSV filename.\n",
    "        merge_tolerance (float): Maximum allowed difference in depth (SB_DEPTH_cm) for merging\n",
    "                                 rows from different logs.\n",
    "        ml_method (str): ML method to use - 'rf', 'rftc', 'xgb', 'xgblgbm' (default)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (target_data_filled, gap_mask)\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if target_log is None or All_logs is None:\n",
    "        raise ValueError(\"Both target_log and All_logs must be provided\")\n",
    "        \n",
    "    if output_csv and (output_dir is None or core_name is None):\n",
    "        raise ValueError(\"output_dir and core_name must be provided when output_csv is True\")\n",
    "    \n",
    "    if ml_method not in ['rf', 'rftc', 'xgb', 'xgblgbm']:\n",
    "        raise ValueError(\"ml_method must be one of: 'rf', 'rftc', 'xgb', 'xgblgbm'\")\n",
    "    \n",
    "    # Prepare feature data\n",
    "    target_data, merged_data, features = prepare_feature_data(target_log, All_logs, merge_tolerance)\n",
    "    \n",
    "    # Create a copy of the original data to hold the interpolated results\n",
    "    target_data_filled = target_data.copy()\n",
    "\n",
    "    # Identify gaps in target data\n",
    "    gap_mask = target_data[target_log].isna()\n",
    "    \n",
    "    # If no gaps exist, save to CSV if requested and return original data\n",
    "    if not gap_mask.any():\n",
    "        if output_csv:\n",
    "            output_path = os.path.join(output_dir, f'{core_name}_{target_log.split(\"_\")[0]}_MLfilled.csv')\n",
    "            target_data_filled.to_csv(output_path, index=False)\n",
    "        return target_data_filled, gap_mask\n",
    "\n",
    "    if ml_method == 'rf':\n",
    "        # Convert all features to float64\n",
    "        for col in merged_data[features].columns:\n",
    "            if merged_data[features][col].dtype.kind in 'biufc':\n",
    "                merged_data[features][col] = merged_data[features][col].astype('float64')\n",
    "        merged_data[target_log] = merged_data[target_log].astype('float64')\n",
    "\n",
    "        # Prepare features and target for ML\n",
    "        X = merged_data[features].copy()\n",
    "        y = merged_data[target_log].copy()\n",
    "\n",
    "        # Split into training (non-gap) and prediction (gap) sets\n",
    "        X_train = X[~gap_mask]\n",
    "        y_train = y[~gap_mask]\n",
    "        X_pred = X[gap_mask]\n",
    "\n",
    "        # Handle outliers using IQR method\n",
    "        quantile_cutoff = 0.025 #fraction at both tail ends to be neglected in ML training\n",
    "        Q1 = y_train.quantile(quantile_cutoff)\n",
    "        Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "        X_train = X_train[outlier_mask]\n",
    "        y_train = y_train[outlier_mask]\n",
    "\n",
    "        # Initialize two ensemble models\n",
    "        models = [\n",
    "            RandomForestRegressor(n_estimators=1000,\n",
    "                                  max_depth=30,\n",
    "                                  min_samples_split=5,\n",
    "                                  min_samples_leaf=5,\n",
    "                                  max_features='sqrt',\n",
    "                                  bootstrap=True,\n",
    "                                  random_state=42,\n",
    "                                  n_jobs=-1),\n",
    "            HistGradientBoostingRegressor(max_iter=800,\n",
    "                                          learning_rate=0.05,\n",
    "                                          max_depth=5,\n",
    "                                          min_samples_leaf=50,\n",
    "                                          l2_regularization=1.0,\n",
    "                                          random_state=42,\n",
    "                                          verbose=0)\n",
    "        ]\n",
    "\n",
    "        # Train models in parallel\n",
    "        predictions = Parallel(n_jobs=-1)(delayed(train_model)(model) for model in models)\n",
    "\n",
    "        # Ensemble predictions by averaging\n",
    "        ensemble_predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "        # Fill gaps with ensemble predictions\n",
    "        target_data_filled.loc[gap_mask, target_log] = ensemble_predictions\n",
    "        \n",
    "    elif ml_method == 'rftc':\n",
    "        # It is best to work with merged_data sorted by depth\n",
    "        merged_data = merged_data.sort_values('SB_DEPTH_cm').reset_index(drop=True)\n",
    "        \n",
    "        # For consistency, recompute gap mask on merged_data\n",
    "        gap_mask = merged_data[target_log].isna()\n",
    "        \n",
    "        # Prepare feature matrix X and target vector y\n",
    "        X = merged_data[features].copy()\n",
    "        y = merged_data[target_log].copy()\n",
    "        \n",
    "        # Ensure all numeric features are float64\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype.kind in 'biufc':\n",
    "                X[col] = X[col].astype('float64')\n",
    "        y = y.astype('float64')\n",
    "        \n",
    "        # Split into training (non-gap) and prediction (gap) sets\n",
    "        X_train = X[~gap_mask]\n",
    "        y_train = y[~gap_mask]\n",
    "        X_pred = X[gap_mask]\n",
    "        \n",
    "        # Handle outliers in training data using the IQR method\n",
    "        quantile_cutoff = 0.15 #fraction at both tail ends to be neglected in ML training\n",
    "        Q1 = y_train.quantile(quantile_cutoff)\n",
    "        Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "        X_train = X_train[outlier_mask]\n",
    "        y_train = y_train[outlier_mask]\n",
    "        \n",
    "        # Initialize two ensemble models\n",
    "        models = [\n",
    "            RandomForestRegressor(n_estimators=1000,\n",
    "                                  max_depth=30,\n",
    "                                  min_samples_split=5,\n",
    "                                  min_samples_leaf=5,\n",
    "                                  max_features='sqrt',\n",
    "                                  bootstrap=True,\n",
    "                                  random_state=42,\n",
    "                                  n_jobs=-1),\n",
    "            HistGradientBoostingRegressor(max_iter=800,\n",
    "                                          learning_rate=0.05,\n",
    "                                          max_depth=5,\n",
    "                                          min_samples_leaf=50,\n",
    "                                          l2_regularization=1.0,\n",
    "                                          random_state=42,\n",
    "                                          verbose=0)\n",
    "        ]\n",
    "        \n",
    "        # Train models in parallel and average their predictions\n",
    "        predictions = Parallel(n_jobs=-1)(delayed(train_model)(model) for model in models)\n",
    "        ensemble_predictions = np.mean(predictions, axis=0)\n",
    "        \n",
    "        # Adjust the ensemble predictions using trend constraints.\n",
    "        adjusted_predictions = adjust_gap_predictions(merged_data, gap_mask, ensemble_predictions, target_log)\n",
    "        \n",
    "        # Fill the gaps in the original (filled) target data.\n",
    "        target_data_filled.loc[merged_data.index[gap_mask], target_log] = adjusted_predictions\n",
    "\n",
    "    elif ml_method == 'xgb':\n",
    "        # Convert target column to float32\n",
    "        target_data_filled[target_log] = target_data_filled[target_log].astype('float32')\n",
    "\n",
    "        # Prepare training data by merging all available logs\n",
    "        merged_data['SB_DEPTH_cm'] = merged_data['SB_DEPTH_cm'].astype('float32')\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = merged_data[features].copy()\n",
    "        y = merged_data[target_log].copy()\n",
    "\n",
    "        # Apply weights to features\n",
    "        X_weighted = apply_feature_weights(X, 'xgb')\n",
    "\n",
    "        # Split into training and prediction sets\n",
    "        X_train = X_weighted[~gap_mask]\n",
    "        y_train = y[~gap_mask]\n",
    "        X_pred = X_weighted[gap_mask]\n",
    "\n",
    "        # Handle outliers using IQR method\n",
    "        quantile_cutoff = 0.025 #fraction at both tail ends to be neglected in ML training\n",
    "        Q1 = y_train.quantile(quantile_cutoff)\n",
    "        Q3 = y_train.quantile(1 - quantile_cutoff)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_mask = (y_train >= Q1 - 1.5 * IQR) & (y_train <= Q3 + 1.5 * IQR)\n",
    "        X_train = X_train[outlier_mask]\n",
    "        y_train = y_train[outlier_mask]\n",
    "\n",
    "        # Create feature pipeline\n",
    "        feature_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=True)),\n",
    "            ('selector', SelectKBest(score_func=f_regression, k='all'))\n",
    "        ])\n",
    "\n",
    "        # Process features\n",
    "        X_train_processed = feature_pipeline.fit_transform(X_train, y_train)\n",
    "        X_pred_processed = feature_pipeline.transform(X_pred)\n",
    "\n",
    "        # Convert processed arrays to float32\n",
    "        X_train_processed = X_train_processed.astype('float32')\n",
    "        X_pred_processed = X_pred_processed.astype('float32')\n",
    "        y_train = y_train.astype('float32')\n",
    "\n",
    "        # Initialize and train XGBoost model with optimized hyperparameters for better accuracy\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=5000,           # More trees for better learning\n",
    "            learning_rate=0.003,         # Slower learning rate to prevent overfitting\n",
    "            max_depth=10,                # Reduced to prevent overfitting\n",
    "            min_child_weight=5,          # Increased to make model more conservative\n",
    "            subsample=0.75,              # sample ratio\n",
    "            colsample_bytree=0.75,       # feature sampling\n",
    "            gamma=0.2,                   # Added minimum loss reduction\n",
    "            reg_alpha=0.3,               # L1 regularization to reduce overfitting\n",
    "            reg_lambda=3.0,              # Increased L2 regularization\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train_processed, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model.predict(X_pred_processed).astype('float32')\n",
    "\n",
    "        # Fill gaps with predictions\n",
    "        target_data_filled.loc[gap_mask, target_log] = predictions\n",
    "\n",
    "    elif ml_method == 'xgblgbm':\n",
    "        target_data_filled[target_log] = target_data_filled[target_log].astype('float32')\n",
    "        \n",
    "        # Add cyclical features based on depth\n",
    "        periods = [10, 50, 100]  # Example periods in cm\n",
    "        for period in periods:\n",
    "            merged_data[f'SB_DEPTH_cm_sin'] = np.sin(2 * np.pi * merged_data['SB_DEPTH_cm'] / period)\n",
    "            merged_data[f'SB_DEPTH_cm_cos'] = np.cos(2 * np.pi * merged_data['SB_DEPTH_cm'] / period)\n",
    "            features.extend([f'SB_DEPTH_cm_sin', f'SB_DEPTH_cm_cos'])\n",
    "\n",
    "        # Prepare training data by merging all available logs\n",
    "        merged_data['SB_DEPTH_cm'] = merged_data['SB_DEPTH_cm'].astype('float32')\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = merged_data[features].copy()\n",
    "        y = merged_data[target_log].copy()\n",
    "\n",
    "        # Apply weights to features\n",
    "        X_weighted = apply_feature_weights(X, 'xgblgbm')\n",
    "\n",
    "        # Split into training and prediction sets\n",
    "        X_train = X_weighted[~gap_mask]\n",
    "        y_train = y[~gap_mask]\n",
    "        X_pred = X_weighted[gap_mask]\n",
    "\n",
    "        # Create feature pipeline\n",
    "        feature_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=True)),\n",
    "            ('selector', SelectKBest(score_func=f_regression, k=min(50, X_train.shape[0]//10)))  # Limit number of features\n",
    "        ])\n",
    "\n",
    "        # Process features\n",
    "        X_train_processed = feature_pipeline.fit_transform(X_train, y_train)\n",
    "        X_pred_processed = feature_pipeline.transform(X_pred)\n",
    "\n",
    "        # Convert processed arrays to float32\n",
    "        X_train_processed = X_train_processed.astype('float32')\n",
    "        X_pred_processed = X_pred_processed.astype('float32')\n",
    "        y_train = y_train.astype('float32')\n",
    "\n",
    "        # Initialize models\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            n_estimators=3000,           # More trees for better learning\n",
    "            learning_rate=0.003,         # Slower learning rate to prevent overfitting\n",
    "            max_depth=10,                # Reduced to prevent overfitting\n",
    "            min_child_weight=5,          # Increased to make model more conservative\n",
    "            subsample=0.75,              # sample ratio\n",
    "            colsample_bytree=0.75,       # feature sampling\n",
    "            gamma=0.2,                   # Added minimum loss reduction\n",
    "            reg_alpha=0.3,               # L1 regularization to reduce overfitting\n",
    "            reg_lambda=3.0,              # Increased L2 regularization\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=3000,           # Increased for more stable predictions\n",
    "            learning_rate=0.003,         # Reduced for more stable learning\n",
    "            max_depth=6,                 # Further reduced depth to prevent overfitting\n",
    "            num_leaves=20,               # Reduced leaves for simpler trees\n",
    "            min_child_samples=50,        # Increased to require more samples per leaf\n",
    "            subsample=0.9,               # Increased sample ratio for stability\n",
    "            colsample_bytree=0.9,        # Increased feature sampling for stability\n",
    "            reg_alpha=0.3,               # Increased L1 regularization\n",
    "            reg_lambda=3.0,              # Increased L2 regularization\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            force_col_wise=True,         # Remove threading overhead\n",
    "            verbose=-1                   # Reduce verbosity\n",
    "        )\n",
    "\n",
    "        # Train both models\n",
    "        xgb_model.fit(X_train_processed, y_train)\n",
    "        lgb_model.fit(X_train_processed, y_train)\n",
    "\n",
    "        # Make predictions with both models\n",
    "        xgb_predictions = xgb_model.predict(X_pred_processed).astype('float32')\n",
    "        lgb_predictions = lgb_model.predict(X_pred_processed).astype('float32')\n",
    "\n",
    "        # Ensemble predictions (simple average)\n",
    "        predictions = (xgb_predictions + lgb_predictions) / 2\n",
    "\n",
    "        # Fill gaps with predictions\n",
    "        target_data_filled.loc[gap_mask, target_log] = predictions\n",
    "\n",
    "    # Save to CSV if requested\n",
    "    if output_csv:\n",
    "        output_path = os.path.join(output_dir, f\"{core_name}_{target_log.split('_')[0]}_MLfilled.csv\")\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        target_data_filled.to_csv(output_path, index=False)\n",
    "\n",
    "    return target_data_filled, gap_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to process and fill logs with chosen ML methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_fill_logs(data_config, ml_method='xgblgbm'):\n",
    "    \"\"\"\n",
    "    Process and fill gaps in core log data using specified ML method.\n",
    "    \n",
    "    Args:\n",
    "        data_config (dict): Dictionary containing configuration parameters including:\n",
    "            - mother_dir (str): Path to the mother directory containing the data\n",
    "            - core_name (str): Name of the core being processed\n",
    "            - core_length (int): Length of the core (In any length unit. Just be sure it is consistent across the data files)\n",
    "            - clean_output_folder (str): Relative path to folder containing cleaned data files\n",
    "            - filled_output_folder (str): Relative path to folder for saving gap-filled data files\n",
    "            - file_patterns (dict): Dictionary mapping data types to filename patterns\n",
    "            - clean_file_suffix (str): File suffix for clean data files\n",
    "            - filled_file_suffix (str): File suffix for filled data files\n",
    "            - column_configs (dict): Dictionary mapping data types to their column configurations\n",
    "        ml_method (str): Machine learning method to use - 'rf' for Random Forest, \n",
    "                                                          'rftc' for Random Forest with trend constraints,\n",
    "                                                          'xgb' for XGBoost, \n",
    "                                                          'xgblgbm' for XGBoost + LightGBM (default)\n",
    "                                            \n",
    "    \"\"\"\n",
    "    # Extract variables from data_config\n",
    "    mother_dir = data_config['mother_dir']\n",
    "    core_name = data_config['core_name']\n",
    "    core_length = data_config['core_length']\n",
    "    filled_output_folder = data_config['filled_output_folder']\n",
    "    column_configs = data_config['column_configs']\n",
    "    \n",
    "    # Create filled output folder if it doesn't exist\n",
    "    filled_output_path = os.path.join(mother_dir, filled_output_folder)\n",
    "    os.makedirs(filled_output_path, exist_ok=True)\n",
    "    \n",
    "    # Build file paths using the helper function\n",
    "    clean_data_paths = build_file_paths(data_config, 'clean')\n",
    "    \n",
    "    # Read processed data files and check if they exist\n",
    "    data_dict = {}\n",
    "    for key, filepath in clean_data_paths.items():\n",
    "        if os.path.exists(filepath):\n",
    "            data = pd.read_csv(filepath)\n",
    "            if not data.empty:\n",
    "                data_dict[key] = data\n",
    "\n",
    "    # Create feature data dictionary using column configurations\n",
    "    feature_data = {}\n",
    "    for data_type, data_df in data_dict.items():\n",
    "        if data_type in column_configs:\n",
    "            config = column_configs[data_type]\n",
    "            \n",
    "            if data_type == 'rgb':\n",
    "                # Handle RGB data with multiple data columns\n",
    "                depth_col = config['depth_col']\n",
    "                data_cols = config['data_cols']\n",
    "                valid_cols = [depth_col] + [col for col in data_cols if col in data_df.columns and not data_df[col].empty]\n",
    "                if len(valid_cols) > 1:  # Must have at least depth and one measurement\n",
    "                    feature_data[data_type] = (data_df, valid_cols)\n",
    "                    \n",
    "            elif data_type == 'mst':\n",
    "                # Handle MST data with multiple measurement types\n",
    "                depth_col = None\n",
    "                valid_cols = []\n",
    "                \n",
    "                # Get depth column from any measurement type\n",
    "                for measurement_type, measurement_config in config.items():\n",
    "                    if 'depth_col' in measurement_config:\n",
    "                        depth_col = measurement_config['depth_col']\n",
    "                        break\n",
    "                \n",
    "                if depth_col and depth_col in data_df.columns:\n",
    "                    valid_cols = [depth_col]\n",
    "                    # Add all available data columns\n",
    "                    for measurement_type, measurement_config in config.items():\n",
    "                        if 'data_col' in measurement_config:\n",
    "                            data_col = measurement_config['data_col']\n",
    "                            if data_col in data_df.columns and not data_df[data_col].empty:\n",
    "                                valid_cols.append(data_col)\n",
    "                    \n",
    "                    if len(valid_cols) > 1:\n",
    "                        feature_data[data_type] = (data_df, valid_cols)\n",
    "                        \n",
    "            else:\n",
    "                # Handle single-column data types (ct, hrms, etc.)\n",
    "                if 'data_col' in config and 'depth_col' in config:\n",
    "                    data_col = config['data_col']\n",
    "                    depth_col = config['depth_col']\n",
    "                    if (data_col in data_df.columns and depth_col in data_df.columns and \n",
    "                        not data_df[[depth_col, data_col]].empty):\n",
    "                        feature_data[data_type] = (data_df, [depth_col, data_col])\n",
    "\n",
    "    # Define logs to process based on available data and column configurations\n",
    "    logs_to_process = []\n",
    "    for data_type, data_df in data_dict.items():\n",
    "        if data_type in column_configs:\n",
    "            config = column_configs[data_type]\n",
    "            \n",
    "            if data_type == 'rgb':\n",
    "                # Process each RGB component separately\n",
    "                for col in config['data_cols']:\n",
    "                    if col in data_df.columns and not data_df[col].empty:\n",
    "                        logs_to_process.append((col, col, data_df))\n",
    "                        \n",
    "            elif data_type == 'mst':\n",
    "                # Process each MST measurement separately\n",
    "                for measurement_type, measurement_config in config.items():\n",
    "                    if 'data_col' in measurement_config:\n",
    "                        data_col = measurement_config['data_col']\n",
    "                        if data_col in data_df.columns and not data_df[data_col].empty:\n",
    "                            logs_to_process.append((data_col, data_col, data_df))\n",
    "                            \n",
    "            else:\n",
    "                # Process single-column data types\n",
    "                if 'data_col' in config:\n",
    "                    data_col = config['data_col']\n",
    "                    if data_col in data_df.columns and not data_df[data_col].empty:\n",
    "                        logs_to_process.append((data_col, data_col, data_df))\n",
    "\n",
    "    # Loop through each log and apply ML gap filling\n",
    "    for target_log, plot_name, data in logs_to_process:\n",
    "        # Determine which features to use based on target log type\n",
    "        filtered_features = {}\n",
    "        \n",
    "        # Check if target log is from RGB data\n",
    "        is_rgb_log = any(target_log in column_configs.get('rgb', {}).get('data_cols', []) \n",
    "                        for data_type in column_configs if data_type == 'rgb')\n",
    "        \n",
    "        if is_rgb_log:\n",
    "            # For RGB logs, prioritize specific feature types\n",
    "            priority_features = ['hrms', 'ct', 'rgb']\n",
    "            \n",
    "            for data_type in priority_features:\n",
    "                if data_type in feature_data:\n",
    "                    if data_type == 'rgb':\n",
    "                        # Only include RGB columns that exist\n",
    "                        df, cols = feature_data[data_type]\n",
    "                        rgb_cols = column_configs['rgb']['data_cols']\n",
    "                        valid_cols = [column_configs['rgb']['depth_col']] + [c for c in rgb_cols if c in cols and c in df.columns]\n",
    "                        if len(valid_cols) > 1:\n",
    "                            filtered_features[data_type] = (df, valid_cols)\n",
    "                    else:\n",
    "                        filtered_features[data_type] = feature_data[data_type]\n",
    "            \n",
    "            # Add density if available from MST data\n",
    "            if 'mst' in feature_data and 'mst' in column_configs:\n",
    "                df, cols = feature_data['mst']\n",
    "                # Look for density column in MST configuration\n",
    "                density_col = None\n",
    "                for measurement_type, measurement_config in column_configs['mst'].items():\n",
    "                    if measurement_type == 'density' and 'data_col' in measurement_config:\n",
    "                        density_col = measurement_config['data_col']\n",
    "                        break\n",
    "                \n",
    "                if density_col and density_col in cols and density_col in df.columns:\n",
    "                    depth_col = column_configs['mst']['density']['depth_col']\n",
    "                    filtered_features['mst'] = (df, [depth_col, density_col])\n",
    "        else:\n",
    "            # Use all available features for non-RGB logs\n",
    "            filtered_features = feature_data.copy()\n",
    "            \n",
    "        # Apply ML gap filling\n",
    "        filled_data, gap_mask = fill_gaps_with_ml(\n",
    "            target_log=target_log,\n",
    "            All_logs=filtered_features,\n",
    "            output_csv=True,\n",
    "            output_dir=mother_dir + filled_output_folder,\n",
    "            core_name=core_name,\n",
    "            ml_method=ml_method\n",
    "        )\n",
    "            \n",
    "        # Plot filled data\n",
    "        plot_filled_data(\n",
    "            plot_name,\n",
    "            data,\n",
    "            filled_data,\n",
    "            core_length,\n",
    "            core_name\n",
    "        )\n",
    "\n",
    "    # Update multi-column data files (like RGB) by combining individual filled results\n",
    "    filled_data_paths = build_file_paths(data_config, 'filled')\n",
    "    \n",
    "    for data_type, data_df in data_dict.items():\n",
    "        if data_type in column_configs:\n",
    "            config = column_configs[data_type]\n",
    "            \n",
    "            # Handle multi-column data types\n",
    "            if data_type == 'rgb' and 'data_cols' in config:\n",
    "                updated = False\n",
    "                for col in config['data_cols']:\n",
    "                    if col in data_df.columns:\n",
    "                        filled_file = os.path.join(mother_dir, filled_output_folder, f'{core_name}_{col}_MLfilled.csv')\n",
    "                        if os.path.exists(filled_file):\n",
    "                            filled_data = pd.read_csv(filled_file)\n",
    "                            if col in filled_data.columns:\n",
    "                                data_df[col] = filled_data[col]\n",
    "                                print(f\"Filled data from {os.path.basename(filled_file)} has replaced the original {col} column\")\n",
    "                                updated = True\n",
    "                \n",
    "                if updated:\n",
    "                    data_df.to_csv(filled_data_paths[data_type], index=False)\n",
    "                    # Remove individual filled files\n",
    "                    for col in config['data_cols']:\n",
    "                        filled_file = os.path.join(mother_dir, filled_output_folder, f'{core_name}_{col}_MLfilled.csv')\n",
    "                        if os.path.exists(filled_file):\n",
    "                            os.remove(filled_file)\n",
    "                            print(f\"Removed {os.path.basename(filled_file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### **Define data structure**\n",
    "\n",
    "#### Define core name and core length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_name = \"M9907-11PC\"  # Core name\n",
    "total_length_cm = 439     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-12PC\"  # Core name\n",
    "# total_length_cm = 488     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-14TC\"  # Core name\n",
    "# total_length_cm = 199     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-22PC\"  # Core name\n",
    "# total_length_cm = 501     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-22TC\"  # Core name\n",
    "# total_length_cm = 173     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-23PC\"  # Core name\n",
    "# total_length_cm = 783     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-25PC\"  # Core name\n",
    "# total_length_cm = 797     # Core length in cm\n",
    "\n",
    "# core_name = \"RR0207-56PC\"  # Core name\n",
    "# total_length_cm = 794     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-30PC\"  # Core name\n",
    "# total_length_cm = 781     # Core length in cm\n",
    "\n",
    "# core_name = \"M9907-31PC\"  # Core name\n",
    "# total_length_cm = 767     # Core length in cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define file path, data configuration, and outliner cut-off thresholds for ML data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    'mother_dir': '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/OSU_dataset/',\n",
    "    'core_name': core_name,\n",
    "    'core_length': total_length_cm,\n",
    "    'data_folder': f'_compiled_logs/{core_name}/',\n",
    "    'clean_output_folder': f'_compiled_logs/{core_name}/ML_clean/',    # Output folder for cleaned files\n",
    "    'filled_output_folder': f'_compiled_logs/{core_name}/ML_filled/',  # Output folder for files wtih ML data-filling\n",
    "    \n",
    "    # File naming patterns\n",
    "    'file_patterns': {\n",
    "        'ct': '{core_name}_CT{suffix}',\n",
    "        'rgb': '{core_name}_RGB{suffix}',\n",
    "        'mst': '{core_name}_MST{suffix}',\n",
    "        'hrms': '{core_name}_hiresMS{suffix}'\n",
    "    },\n",
    "    \n",
    "    # File suffixes\n",
    "    'clean_file_suffix': '_clean.csv',\n",
    "    'filled_file_suffix': '_MLfilled.csv',\n",
    "    \n",
    "    # Image paths\n",
    "    'image_folder_pattern': '_compiled_logs/{core_name}/',\n",
    "    'ct_image_pattern': '{core_name}_CT.tiff',\n",
    "    'rgb_image_pattern': '{core_name}_RGB.tiff',\n",
    "    \n",
    "    # Column configurations\n",
    "    'column_configs': {\n",
    "        'ct': {'data_col': 'CT', 'std_col': 'CT_std', 'depth_col': 'SB_DEPTH_cm'},\n",
    "        'rgb': {\n",
    "            'data_cols': ['R', 'G', 'B', 'Lumin'],\n",
    "            'std_cols': ['R_std', 'G_std', 'B_std', 'Lumin_std'],\n",
    "            'depth_col': 'SB_DEPTH_cm'\n",
    "        },\n",
    "        'mst': {\n",
    "            'density': {'data_col': 'Den_gm/cc', 'depth_col': 'SB_DEPTH_cm'},\n",
    "            'pwvel': {'data_col': 'PWVel_m/s', 'depth_col': 'SB_DEPTH_cm'},\n",
    "            'pwamp': {'data_col': 'PWAmp', 'depth_col': 'SB_DEPTH_cm'},\n",
    "            'elecres': {'data_col': 'ElecRes_ohmm', 'depth_col': 'SB_DEPTH_cm'},\n",
    "            'ms': {'data_col': 'MS', 'depth_col': 'SB_DEPTH_cm'}\n",
    "        },\n",
    "        'hrms': {'data_col': 'hiresMS', 'depth_col': 'SB_DEPTH_cm'}\n",
    "    },\n",
    "    \n",
    "    # Thresholds\n",
    "    'thresholds': {\n",
    "        'ms': ['>', 180, 1],\n",
    "        'pwvel': ['>=', 1077, 1], \n",
    "        'den': ['<', 1.14, 5],\n",
    "        'elecres': ['<', 0, 1],\n",
    "        'hiresms': ['<=', 18, 1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract variables from data config using setup function\n",
    "(core_name, mother_dir, core_length, clean_output_folder, filled_output_folder,\n",
    " clean_data_paths, filled_data_paths, ct_img, rgb_img, column_configs) = setup_execution_variables(data_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run data cleaning function\n",
    "preprocess_core_data(data_config, shift_limit_multiplier=3.0)\n",
    "\n",
    "# Plot processed logs using new function signature\n",
    "fig, axes = plot_core_logs(\n",
    "    data_config,                           # Data configuration containing all parameters\n",
    "    file_type='clean',                     # Type of data files to plot ('clean' or 'filled')\n",
    "    title=f'{core_name} Cleaned Logs'      # Title for the plot figure\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-based data gap filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_fill_logs(data_config,              # Data configuration containing all parameters\n",
    "                      ml_method='xgblgbm')      # Available ml_method options: 'rf', 'rftc', 'xgb', 'xgblgbm'\n",
    "                                                    # - 'rf': Random Forest ML\n",
    "                                                    # - 'rftc': Random Forest ML with trend constraints\n",
    "                                                    # - 'xgb': XGBoost ML\n",
    "                                                    # - 'xgblgbm': XGBoost + LightGBM ML         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ML-based gap-filled log diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot filled logs using new function signature\n",
    "fig, axes = plot_core_logs(\n",
    "    data_config,                                              # Data configuration containing all parameters\n",
    "    file_type='filled',                                       # Type of data files to plot ('filled' for gap-filled data)\n",
    "    title=f'{core_name} XGBoost + LightGBM ML-Filled Logs'    # Title for the plot figure\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
