{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25kTe2nS84Tf"
   },
   "source": [
    "# **pyCoreRelator** [![GitHub](https://img.shields.io/badge/GitHub-pyCoreRelator-blue?logo=github)](https://github.com/GeoLarryLai/pyCoreRelator) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.xxxxxxxx.svg)](https://doi.org/10.5281/zenodo.xxxxxxxx)\n",
    "## **Workshop Notebook #7: Compare Real Core Correlation to Synthetic Null Hypothesis**   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GeoLarryLai/pyCoreRelator/blob/main/pyCoreRelator_7_compare2syn.ipynb)\n",
    "This notebook demonstrates the workflow for comparing real core pair correlation quality metrics against synthetic null hypothesis distributions using **pyCoreRelator**.\n",
    "\n",
    "### Key Functions from **pyCoreRelator**\n",
    "- **`load_log_data()`**: Load and process core log data\n",
    "- **`load_core_age_constraints()`**: Load age constraint data for cores\n",
    "- **`load_pickeddepth_ages_from_csv()`**: Load estimated ages for picked boundaries\n",
    "- **`run_multi_parameter_analysis()`**: Run correlation analysis with multiple parameter combinations\n",
    "- **`calculate_quality_comparison_t_statistics()`**: Calculate statistical comparison metrics\n",
    "- **`plot_quality_comparison_t_statistics()`**: Visualize comparison results\n",
    "\n",
    "For advanced usage, see [FUNCTION_DOCUMENTATION.md](https://github.com/GeoLarryLai/pyCoreRelator/blob/main/FUNCTION_DOCUMENTATION.md) for more details.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4awuK6ffO2t"
   },
   "source": [
    "# **Import Packages**\n",
    "Load correlation analysis and comparison functions from **pyCoreRelator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47Kk07X_84Th"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyCoreRelator import (\n",
    "    load_log_data,\n",
    "    load_core_age_constraints,\n",
    "    load_pickeddepth_ages_from_csv,\n",
    "    run_multi_parameter_analysis,\n",
    "    calculate_quality_comparison_t_statistics,\n",
    "    plot_quality_comparison_t_statistics\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# **Configure Core Pair and Data Paths**\n",
    "\n",
    "Define the core pairs to analyze and specify paths to log data and picked datum files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Core Pair\n",
    "\n",
    "Select which cores to correlate (Core A and Core B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_A = \"M9907-25PC\"\n",
    "# CORE_A = \"M9907-23PC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_B = \"M9907-23PC\"\n",
    "# CORE_B = \"M9907-11PC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Log Data Paths and Column Structure\n",
    "\n",
    "Configure which log types to use for correlation and specify file paths for each core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log columns to extract\n",
    "LOG_COLUMNS = ['hiresMS', 'CT', 'Lumin']  # Choose which logs to include\n",
    "# LOG_COLUMNS = ['hiresMS']  # Choose which logs to include\n",
    "\n",
    "# Define depth column\n",
    "DEPTH_COLUMN = 'SB_DEPTH_cm'\n",
    "\n",
    "# Define paths for Core A\n",
    "core_a_log_paths = {\n",
    "    'hiresMS': f'example_data/processed_data/{CORE_A}/{CORE_A}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'example_data/processed_data/{CORE_A}/{CORE_A}_CT_MLfilled.csv',\n",
    "    'Lumin': f'example_data/processed_data/{CORE_A}/{CORE_A}_RGB_MLfilled.csv',\n",
    "}\n",
    "\n",
    "# Define paths for Core B\n",
    "core_b_log_paths = {\n",
    "    'hiresMS': f'example_data/processed_data/{CORE_B}/{CORE_B}_hiresMS_MLfilled.csv',\n",
    "    'CT': f'example_data/processed_data/{CORE_B}/{CORE_B}_CT_MLfilled.csv',\n",
    "    'Lumin': f'example_data/processed_data/{CORE_B}/{CORE_B}_RGB_MLfilled.csv',\n",
    "}\n",
    "\n",
    "# Define column mapping for alternative column names\n",
    "column_alternatives = {\n",
    "    'hiresMS': ['MS'],\n",
    "    'CT': ['CT_value'],\n",
    "    'Lumin': ['luminance', 'Luminance'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Log Data\n",
    "\n",
    "**Function: `load_log_data()`**\n",
    "\n",
    "**What it does:**\n",
    "Loads and processes core log data from CSV files, resamples to common depth scale, and optionally loads core images.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `log_paths` *(dict)*: Dictionary mapping log names to CSV file paths\n",
    "- `img_paths` *(dict, default=None)*: Dictionary mapping image types ('rgb', 'ct') to file paths or directories\n",
    "- `log_columns` *(list)*: List of log column names to extract from CSV files\n",
    "- `depth_column` *(str, default='SB_DEPTH_cm')*: Name of the depth column in CSV files\n",
    "- `normalize` *(bool, default=True)*: Whether to normalize log values to [0, 1] range\n",
    "- `column_alternatives` *(dict, default=None)*: Dictionary mapping log names to lists of alternative column names\n",
    "\n",
    "**Returns:**\n",
    "- `log` (numpy.ndarray): Log data array with shape (n_samples, n_logs) or (n_samples,) for single log\n",
    "- `md` (numpy.ndarray): Measured depth array\n",
    "- `available_columns` (list): Names of successfully loaded logs\n",
    "- `rgb_img` (numpy.ndarray or None): RGB image array if available\n",
    "- `ct_img` (numpy.ndarray or None): CT image array if available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# **Load Core Data**\n",
    "\n",
    "Load log data, picked depth boundaries, and age constraints for both cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for Core A\n",
    "log_a, md_a, _, _, _ = load_log_data(\n",
    "    core_a_log_paths,\n",
    "    log_columns=LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")\n",
    "\n",
    "# Load data for Core B\n",
    "log_b, md_b, _, _, _ = load_log_data(\n",
    "    core_b_log_paths,\n",
    "    log_columns=LOG_COLUMNS,\n",
    "    depth_column=DEPTH_COLUMN,\n",
    "    normalize=True,\n",
    "    column_alternatives=column_alternatives\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Picked Depth Boundaries\n",
    "\n",
    "Load the manually picked stratigraphic boundaries (datum) for both cores from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the CSV files\n",
    "pickeddepth_a_csv = f'pickeddepth/{CORE_A}_pickeddepth.csv'\n",
    "pickeddepth_b_csv = f'pickeddepth/{CORE_B}_pickeddepth.csv'\n",
    "\n",
    "# Load picked depths and extract category 1 depths\n",
    "if os.path.exists(pickeddepth_a_csv):\n",
    "    picked_data_a = pd.read_csv(pickeddepth_a_csv)\n",
    "    all_depths_a_cat1 = picked_data_a[picked_data_a['category'] == 1]['picked_depths_cm'].values.astype('float32')\n",
    "    intepreted_bed_a_cat1 = picked_data_a[picked_data_a['category'] == 1]['interpreted_bed'].fillna('').values.astype('str')\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_a_csv} not found. Using empty array for all_depths_a_cat1.\")\n",
    "    all_depths_a_cat1 = np.array([]).astype('float32')\n",
    "    intepreted_bed_a_cat1 = np.array([]).astype('str').fillna('')\n",
    "\n",
    "if os.path.exists(pickeddepth_b_csv):\n",
    "    picked_data_b = pd.read_csv(pickeddepth_b_csv)\n",
    "    all_depths_b_cat1 = picked_data_b[picked_data_b['category'] == 1]['picked_depths_cm'].values.astype('float32')\n",
    "    intepreted_bed_b_cat1 = picked_data_b[picked_data_b['category'] == 1]['interpreted_bed'].fillna('').values.astype('str')\n",
    "else:\n",
    "    print(f\"Warning: {pickeddepth_b_csv} not found. Using empty array for all_depths_b_cat1.\")\n",
    "    all_depths_b_cat1 = np.array([]).astype('float32')\n",
    "    intepreted_bed_b_cat1 = np.array([]).astype('str').fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Age Constraints\n",
    "\n",
    "**Function: `load_core_age_constraints()`**\n",
    "\n",
    "**What it does:**\n",
    "Loads age constraint data for a single core from CSV files in a directory. Searches for all CSV files containing the core name and combines them.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `core_name` *(str)*: Name of the core to load age constraints for\n",
    "- `age_base_path` *(str)*: Base directory path containing age constraint CSV files\n",
    "- `data_columns` *(dict, default=None)*: Dictionary mapping standard column names to actual CSV column names. Required keys: 'age', 'pos_error', 'neg_error', 'min_depth', 'max_depth', 'in_sequence', 'core', 'interpreted_bed'\n",
    "- `mute_mode` *(bool, default=False)*: If True, suppress all print statements\n",
    "\n",
    "**Returns:**\n",
    "- `age_data` (dict): Dictionary containing age constraint data with keys:\n",
    "  - `depths` (list): Mean depths of age constraints\n",
    "  - `ages` (list): Calibrated ages in years BP\n",
    "  - `pos_errors` (list): Positive 2-sigma uncertainties\n",
    "  - `neg_errors` (list): Negative 2-sigma uncertainties\n",
    "  - `in_sequence_flags` (list): Boolean flags for stratigraphic sequence\n",
    "  - `in_sequence_depths`, `in_sequence_ages`, `in_sequence_pos_errors`, `in_sequence_neg_errors` (lists): Filtered in-sequence data\n",
    "  - `out_sequence_depths`, `out_sequence_ages`, `out_sequence_pos_errors`, `out_sequence_neg_errors` (lists): Out-of-sequence data\n",
    "  - `core` (list): Core identifiers\n",
    "  - `interpreted_bed` (list): Interpreted bed names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = {\n",
    "    'age': 'calib810_agebp',\n",
    "    'pos_error': 'calib810_2sigma_pos', \n",
    "    'neg_error': 'calib810_2sigma_neg',\n",
    "    'min_depth': 'mindepth_cm',\n",
    "    'max_depth': 'maxdepth_cm',\n",
    "    'in_sequence': 'in_sequence',\n",
    "    'core': 'core',\n",
    "    'interpreted_bed': 'interpreted_bed'\n",
    "}\n",
    "\n",
    "# Define the path to the age constraints csv file\n",
    "age_base_path = '/Users/larryslai/Library/CloudStorage/Dropbox/My Documents/University of Texas Austin/(Project) NWP turbidites/Cascadia_core_data/Age constraints/Goldfinger2012'\n",
    "\n",
    "# Load age constraints for both cores from the csv file\n",
    "age_data_a = load_core_age_constraints(CORE_A, age_base_path, data_columns=data_columns, mute_mode=True)\n",
    "age_data_b = load_core_age_constraints(CORE_B, age_base_path, data_columns=data_columns, mute_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Estimated Ages for Picked Boundaries\n",
    "\n",
    "**Function: `load_pickeddepth_ages_from_csv()`**\n",
    "\n",
    "**What it does:**\n",
    "Loads pre-calculated interpolated ages for picked depth boundaries from CSV file (output from `calculate_interpolated_ages()`).\n",
    "\n",
    "**Key Parameters:**\n",
    "- `pickeddepth_age_csv` *(str)*: Path to CSV file containing pre-calculated ages\n",
    "\n",
    "**Returns:**\n",
    "- `age_data` (dict): Dictionary containing age estimates with keys:\n",
    "  - `depths` (numpy.ndarray): Picked datum depths\n",
    "  - `ages` (numpy.ndarray): Interpolated ages in years BP\n",
    "  - `pos_uncertainties` (numpy.ndarray): Positive age uncertainties\n",
    "  - `neg_uncertainties` (numpy.ndarray): Negative age uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = [CORE_A, CORE_B]\n",
    "pickeddepth_ages = {}\n",
    "\n",
    "# Define the uncertainty method: 'MonteCarlo', 'Linear', or 'Gaussian'\n",
    "uncertainty_method='MonteCarlo'   \n",
    "\n",
    "for core in cores:\n",
    "    core_age_csv = f\"pickeddepth_ages/{core}_pickeddepth_ages_{uncertainty_method}.csv\"\n",
    "    pickeddepth_ages[core] = load_pickeddepth_ages_from_csv(core_age_csv)\n",
    "\n",
    "# Assign to individual variables\n",
    "if CORE_A in pickeddepth_ages:\n",
    "    pickeddepth_ages_a = pickeddepth_ages[CORE_A]\n",
    "if CORE_B in pickeddepth_ages:\n",
    "    pickeddepth_ages_b = pickeddepth_ages[CORE_B]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Comprehensive DTW Analysis\n",
    "\n",
    "**Function: `run_multi_parameter_analysis()`**\n",
    "\n",
    "**What it does:**\n",
    "1. Runs DTW correlation analysis for multiple parameter combinations (with/without age constraints)\n",
    "2. Tests scenarios with progressive removal of age constraints (if enabled)\n",
    "3. Computes quality metric distributions for each scenario\n",
    "4. Fits probability distributions and calculates statistical parameters\n",
    "5. Exports fit parameters and distribution data to CSV files for comparison\n",
    "\n",
    "**Key Parameters:**\n",
    "- `log_a`, `log_b` *(array-like)*: Core log data arrays\n",
    "- `md_a`, `md_b` *(array-like)*: Measured depth arrays\n",
    "- `all_depths_a_cat1`, `all_depths_b_cat1` *(array-like)*: Picked boundary depths (category 1)\n",
    "- `pickeddepth_ages_a`, `pickeddepth_ages_b` *(dict)*: Age interpolation results for picked depths with keys: 'depths', 'ages', 'pos_uncertainties', 'neg_uncertainties'\n",
    "- `age_data_a`, `age_data_b` *(dict)*: Age constraint data from `load_core_age_constraints()`\n",
    "- `uncertainty_method` *(str)*: Age uncertainty calculation method ('MonteCarlo', 'Linear', or 'Gaussian')\n",
    "- `parameter_combinations` *(list of dict)*: List of parameter dictionaries to test. Each dict should contain: 'age_consideration', 'restricted_age_correlation', 'shortest_path_search'\n",
    "- `target_quality_indices` *(list)*: Quality metrics to analyze (e.g., ['corr_coef', 'norm_dtw', 'perc_diag'])\n",
    "- `test_age_constraint_removal` *(bool, default=True)*: Whether to test progressive age constraint removal scenarios\n",
    "- `core_a_name`, `core_b_name` *(str)*: Core identifiers for output file naming\n",
    "- `output_csv_filenames` *(dict)*: Dictionary mapping quality_index to output CSV filename paths\n",
    "- `synthetic_csv_filenames` *(dict, default=None)*: Dictionary mapping quality_index to synthetic CSV filenames for consistent bin sizing\n",
    "- `pca_for_dependent_dtw` *(bool, default=False)*: Use PCA for dependent multidimensional DTW\n",
    "- `n_jobs` *(int, default=-1)*: Number of parallel jobs (-1 uses all available CPU cores, 1 for sequential)\n",
    "- `max_search_per_layer` *(int or None, default=None)*: Maximum scenarios per constraint removal layer. If None, processes all scenarios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# **Run Multi-Parameter Correlation Analysis**\n",
    "\n",
    "Execute correlation analysis with different parameter combinations to test various hypotheses and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter combinations to test\n",
    "parameter_combinations = [\n",
    "    {'age_consideration': True, 'restricted_age_correlation': True, 'shortest_path_search': True},\n",
    "    {'age_consideration': False, 'restricted_age_correlation': False, 'shortest_path_search': True}\n",
    "]\n",
    "\n",
    "# Define quality indices to process\n",
    "target_quality_indices = ['corr_coef', 'norm_dtw']\n",
    "\n",
    "output_csv_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    output_csv_filenames[quality_index] = f'example_data/analytical_outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_fit_params.csv'\n",
    "\n",
    "# Define synthetic CSV filenames for consistent bin sizing\n",
    "synthetic_csv_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    synthetic_csv_filenames[quality_index] = f'example_data/analytical_outputs/synthetic_PDFs_{\"_\".join(LOG_COLUMNS)}_{quality_index}.csv'\n",
    "\n",
    "# Run the multi-parameter analysis\n",
    "run_multi_parameter_analysis(\n",
    "    # Core data inputs\n",
    "    log_a=log_a, \n",
    "    log_b=log_b, \n",
    "    md_a=md_a, \n",
    "    md_b=md_b,\n",
    "    all_depths_a_cat1=all_depths_a_cat1,\n",
    "    all_depths_b_cat1=all_depths_b_cat1,\n",
    "    pickeddepth_ages_a=pickeddepth_ages_a,\n",
    "    pickeddepth_ages_b=pickeddepth_ages_b,\n",
    "    age_data_a=age_data_a,\n",
    "    age_data_b=age_data_b,\n",
    "    uncertainty_method=uncertainty_method,\n",
    "    \n",
    "    # Analysis parameters\n",
    "    parameter_combinations=parameter_combinations,\n",
    "    target_quality_indices=target_quality_indices,\n",
    "    \n",
    "    # Core identifiers\n",
    "    core_a_name=CORE_A,\n",
    "    core_b_name=CORE_B,\n",
    "    \n",
    "    # Output configuration\n",
    "    output_csv_filenames=output_csv_filenames,\n",
    "    \n",
    "    max_search_per_layer=50      # Maximum scenarios per constraint removal layer (higher = better coverage, longer runtime)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Statistical Comparison Metrics\n",
    "\n",
    "**Function: `calculate_quality_comparison_t_statistics()`**\n",
    "\n",
    "**What it does:**\n",
    "1. Loads real core correlation quality distributions from master CSV files\n",
    "2. Loads synthetic null hypothesis distributions from synthetic CSV files\n",
    "3. Calculates t-statistics comparing real vs. synthetic distributions\n",
    "4. Computes percentile rankings and significance levels\n",
    "5. Stores statistical comparison results for visualization\n",
    "\n",
    "**Key Parameters:**\n",
    "- `target_quality_indices` *(list)*: Quality metrics to compare (e.g., ['corr_coef', 'norm_dtw'])\n",
    "- `master_csv_filenames` *(dict)*: Dictionary mapping quality indices to real data CSV paths\n",
    "- `synthetic_csv_filenames` *(dict)*: Dictionary mapping quality indices to synthetic data CSV paths\n",
    "- `CORE_A`, `CORE_B` *(str)*: Core identifiers for context\n",
    "- `mute_mode` *(bool, default=False)*: Suppress console output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# **Compare Real Correlation to Synthetic Null Hypothesis**\n",
    "\n",
    "Calculate statistical comparisons and visualize how real core correlation quality compares to synthetic distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_quality_indices = ['corr_coef', 'norm_dtw']\n",
    "\n",
    "# Define paths to real core correlation fit parameters\n",
    "master_csv_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    master_csv_filenames[quality_index] = f'example_data/analytical_outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_fit_params.csv'\n",
    "\n",
    "# Define paths to synthetic null hypothesis distributions\n",
    "synthetic_csv_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    synthetic_csv_filenames[quality_index] = f'example_data/analytical_outputs/synthetic_PDFs_{\"_\".join(LOG_COLUMNS)}_{quality_index}.csv'\n",
    "\n",
    "# Calculate statistical comparison metrics\n",
    "calculate_quality_comparison_t_statistics(\n",
    "    target_quality_indices=target_quality_indices,\n",
    "    master_csv_filenames=master_csv_filenames,\n",
    "    synthetic_csv_filenames=synthetic_csv_filenames,\n",
    "    CORE_A=CORE_A,\n",
    "    CORE_B=CORE_B,\n",
    "    mute_mode=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Static Comparison Plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Comparison Results\n",
    "\n",
    "**Function: `plot_quality_comparison_t_statistics()`**\n",
    "\n",
    "**What it does:**\n",
    "1. Creates visualizations comparing real core correlation quality to synthetic null hypothesis\n",
    "2. Plots probability distribution curves overlaying real and synthetic data\n",
    "3. Shows statistical significance and percentile rankings\n",
    "4. Can generate static plots (PNG/PDF/SVG) or animated GIFs showing progressive constraint addition\n",
    "5. Optionally highlights the optimal datum match solution\n",
    "\n",
    "**Key Parameters:**\n",
    "- `target_quality_indices` *(list)*: Quality metrics to plot (e.g., ['corr_coef', 'norm_dtw', 'perc_diag'])\n",
    "- `master_csv_filenames` *(dict)*: Dictionary mapping quality indices to master CSV file paths (should contain t-statistics columns)\n",
    "- `synthetic_csv_filenames` *(dict)*: Dictionary mapping quality indices to synthetic CSV file paths\n",
    "- `CORE_A` *(str)*: Name of core A for plot titles\n",
    "- `CORE_B` *(str)*: Name of core B for plot titles\n",
    "- `mute_mode` *(bool, default=False)*: If True, suppress detailed output messages and show only essential progress information\n",
    "- `save_fig` *(bool, default=False)*: If True, save static figures to files\n",
    "- `output_figure_filenames` *(dict, default=None)*: Dictionary mapping quality indices to output figure file paths (only used when save_fig=True)\n",
    "- `save_gif` *(bool, default=False)*: If True, create animated GIF showing progressive addition of age constraints.\n",
    "- `output_gif_filenames` *(dict, default=None)*: Dictionary mapping quality indices to GIF file paths (only used when save_gif=True)\n",
    "- `max_frames` *(int, default=50)*: Maximum number of frames for GIF animations\n",
    "- `plot_real_data_histogram` *(bool, default=False)*: If True, plot histograms for real data (no age and all age constraint cases)\n",
    "- `plot_age_removal_step_pdf` *(bool, default=False)*: If True, plot all PDF curves including dashed lines for partially removed constraints\n",
    "- `show_best_datum_match` *(bool, default=True)*: If True, plot vertical line showing best datum match value from sequential_mappings_csv\n",
    "- `sequential_mappings_csv` *(str or dict, default=None)*: Path to CSV file(s) containing sequential mappings with 'Ranking_datums' column. Can be a single CSV path (str) or dictionary mapping quality indices to CSV paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to optimal mapping CSV (try restricted_age_optimal first, fallback to no_age_optimal)\n",
    "sequential_mappings_csv = f'example_data/analytical_outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/mappings_restricted_age_optimal.csv'\n",
    "if not os.path.exists(sequential_mappings_csv):\n",
    "    sequential_mappings_csv = f'example_data/analytical_outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/mappings_no_age_optimal.csv'\n",
    "    if not os.path.exists(sequential_mappings_csv):\n",
    "        sequential_mappings_csv = None\n",
    "\n",
    "# Define output paths for static comparison figures\n",
    "output_figure_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    output_figure_filenames[quality_index] = f'example_data/analytical_outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_compare2null.png'\n",
    "\n",
    "# Generate static comparison plots\n",
    "plot_quality_comparison_t_statistics(\n",
    "    target_quality_indices=target_quality_indices,\n",
    "    master_csv_filenames=master_csv_filenames,\n",
    "    synthetic_csv_filenames=synthetic_csv_filenames,\n",
    "    CORE_A=CORE_A,\n",
    "    CORE_B=CORE_B,\n",
    "    save_fig=True,\n",
    "    plot_real_data_histogram=True,\n",
    "    output_figure_filenames=output_figure_filenames,   # Acceptable formats: png, jpg, svg, pdf\n",
    "    sequential_mappings_csv=sequential_mappings_csv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Animated GIF Showing Progressive Constraint Addition\n",
    "\n",
    "Create animated visualizations showing how the quality metric distributions evolve as age constraints are progressively removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output paths for animated GIFs\n",
    "output_gif_filenames = {}\n",
    "for quality_index in target_quality_indices:\n",
    "    output_gif_filenames[quality_index] = f'example_data/analytical_outputs/{CORE_A}_{CORE_B}/{\"_\".join(LOG_COLUMNS)}/{quality_index}_compare2null.gif'\n",
    "\n",
    "plot_quality_comparison_t_statistics(\n",
    "    target_quality_indices=target_quality_indices,\n",
    "    master_csv_filenames=master_csv_filenames,\n",
    "    synthetic_csv_filenames=synthetic_csv_filenames,\n",
    "    CORE_A=CORE_A,\n",
    "    CORE_B=CORE_B,\n",
    "    save_gif=True, \n",
    "    output_gif_filenames=output_gif_filenames,\n",
    "    plot_real_data_histogram=False,\n",
    "    plot_age_removal_step_pdf=True,\n",
    "    sequential_mappings_csv=sequential_mappings_csv\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "11-TimeSeriesCorrelation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
